{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"What is the Citrix Ingress Controller? Citrix provides an Ingress Controller to Citrix ADC MPX (hardware), Citrix ADC VPX (virtualized), and Citrix ADC CPX (containerized) for bare metal and cloud deployments. It is built around Kubernetes Ingress and automatically configures one or more Citrix ADC based on the Ingress resource configuration. Clients outside a Kubernetes cluster need a way to access the services provided by pods inside the cluster. For services that provide HTTP(s) access, this access is provided through a layer-7 proxy also known as Application Delivery Controller (ADC) device or a load balancer device. Kubernetes provides an API object, called Ingress that defines rules on how clients access services in a Kubernetes cluster. Service owners create these Ingress resources that define rules for directing HTTP(s) traffic. An Ingress controller watches the Kubernetes API server for updates to the Ingress resource and accordingly reconfigures the Ingress ADC. The Citrix Ingress Controller supports topologies and traffic management beyond standard HTTP(s) Ingress. Citrix ADCs with Citrix Ingress Controllers support Single-Tier and Dual-Tier traffic load balancing. The service owners can also control ingress TCP/TLS and UDP traffic. Citrix Ingress Controller (CIC) automates the configuration of Citrix ADCs to proxy traffic into ( North-South ) and between ( East-West ) the microservices in a Kubernetes cluster. The North-South traffic refers to the traffic from clients outside the cluster to microservices in the Kubernetes cluster. The East-West traffic refers to the traffic between the microservices inside the Kubernetes cluster. Typically, North-South traffic is load balanced by Ingress devices such as Citrix ADCs while East-West traffic is load balanced by kube-proxy . Since kube-proxy only provides limited layer-4 load balancing, service owners can utilize the Citrix Ingress Controller to achieve sophisticated layer-7 controls for East-West traffic using the Ingress CPX ADCs . Deploying Citrix Ingress Controller You can deploy Citrix Ingress Controller in the following deployment modes: As a standalone pod. This mode is used when managing ADCs such as, Citrix ADC MPX or VPX that are outside the Kubernetes cluster. As a sidecar in a pod along with the Citrix ADC CPX in the same pod. The controller is only responsible for the Citrix ADC CPX that resides in the same pod. You can deploy Citrix Ingress Controller using Kubernetes YAML or Helm charts. For more information, see Deploying Citrix Ingress Controller using YAML or Deploying Citrix Ingress Controller using Helm charts .","title":"Overview"},{"location":"#what-is-the-citrix-ingress-controller","text":"Citrix provides an Ingress Controller to Citrix ADC MPX (hardware), Citrix ADC VPX (virtualized), and Citrix ADC CPX (containerized) for bare metal and cloud deployments. It is built around Kubernetes Ingress and automatically configures one or more Citrix ADC based on the Ingress resource configuration. Clients outside a Kubernetes cluster need a way to access the services provided by pods inside the cluster. For services that provide HTTP(s) access, this access is provided through a layer-7 proxy also known as Application Delivery Controller (ADC) device or a load balancer device. Kubernetes provides an API object, called Ingress that defines rules on how clients access services in a Kubernetes cluster. Service owners create these Ingress resources that define rules for directing HTTP(s) traffic. An Ingress controller watches the Kubernetes API server for updates to the Ingress resource and accordingly reconfigures the Ingress ADC. The Citrix Ingress Controller supports topologies and traffic management beyond standard HTTP(s) Ingress. Citrix ADCs with Citrix Ingress Controllers support Single-Tier and Dual-Tier traffic load balancing. The service owners can also control ingress TCP/TLS and UDP traffic. Citrix Ingress Controller (CIC) automates the configuration of Citrix ADCs to proxy traffic into ( North-South ) and between ( East-West ) the microservices in a Kubernetes cluster. The North-South traffic refers to the traffic from clients outside the cluster to microservices in the Kubernetes cluster. The East-West traffic refers to the traffic between the microservices inside the Kubernetes cluster. Typically, North-South traffic is load balanced by Ingress devices such as Citrix ADCs while East-West traffic is load balanced by kube-proxy . Since kube-proxy only provides limited layer-4 load balancing, service owners can utilize the Citrix Ingress Controller to achieve sophisticated layer-7 controls for East-West traffic using the Ingress CPX ADCs .","title":"What is the Citrix Ingress Controller?"},{"location":"#deploying-citrix-ingress-controller","text":"You can deploy Citrix Ingress Controller in the following deployment modes: As a standalone pod. This mode is used when managing ADCs such as, Citrix ADC MPX or VPX that are outside the Kubernetes cluster. As a sidecar in a pod along with the Citrix ADC CPX in the same pod. The controller is only responsible for the Citrix ADC CPX that resides in the same pod. You can deploy Citrix Ingress Controller using Kubernetes YAML or Helm charts. For more information, see Deploying Citrix Ingress Controller using YAML or Deploying Citrix Ingress Controller using Helm charts .","title":"Deploying Citrix Ingress Controller"},{"location":"deployment-topologies/","text":"Deployment topologies Citrix ADCs can be combined in powerful and flexible topologies that complement organizational boundaries. Dual-tier deployments employ high-capacity hardware or virtualized Citrix ADCs (Citrix ADC MPX and VPX) in the first tier to offload security functions and implement relatively static organizational policies while segmenting control between network operators and Kubernetes operators. In Dual-tier deployments, the second tier is within the Kubernetes Cluster (using the Citrix ADC CPX) and is under control of the service owners. This setup provides stability for network operators, while allowing Kubernetes users to implement high-velocity changes. Single-tier topologies are suited to organizations that need to handle high rates of change. Single-Tier topology In a Single-Tier topology, Citrix ADC MPX or VPX devices proxy the (North-South) traffic from the clients to microservices inside the cluster. The Citrix Ingress Controller (CIC) is deployed as a pod in the Kubernetes cluster. The controller automates the configuration of Citrix ADCs (MPX or VPX) based on the changes to the microservices or the Ingress resources. Dual-Tier topology In Dual-Tier topology, Citrix ADC MPX or VPX devices in Tier-1 proxy the traffic (North-South) from the client to Citrix ADC CPXs in Tier-2. The Tier-2 Citrix ADC CPX then routes the traffic to the microservices in the Kubernetes cluster. The Citrix Ingress Controller deployed as a standalone pod configures the Tier-1 devices. And, the sidecar controller in one or more Citrix ADC CPX pods configures the associated Citrix ADC CPX in the same pod. Cloud topology Kubernetes clusters in public clouds such as Amazon Web Services (AWS) , Google Cloud , and Microsoft Azure can use their native load balancing services such as, AWS Elastic Load Balancing , Google Cloud Load Balancing , and Microsoft Azure NLB as the first (relatively static) tier of load balancing to a second tier of Citrix ADC CPX. Citrix ADC CPX operates inside the Kubernetes cluster with the sidecar Ingress controller. The Kubernetes clusters can be self-hosted or managed by the cloud provider (for example, AWS EKS , Google GKE and Azure AKS ) while using the Citrix ADC CPX as the Ingress. If the cloud-based Kubernetes cluster is self-hosted or self-managed, the Citrix ADC VPX can be used as the first tier in a Dual-tier topology. Cloud deployment with Citrix ADC (VPX) in tier-1: Cloud deployment with Cloud LB in tier-1: Using the Ingress ADC for East-West traffic When the Citrix ADC CPX is deployed inside the cluster as an Ingress, it can be used to proxy network (East-West) traffic between microservices within the cluster. For this, the target microservice needs to be deployed in headless mode to bypass kube-proxy , so that you can benefit from the advanced ADC functionalities provided by Citrix ADC.","title":"Deployment topologies"},{"location":"deployment-topologies/#deployment-topologies","text":"Citrix ADCs can be combined in powerful and flexible topologies that complement organizational boundaries. Dual-tier deployments employ high-capacity hardware or virtualized Citrix ADCs (Citrix ADC MPX and VPX) in the first tier to offload security functions and implement relatively static organizational policies while segmenting control between network operators and Kubernetes operators. In Dual-tier deployments, the second tier is within the Kubernetes Cluster (using the Citrix ADC CPX) and is under control of the service owners. This setup provides stability for network operators, while allowing Kubernetes users to implement high-velocity changes. Single-tier topologies are suited to organizations that need to handle high rates of change.","title":"Deployment topologies"},{"location":"deployment-topologies/#single-tier-topology","text":"In a Single-Tier topology, Citrix ADC MPX or VPX devices proxy the (North-South) traffic from the clients to microservices inside the cluster. The Citrix Ingress Controller (CIC) is deployed as a pod in the Kubernetes cluster. The controller automates the configuration of Citrix ADCs (MPX or VPX) based on the changes to the microservices or the Ingress resources.","title":"Single-Tier topology"},{"location":"deployment-topologies/#dual-tier-topology","text":"In Dual-Tier topology, Citrix ADC MPX or VPX devices in Tier-1 proxy the traffic (North-South) from the client to Citrix ADC CPXs in Tier-2. The Tier-2 Citrix ADC CPX then routes the traffic to the microservices in the Kubernetes cluster. The Citrix Ingress Controller deployed as a standalone pod configures the Tier-1 devices. And, the sidecar controller in one or more Citrix ADC CPX pods configures the associated Citrix ADC CPX in the same pod.","title":"Dual-Tier topology"},{"location":"deployment-topologies/#cloud-topology","text":"Kubernetes clusters in public clouds such as Amazon Web Services (AWS) , Google Cloud , and Microsoft Azure can use their native load balancing services such as, AWS Elastic Load Balancing , Google Cloud Load Balancing , and Microsoft Azure NLB as the first (relatively static) tier of load balancing to a second tier of Citrix ADC CPX. Citrix ADC CPX operates inside the Kubernetes cluster with the sidecar Ingress controller. The Kubernetes clusters can be self-hosted or managed by the cloud provider (for example, AWS EKS , Google GKE and Azure AKS ) while using the Citrix ADC CPX as the Ingress. If the cloud-based Kubernetes cluster is self-hosted or self-managed, the Citrix ADC VPX can be used as the first tier in a Dual-tier topology. Cloud deployment with Citrix ADC (VPX) in tier-1: Cloud deployment with Cloud LB in tier-1:","title":"Cloud topology"},{"location":"deployment-topologies/#using-the-ingress-adc-for-east-west-traffic","text":"When the Citrix ADC CPX is deployed inside the cluster as an Ingress, it can be used to proxy network (East-West) traffic between microservices within the cluster. For this, the target microservice needs to be deployed in headless mode to bypass kube-proxy , so that you can benefit from the advanced ADC functionalities provided by Citrix ADC.","title":"Using the Ingress ADC for East-West traffic"},{"location":"licensing/","text":"Licensing For licensing the Citrix ADC CPX, you need to provide the following information in the YAML for the Citrix Application Delivery Management (ADM) to automatically pick the licensing information: LS_IP (License server IP) \u2013 Specify the Citrix ADM IP address. LS_PORT (License server Port) \u2013 This is not a mandatory field. You must specify the ADM port only if you have changed it. The default port is 27000. PLATFORM \u2013 Specify the Platform License. Platform is CP1000 . The following is a sample yaml file: apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cpx-ingress labels: name: cpx-ingress spec: replicas: 1 selector: matchLabels: name: cpx-ingress template: metadata: labels: name: cpx-ingress annotations: NETSCALER_AS_APP: True spec: serviceAccountName: cpx containers: - name: cpx-ingress image: cpx-ingress:latest securityContext: privileged: true env: - name: EULA value: YES - name: NS_PROTOCOL value: HTTP #Define the NITRO port here - name: NS_PORT value: 9080 - name: LS_IP value: ADM IP - name: LS_PORT value: 27000 - name: PLATFORM value: CP1000 args: - --ingress-classes citrix-ingress ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: nitro-http containerPort: 9080 - name: nitro-https containerPort: 9443 imagePullPolicy: Always","title":"Licensing"},{"location":"licensing/#licensing","text":"For licensing the Citrix ADC CPX, you need to provide the following information in the YAML for the Citrix Application Delivery Management (ADM) to automatically pick the licensing information: LS_IP (License server IP) \u2013 Specify the Citrix ADM IP address. LS_PORT (License server Port) \u2013 This is not a mandatory field. You must specify the ADM port only if you have changed it. The default port is 27000. PLATFORM \u2013 Specify the Platform License. Platform is CP1000 . The following is a sample yaml file: apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cpx-ingress labels: name: cpx-ingress spec: replicas: 1 selector: matchLabels: name: cpx-ingress template: metadata: labels: name: cpx-ingress annotations: NETSCALER_AS_APP: True spec: serviceAccountName: cpx containers: - name: cpx-ingress image: cpx-ingress:latest securityContext: privileged: true env: - name: EULA value: YES - name: NS_PROTOCOL value: HTTP #Define the NITRO port here - name: NS_PORT value: 9080 - name: LS_IP value: ADM IP - name: LS_PORT value: 27000 - name: PLATFORM value: CP1000 args: - --ingress-classes citrix-ingress ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: nitro-http containerPort: 9080 - name: nitro-https containerPort: 9443 imagePullPolicy: Always","title":"Licensing"},{"location":"troubleshooting/","text":"Troubleshooting The following table describes some of the common issues and workarounds. Problem Log Workaround Citrix ADC instance is not reachable 2019-01-10 05:05:27,250 - ERROR - [nitrointerface.py:login_logout:94] (MainThread) Exception: HTTPConnectionPool(host='10.106.76.200', port=80): Max retries exceeded with url: /nitro/v1/config/login (Caused by NewConnectionError(' : Failed to establish a new connection: [Errno 113] No route to host',)) Ensure that Citrix ADC is up and running, and you can ping the NSIP address Wrong user name password 2019-01-10 05:03:05,958 - ERROR - [nitrointerface.py:login_logout:90] (MainThread) Nitro Exception::login_logout::errorcode=354,message=Invalid username or password SNIP is not enabled with management access 2019-01-10 05:43:03,418 - ERROR - [nitrointerface.py:login_logout:94] (MainThread) Exception: HTTPConnectionPool(host='10.106.76.242', port=80): Max retries exceeded with url: /nitro/v1/config/login (Caused by NewConnectionError(' : Failed to establish a new connection: [Errno 110] Connection timed out',)) Ensure that you have enabled the management access in Citrix ADC (for Citrix ADC VPX high availability) and set ip with management access enabled Error while parsing annotations 2019-01-10 05:16:10,611 - ERROR - [kubernetes.py:set_annotations_to_csapp:1040] (MainThread) set_annotations_to_csapp: Error message=No JSON object could be decodedInvalid Annotation $service_weights please fix and apply ${\"frontend\":, \"catalog\":95} Wrong port for NITRO access 2019-01-10 05:18:53,964 - ERROR - [nitrointerface.py:login_logout:94] (MainThread) Exception: HTTPConnectionPool(host='10.106.76.242', port=34438): Max retries exceeded with url: /nitro/v1/config/login (Caused by NewConnectionError(' : Failed to establish a new connection: [Errno 111] Connection refused',)) Verify if the correct port is specified for NITRO access. By default, CIC connects using port 80 Ingress class is wrong 2019-01-10 05:27:27,149 - INFO - [kubernetes.py:get_all_ingresses:1329] (MainThread) Unsupported Ingress class for ingress object web-ingress.default Verify that the ingress file belongs to the ingress class that CIC monitors See the following log for information about the ingress classes listened by CIC Log: 2019-01-10 05:27:27,120 - DEBUG - [kubernetes.py: init :63] (MainThread) Ingress classes allowed: 2019-01-10 05:27:27,120 - DEBUG - [kubernetes.py: init :64] (MainThread) ['vpxclass'] Kubernetes API is not reachable 2019-01-10 05:32:09,729 - ERROR - [kubernetes.py:_get:222] (Thread-1) Error while calling /services:HTTPSConnectionPool(host='10.106.76.237', port=6443): Max retries exceeded with url: /api/v1/services (Caused by NewConnectionError(' : Failed to establish a new connection: [Errno 111] Connection refused',)) Check if the kubernetes_url is correct Use the following command to get the URL information. root@node2:~/cic# kubectl cluster-info Ensure that the Kubernetes master is running at https://10.106.76.236:6443 Ensure that the Kubernetes API server pod is up and running. Incorrect service port specified in the YAML file Provide the correct port details in the ingress YAML file and reapply to solve the issue. Load balancing virtual server and service group are created but they are down Check for the service name and port used in the YAML file. For Citrix ADC VPX, ensure that --feature-node-watch is set to 'true' while bringing up the CIC. CS virtual server is not getting created for Citrix ADC VPX. ingress.citrix.com/frontend-ip Provide annotation in the ingress YAML file for Citrix ADC VPX. Incorrect secret provided in TLS section in the ingress YAML file 2019-01-10 09:30:50,673 - INFO - [kubernetes.py:_get:231] (MainThread) Resource not found: /secrets/default-secret12345 namespace default 2019-01-10 09:30:50,673 - INFO - [kubernetes.py:get_secret:1712] (MainThread) Failed to get secret for the app default-secret12345.default Correct the values in the YAML file and reapply to solve the issue Troubleshooting - Prometheus and Grafana Problem Description Workaround Grafana dashboard has no plots If the graphs on the Grafana dashboards do not have any values plotted, then Grafana is unable to obtain statistics from its datasource. Check if the Prometheus datasource is saved and working properly. On saving the datasource after providing the Name and IP, a \"Data source is working\" message appears in green indicating the datasource is reachable and detected. If the dashboard is created using sample_grafana_dashboard.json , ensure that the name given to the Prometheus datasource begins with the word \"prometheus\" in lowercase letters. Check the Targets page of Prometheus to see if the required target exporter is in DOWN state. DOWN: Context deadline exceeded If the message appears against any of the exporter targets of Prometheus, then Prometheus is either unable to connect to the exporter or unable to fetch all the metrics within the given scrape_timeout . If you are using Prometheus Operator, scrape_timeout is adjusted automatically and the error means that the exporter itself is not reachable. If a standalone Prometheus container or pod is used, try increasing the scrape_interval and scrape_timeout values in the /etc/prometheus/prometheus.cfg file to increase the time interval for collecting the metrics.","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"The following table describes some of the common issues and workarounds. Problem Log Workaround Citrix ADC instance is not reachable 2019-01-10 05:05:27,250 - ERROR - [nitrointerface.py:login_logout:94] (MainThread) Exception: HTTPConnectionPool(host='10.106.76.200', port=80): Max retries exceeded with url: /nitro/v1/config/login (Caused by NewConnectionError(' : Failed to establish a new connection: [Errno 113] No route to host',)) Ensure that Citrix ADC is up and running, and you can ping the NSIP address Wrong user name password 2019-01-10 05:03:05,958 - ERROR - [nitrointerface.py:login_logout:90] (MainThread) Nitro Exception::login_logout::errorcode=354,message=Invalid username or password SNIP is not enabled with management access 2019-01-10 05:43:03,418 - ERROR - [nitrointerface.py:login_logout:94] (MainThread) Exception: HTTPConnectionPool(host='10.106.76.242', port=80): Max retries exceeded with url: /nitro/v1/config/login (Caused by NewConnectionError(' : Failed to establish a new connection: [Errno 110] Connection timed out',)) Ensure that you have enabled the management access in Citrix ADC (for Citrix ADC VPX high availability) and set ip with management access enabled Error while parsing annotations 2019-01-10 05:16:10,611 - ERROR - [kubernetes.py:set_annotations_to_csapp:1040] (MainThread) set_annotations_to_csapp: Error message=No JSON object could be decodedInvalid Annotation $service_weights please fix and apply ${\"frontend\":, \"catalog\":95} Wrong port for NITRO access 2019-01-10 05:18:53,964 - ERROR - [nitrointerface.py:login_logout:94] (MainThread) Exception: HTTPConnectionPool(host='10.106.76.242', port=34438): Max retries exceeded with url: /nitro/v1/config/login (Caused by NewConnectionError(' : Failed to establish a new connection: [Errno 111] Connection refused',)) Verify if the correct port is specified for NITRO access. By default, CIC connects using port 80 Ingress class is wrong 2019-01-10 05:27:27,149 - INFO - [kubernetes.py:get_all_ingresses:1329] (MainThread) Unsupported Ingress class for ingress object web-ingress.default Verify that the ingress file belongs to the ingress class that CIC monitors See the following log for information about the ingress classes listened by CIC Log: 2019-01-10 05:27:27,120 - DEBUG - [kubernetes.py: init :63] (MainThread) Ingress classes allowed: 2019-01-10 05:27:27,120 - DEBUG - [kubernetes.py: init :64] (MainThread) ['vpxclass'] Kubernetes API is not reachable 2019-01-10 05:32:09,729 - ERROR - [kubernetes.py:_get:222] (Thread-1) Error while calling /services:HTTPSConnectionPool(host='10.106.76.237', port=6443): Max retries exceeded with url: /api/v1/services (Caused by NewConnectionError(' : Failed to establish a new connection: [Errno 111] Connection refused',)) Check if the kubernetes_url is correct Use the following command to get the URL information. root@node2:~/cic# kubectl cluster-info Ensure that the Kubernetes master is running at https://10.106.76.236:6443 Ensure that the Kubernetes API server pod is up and running. Incorrect service port specified in the YAML file Provide the correct port details in the ingress YAML file and reapply to solve the issue. Load balancing virtual server and service group are created but they are down Check for the service name and port used in the YAML file. For Citrix ADC VPX, ensure that --feature-node-watch is set to 'true' while bringing up the CIC. CS virtual server is not getting created for Citrix ADC VPX. ingress.citrix.com/frontend-ip Provide annotation in the ingress YAML file for Citrix ADC VPX. Incorrect secret provided in TLS section in the ingress YAML file 2019-01-10 09:30:50,673 - INFO - [kubernetes.py:_get:231] (MainThread) Resource not found: /secrets/default-secret12345 namespace default 2019-01-10 09:30:50,673 - INFO - [kubernetes.py:get_secret:1712] (MainThread) Failed to get secret for the app default-secret12345.default Correct the values in the YAML file and reapply to solve the issue","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting-prometheus-and-grafana","text":"Problem Description Workaround Grafana dashboard has no plots If the graphs on the Grafana dashboards do not have any values plotted, then Grafana is unable to obtain statistics from its datasource. Check if the Prometheus datasource is saved and working properly. On saving the datasource after providing the Name and IP, a \"Data source is working\" message appears in green indicating the datasource is reachable and detected. If the dashboard is created using sample_grafana_dashboard.json , ensure that the name given to the Prometheus datasource begins with the word \"prometheus\" in lowercase letters. Check the Targets page of Prometheus to see if the required target exporter is in DOWN state. DOWN: Context deadline exceeded If the message appears against any of the exporter targets of Prometheus, then Prometheus is either unable to connect to the exporter or unable to fetch all the metrics within the given scrape_timeout . If you are using Prometheus Operator, scrape_timeout is adjusted automatically and the error means that the exporter itself is not reachable. If a standalone Prometheus container or pod is used, try increasing the scrape_interval and scrape_timeout values in the /etc/prometheus/prometheus.cfg file to increase the time interval for collecting the metrics.","title":"Troubleshooting - Prometheus and Grafana"},{"location":"upgrade/","text":"Upgrading Citrix Ingress Controller This topic explains how to upgrade the Citrix Ingress Controller (CIC) instance for Citrix ADC CPX with CIC as sidecar and CIC standalone deployments. Upgrading Citrix ADC CPX with CIC as Sidecar To upgrade a Citrix ADC CPX instance with CIC as sidecar, you can follow the procedure available at: Upgrading a NetScaler CPX Instance . Upgrading a Standalone CIC To upgrade a standalone CIC instance, you can either modify the YAML definition file or use the Helm chart. In the YAML file, you need to change the version for the image under containers. For example, consider you have the following YAML file. apiVersion: v1 kind: Pod metadata: name: cic-k8s-ingress-controller labels: app: ... spec: serviceAccountName: ... containers: - name: cic-k8s-ingress-controller image: quay.io/citrix/citrix-k8s-ingress-controller:0.1.0 env: ... args: ... You should change the version of the image to the required version. For example, quay.io/citrix/citrix-k8s-ingress-controller:0.18.0 . After updating the YAML file, you can use one of the following ways to upgrade the CIC image: Using the kubectl edit command, type kubectl edit Deployment cic-k8s-ingress-controller . This command enables you to upgrade the CIC image after you save the changes. Using the kubectl set command, type kubectl set image Deployment/cic-k8s-ingress-controller cic-k8s-ingress-controller=quay.io/citrix/citrix-k8s-ingress-controller:0.18.0 This command enables you to upgrade the CIC image.","title":"Upgrade"},{"location":"upgrade/#upgrading-citrix-ingress-controller","text":"This topic explains how to upgrade the Citrix Ingress Controller (CIC) instance for Citrix ADC CPX with CIC as sidecar and CIC standalone deployments.","title":"Upgrading Citrix Ingress Controller"},{"location":"upgrade/#upgrading-citrix-adc-cpx-with-cic-as-sidecar","text":"To upgrade a Citrix ADC CPX instance with CIC as sidecar, you can follow the procedure available at: Upgrading a NetScaler CPX Instance .","title":"Upgrading Citrix ADC CPX with CIC as Sidecar"},{"location":"upgrade/#upgrading-a-standalone-cic","text":"To upgrade a standalone CIC instance, you can either modify the YAML definition file or use the Helm chart. In the YAML file, you need to change the version for the image under containers. For example, consider you have the following YAML file. apiVersion: v1 kind: Pod metadata: name: cic-k8s-ingress-controller labels: app: ... spec: serviceAccountName: ... containers: - name: cic-k8s-ingress-controller image: quay.io/citrix/citrix-k8s-ingress-controller:0.1.0 env: ... args: ... You should change the version of the image to the required version. For example, quay.io/citrix/citrix-k8s-ingress-controller:0.18.0 . After updating the YAML file, you can use one of the following ways to upgrade the CIC image: Using the kubectl edit command, type kubectl edit Deployment cic-k8s-ingress-controller . This command enables you to upgrade the CIC image after you save the changes. Using the kubectl set command, type kubectl set image Deployment/cic-k8s-ingress-controller cic-k8s-ingress-controller=quay.io/citrix/citrix-k8s-ingress-controller:0.18.0 This command enables you to upgrade the CIC image.","title":"Upgrading a Standalone CIC"},{"location":"certificate-management/acme/","text":"Deploying HTTPS web application on Kubernetes with Citrix Ingress Controller and Let`s Encrypt using cert-manager Let's Encrypt and the ACME (Automatic Certificate Management Environment) protocol enables you to set up an HTTPS server and automatically obtain a browser-trusted certificate. To get a certificate for your website\u2019s domain from Let\u2019s Encrypt, you have to demonstrate control over the domain. Currently there are two different challenge types, http-01 and dns-01. A challenge is one of a list of specified tasks that only someone who controls the domain should be able to accomplish, such as: HTTP-01 challenge: Posting a specified file in a specified location on a web site (the HTTP-01 challenge). Let's Encrypt CA verifies the file by making an HTTP request on the HTTP URI to satisfy the challenge. DNS-01 challenge: Posting a specified DNS TXT record in the domain name system. Let's Encrypt requests your domain's DNS servers for the value of the TXT record to satisfy the challenge. On successful validation of the challenge, a certificate is granted for the domain. This topic provides information on how to securely deploy an HTTPS web application on a Kubernetes cluster, using: Citrix Ingress Controller (CIC) JetStack's cert-manager to provision TLS certificates from the Let's Encrypt project . Prerequisites Ensure that you have: Enabled RBAC on your Kubernetes cluster. Deployed Citrix ADC MPX, VPX, or CPX deployed in Tier 1 or Tier 2 deployment model. In Tier 1 deployment model, Citrix ADC MPX or VPX is used as an Application Delivery Controller (ADC) and Citrix Ingress Controller (CIC) running in kubernetes cluster configures the virtual services for the services running on kubernetes cluster. Citrix ADC runs the virtual service on the publicly routable IP address and offloads SSL for client traffic with the help of Let's Encrypt generated certificate. Similarly in Tier 2 deployment model, a TCP service is configured on the Citrix ADC (VPX/MPX) running outside the Kubernetes cluster to forward the traffic to Citrix ADC CPX instances running in kubernetes cluster. Citrix ADC CPX ends the SSL session and load-balances the traffic to actual service pods. Deployed Citrix ingress controller. Click here for various deployment scenarios. Opened Port 80 for the Virtual IP address on the firewall for the Let's Encrypt CA to validate the domain for HTTP01 challenge. A DNS domain that you control, where you host your web application for ACME DNS01 challenge. Administrator permissions for all the deployment steps. If you encounter failures due to permissions, make sure you have administrator permission. Deploy cert-manager using the manifest file To keep things simple, let's skip cert-manager's Helm installation, and instead use the supplied YAML manifests. Download the latest source of cert-manager from github.com/jetstack/cert-manager repository using the following command: wget https://github.com/jetstack/cert-manager/archive/v0.6.2.tar.gz tar -zxvf v0.6.2.tar.gz Then deploy the cert-manager using the following command: kubectl apply -f deploy/manifests/cert-manager.yaml Alternatively, you can also install the cert-manager with Helm, for more information see cert-manager documentation Verify in the cert-manager is up and running using the following command: % kubectl -n cert-manager get all NAME READY STATUS RESTARTS AGE pod/cert-manager-77fd74fb64-d68v7 1/1 Running 0 4m41s pod/cert-manager-webhook-67bf86d45-k77jj 1/1 Running 0 4m41s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cert-manager-webhook ClusterIP 10.108.161.154 none 443/TCP 13d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cert-manager 1/1 1 1 13d deployment.apps/cert-manager-webhook 1/1 1 1 13d NAME DESIRED CURRENT READY AGE replicaset.apps/cert-manager-77fd74fb64 1 1 1 13d replicaset.apps/cert-manager-webhook-67bf86d45 1 1 1 13d NAME COMPLETIONS DURATION AGE job.batch/cert-manager-webhook-ca-sync 1/1 22s 13d job.batch/cert-manager-webhook-ca-sync-1549756800 1/1 21s 10d job.batch/cert-manager-webhook-ca-sync-1550361600 1/1 19s 3d8h NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE cronjob.batch/cert-manager-webhook-ca-sync @weekly False 0 3d8h 13d Deploy a sample web application Perform the following to deploy a sample web application: Note Kuard , a kubernetes demo application is used for reference in this topic. Create a deployment YAML file ( kuard-deployment.yaml ) for Kuard with the following configuration: apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kuard spec: replicas: 1 template: metadata: labels: app: kuard spec: containers: - image: gcr.io/kuar-demo/kuard-amd64:1 imagePullPolicy: Always name: kuard ports: - containerPort: 8080 Deploy Kuard deployment file ( kuard-deployment.yaml ) to your cluster, using the following commands: % kubectl create -f kuard-deployment.yaml deployment.extensions/kuard created % kubectl get pod -l app=kuard NAME READY STATUS RESTARTS AGE kuard-6fc4d89bfb-djljt 1/1 Running 0 24s Create a service for the deployment. Create a file called service.yaml with the following configuration: apiVersion: v1 kind: Service metadata: name: kuard spec: ports: - port: 80 targetPort: 8080 protocol: TCP selector: app: kuard Deploy and verify the service using the following commands: % kubectl create -f service.yaml service/kuard created % kubectl get svc kuard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kuard ClusterIP 10.103.49.171 none 80/TCP 13s Expose this service to outside world by creating and Ingress that is deployed on Citrix ADC CPX or VPX as Content switching virtual server. Note Ensure that you change kubernetes.io/ingress.class to your ingress class on which CIC is started. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kuard annotations: kubernetes.io/ingress.class: citrix spec: rules: - host: kuard.example.com http: paths: - backend: serviceName: kuard servicePort: 80 Important Change the value of spec.rules.host to the domain that you control. Ensure that a DNS entry exists to route the traffic to Citrix ADC CPX or VPX. Deploy the Ingress using the following command: % kubectl apply -f ingress.yml ingress.extensions/kuard created root@ubuntu-vivek-225:~/cert-manager# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE kuard kuard.example.com 80 7s Verify if the ingress is configured on Citrix ADC CPX or VPX using the following command: $ kubectl exec -it cpx-ingress-5b85d7c69d-ngd72 /bin/bash root@cpx-ingress-5b85d7c69d-ngd72:/# cli_script.sh 'sh cs vs' exec: sh cs vs 1) k8s-10.244.1.50:80:http (10.244.1.50:80) - HTTP Type: CONTENT State: UP Last state change was at Thu Feb 21 09:02:14 2019 Time since last state change: 0 days, 00:00:41.140 Client Idle Timeout: 180 sec Down state flush: ENABLED Disable Primary Vserver On Down : DISABLED Comment: uid=75VBGFO7NZXV7SCI4LSDJML2Q5X6FSNK6NXQPWGMDOYGBW2IMOGQ==== Appflow logging: ENABLED Port Rewrite : DISABLED State Update: DISABLED Default: Content Precedence: RULE Vserver IP and Port insertion: OFF L2Conn: OFF Case Sensitivity: ON Authentication: OFF 401 Based Authentication: OFF Push: DISABLED Push VServer: Push Label Rule: none Listen Policy: NONE IcmpResponse: PASSIVE RHIstate: PASSIVE Traffic Domain: 0 Done root@cpx-ingress-5b85d7c69d-ngd72:/# exit exit Verify if the page is correctly being served when requested using the curl command. % curl -sS -D - kuard.example.com -o /dev/null HTTP/1.1 200 OK Content-Length: 1458 Content-Type: text/html Date: Thu, 21 Feb 2019 09:09:05 GMT Configure issuing ACME certificate using HTTP challenge This section describes a way to issue ACME certificate using HTTP validation. If you want to use DNS validation, skip this section and proceed to the next section . HTTP validation using cert-manager is simple way of getting a certificate from Let's Encrypt for your domain, wherein you prove ownership of a domain by ensuring that a particular file is present at the domain. It is assumed that you control the domain if you are able to publish the given file under a given path. Deploy the Let's Encrypt cluster issuer with http01 challenge provider The cert-manager supports two different CRDs for configuration, an Issuer , which is scoped to a single namespace, and a ClusterIssuer , which is cluster-wide. For CIC to use ingress from any namespace, use ClusterIssuer . Alternatively you can create an Issuer for each namespace on which you are creating an Ingress resource. Create a file called issuer-letsencrypt-staging.yaml with the following configuration: apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # Enable the HTTP-01 challenge provider http01: {} Note http01 challenge provider is enabled in the ClusterIssuer CRD. Replace user@example.com with your email address. This is the email address that Let's Encrypt uses to communicate with you about certificates you request. For more information, see Issuer reference docs . The staging Let's Encrypt server issues fake certificate, but it is not bound by the API rate limits of the production server . This approach lets you set up and test your environment without worrying about rate limits. You can repeat the same step for Let's Encrypt Production server. After you edit and save the file, deploy the file using the following command: % kubectl apply -f issuer-letsencrypt-staging.yaml clusterissuer letsencrypt-staging created Verify in the issuer is created and registered to the ACME server. % kubectl get issuer NAME AGE letsencrypt-staging 8d Verify if the ClusterIssuer is properly registered using the command kubectl describe issuer letsencrypt-staging : Status: Acme: Uri: https://acme-staging-v02.api.letsencrypt.org/acme/acct/8200869 Conditions: Last Transition Time: 2019-02-11T12:06:31Z Message: The ACME account was registered with the ACME server Reason: ACMEAccountRegistered Status: True Type: Ready Issue certificate for ingress object Once the issuer is successfully registered, now lets proceed to get certificate for the ingress domain 'kuard.example.com' You can request certificate for a given ingress resource using the following methods: Adding Ingress-shim annotations to the ingress object. Creating a certificate CRD object. First method is quick and simple, but if you need more customization and granularity in terms of certificate renewal, you can choose the second method. Depending on your selection, skip the other method. Adding Ingress-shim annotations to Ingress object In this approach, we'll add these two annotations to ingress object for which you request certificate to be issued by the ACME server. kubernetes.io/tls-acme: true certmanager.k8s.io/cluster-issuer: letsencrypt-staging Note You can find all supported annotations from cert-manager for ingress-shim, click here . Also, modify the ingress.yaml to use TLS by specifying a secret. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kuard annotations: kubernetes.io/ingress.class: citrix kubernetes.io/tls-acme: true certmanager.k8s.io/cluster-issuer: letsencrypt-staging spec: tls: - hosts: - kuard.example.com secretName: kuard-example-tls rules: - host: kuard.example.com http: paths: - backend: serviceName: kuard servicePort: 80 The kubernetes.io/tls-acme: \"true\" annotation tells cert-manager to use the letsencrypt-staging cluster-wide issuer that was created earlier to request a certificate from Let's Encrypt's staging servers. Cert-manager creates a certificate object that is used to manage the lifecycle of the certificate for kuard.example.com , and the value for the domain name and challenge method for the certificate object is derived from the ingress object. Cert-manager manages the contents of the secret as long as the Ingress is present in your cluster. Deploy the ingress.yaml using the following command: % kubectl apply -f ingress.yml ingress.extensions/kuard configured % kubectl get ingress kuard NAME HOSTS ADDRESS PORTS AGE kuard kuard.example.com 80, 443 4h39m Create a Certificate CRD resource Alternatively, you can deploy a certificate CRD object independent of ingress object. Documentation of \"certificate\" CRD can be found here. Create a file with certificate.yaml with the following configuration: apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: kuard-example-tls namespace: default spec: secretName: kuard-exmaple-tls issuerRef: name: letsencrypt-staging commonName: kuard.example.com #Renew before 15 days of expiry renewBefore: 360h dnsNames: - kuard.example.com acme: config: - http01: ingressClass: citrix domains: - kuard.example.com ingressClass refers to the ingress class CIC or CPX is running and spec.secretName is the name of the secret where the certificate is stored on successful issuing the certificate. Deploy the certificate.yaml on the Kubernetes cluster: kubectl create -f certificate.yaml certificate.certmanager.k8s.io/kuard-example-tls created Issuing an ACME certificate using DNS challenge This section describes a way to use DNS validation to get ACME certificate from Let'sEncrypt CA. With a DNS-01 challenge, you prove the ownership of a domain by proving you control its DNS records. This is done by creating a TXT record with specific content that proves you have control of the domain's DNS records. For detailed explanation of DNS challenge and best security practices in deploying DNS challenge, see A Technical Deep Dive: Securing the Automation of ACME DNS Challenge Validation . Deploy the Let's Encrypt cluster issuer with dns01 challenge provider Create an Issuer or ClusterIssuer with dns01 challenge provider. You can provide multiple providers under dns01, and specify which provider to be used at the time of certificate creation. You need to have access to the DNS provider for cert-manager to create a TXT record, the credentials are stored in Kubernetes secret specified in spec.dns01.secretAccessKeySecretRef . For detailed instructions on how to obtain the credentials, see the DNS provider documentation. apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # Enable the DNS-01 challenge provider dns01: providers: - name: dns route53: region: us-east-1 hostedZoneID: YOURZONEID accessKeyID: YOURACCESSKEYID secretAccessKeySecretRef: name: acme-route53 key: secret-access-key Note Replace user@example.com with your email address. For each domain mentioned in a dns01 stanza, cert-manager will use the provider's credentials from the referenced Issuer to create a TXT record called _acme-challenge . This record is then verified by the ACME server in order to issue the certificate. For more information about the DNS provider configuration, and the list of supported providers, see dns01 reference doc . After you edit and save the file, deploy the file using the following command: % kubectl apply -f issuer-letsencrypt-staging.yaml clusterissuer letsencrypt-staging created Verify if the issuer is created and registered to the ACME server using the following command: % kubectl get issuer NAME AGE letsencrypt-staging 8d Verify if the ClusterIssuer is properly registered using the command kubectl describe issuer letsencrypt-staging : Status: Acme: Uri: https://acme-staging-v02.api.letsencrypt.org/acme/acct/8200869 Conditions: Last Transition Time: 2019-02-11T12:06:31Z Message: The ACME account was registered with the ACME server Reason: ACMEAccountRegistered Status: True Type: Ready Issue certificate for ingress object Once the issuer is successfully registered, lets proceed to get certificate for the ingress domain kuard.example.com . Similar to http01 challenge, there are two ways you can request the certificate for a given ingress resource: Adding Ingress-shim annotations to the ingress object. Creating a certificate CRD object. For detailed instructions, see Create a Certificate CRD resource Adding Ingress-shim annotations to the ingress object Add the following annotations to the ingress object along with spec.tls section: kubernetes.io/tls-acme: true certmanager.k8s.io/cluster-issuer: letsencrypt-staging certmanager.k8s.io/acme-challenge-type: dns01 certmanager.k8s.io/acme-dns01-provider: dns apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kuard annotations: kubernetes.io/ingress.class: citrix kubernetes.io/tls-acme: true certmanager.k8s.io/cluster-issuer: letsencrypt-staging certmanager.k8s.io/acme-challenge-type: dns01 certmanager.k8s.io/acme-dns01-provider: dns spec: tls: - hosts: - kuard.example.com secretName: kuard-example-tls rules: - host: kuard.example.com http: paths: - backend: serviceName: kuard servicePort: 80 The cert-manager creates a Certificate CRD resource with dns01 challenge and it uses the credentials given in the ClusterIssuer to create a TXT record in the DNS server for the domain you own. Then, Let's Encypt CA validates the content of the TXT record to complete the challenge. Verify if the certificate is issued For HTTP challenge, cert-manager will create a temporary ingress resource to route the Let's Encrypt CA generated traffic to cert-manager pods. On successful validations of the domain, this temporary ingress is deleted. You can watch the progress of the certificate as it's issued, use the following command: % kubectl describe certificates kuard-example-tls | tail -n 6 Type Reason Age From Message ---- ------ ---- ---- ------- Normal Generated 25m cert-manager Generated new private key Normal OrderCreated 14m (x2 over 25m) cert-manager Created Order resource kuard-example-tls-1006173429 Normal OrderComplete 13m (x2 over 25m) cert-manager Order kuard-example-tls-1006173429 completed successfully Normal CertIssued 13m (x2 over 25m) cert-manager Certificate issued successfully Letsencrypt CA successfully validated the domain and issued a new certificate for the domain. A kubernetes.io/tls secret is created with the secretName specified in the tls: field of the Ingress. Also, cert-manager automatically initiates a renewal, 30 days before the expiry. Verify in the secret is created using the following command: % kubectl get secret kuard-example-tls NAME TYPE DATA AGE kuard-example-tls kubernetes.io/tls 3 30m The secret is picked up by Citrix ingress controller and binds the certificate to the Content switching virtual server on the Citrix ADC CPX. Log on to Citrix ADC CPX and verify if the certificate is bound to the SSL virtual server. kubectl exec -it cpx-ingress-668bf6695f-4fwh8 bash root@cpx-ingress-668bf6695f-4fwh8:/# cli_script.sh 'sh ssl vserver' exec: sh ssl vserver 1) Vserver Name: k8s-10.244.3.148:443:ssl DH: DISABLED DH Private-Key Exponent Size Limit: DISABLED Ephemeral RSA: ENABLED Refresh Count: 0 Session Reuse: ENABLED Timeout: 120 seconds Cipher Redirect: DISABLED SSLv2 Redirect: DISABLED ClearText Port: 0 Client Auth: DISABLED SSL Redirect: DISABLED Non FIPS Ciphers: DISABLED SNI: ENABLED OCSP Stapling: DISABLED HSTS: DISABLED HSTS IncludeSubDomains: NO HSTS Max-Age: 0 SSLv2: DISABLED SSLv3: ENABLED TLSv1.0: ENABLED TLSv1.1: ENABLED TLSv1.2: ENABLED TLSv1.3: DISABLED Push Encryption Trigger: Always Send Close-Notify: YES Strict Sig-Digest Check: DISABLED Zero RTT Early Data: DISABLED DHE Key Exchange With PSK: NO Tickets Per Authentication Context: 1 Done root@cpx-ingress-668bf6695f-4fwh8:/# cli_script.sh 'sh ssl vserver k8s-10.244.3.148:443:ssl' exec: sh ssl vserver k8s-10.244.3.148:443:ssl Advanced SSL configuration for VServer k8s-10.244.3.148:443:ssl: DH: DISABLED DH Private-Key Exponent Size Limit: DISABLED Ephemeral RSA: ENABLED Refresh Count: 0 Session Reuse: ENABLED Timeout: 120 seconds Cipher Redirect: DISABLED SSLv2 Redirect: DISABLED ClearText Port: 0 Client Auth: DISABLED SSL Redirect: DISABLED Non FIPS Ciphers: DISABLED SNI: ENABLED OCSP Stapling: DISABLED HSTS: DISABLED HSTS IncludeSubDomains: NO HSTS Max-Age: 0 SSLv2: DISABLED SSLv3: ENABLED TLSv1.0: ENABLED TLSv1.1: ENABLED TLSv1.2: ENABLED TLSv1.3: DISABLED Push Encryption Trigger: Always Send Close-Notify: YES Strict Sig-Digest Check: DISABLED Zero RTT Early Data: DISABLED DHE Key Exchange With PSK: NO Tickets Per Authentication Context: 1 , P_256, P_384, P_224, P_5216) CertKey Name: k8s-VN4RXHGMCZSTMQZOQ3XGBVMM2OO Server Certificate for SNI 7) Cipher Name: DEFAULT Description: Default cipher list with encryption strength = 128bit Done rcli_script.sh 'sh certkey k8s-VN4RXHGMCZSTMQZOQ3XGBVMM2OO' exec: sh certkey k8s-VN4RXHGMCZSTMQZOQ3XGBVMM2OO Name: k8s-VN4RXHGMCZSTMQZOQ3XGBVMM2OO Status: Valid, Days to expiration:89 Version: 3 Serial Number: FA0DFEDAB578C0228273927DA7C5E17CF098 Signature Algorithm: sha256WithRSAEncryption Issuer: CN=Fake LE Intermediate X1 Validity Not Before: Feb 25 08:00:38 2019 GMT Not After : May 26 08:00:38 2019 GMT Certificate Type: Client Certificate Server Certificate Subject: CN=kuard.example.com Public Key Algorithm: rsaEncryption Public Key size: 2048 Ocsp Response Status: NONE 2) VServer name: k8s-10.244.3.148:443:ssl Server Certificate for SNI Done The HTTPS webserver is now UP with fake LE signed certificate. Next step is to move to production with actual Let's Encrypt certificate. Move to production After successfully testing with Let's Encrypt-staging, you can get the actual Let's Encrypt certificates. You need to change Let's Encrypt endpoint from https:acme-staging-v02.api.letsencrypt.org/directory to https:acme-v02.api.letsencrypt.org/directory Then, change the name of the ClusterIssuer from letsencrypt-staging to letsencrypt-production apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prod namespace: cert-manager spec: acme: email: user@example.com http01: {} privateKeySecretRef: name: letsencrypt-prod server: https://acme-v02.api.letsencrypt.org/directory Note Replace user@example.com with your email address. Deploy the file using the following command: % kubectl apply -f letsencrypt-prod.yaml clusterissuer letsencrypt-prod created Now repeat the procedure of modifying the annotation in ingress or creating a new CRD certificate which will trigger the generation of new certificate. Note Ensure that you delete the old secret so that cert-manager starts a fresh challenge with the production CA. % kubectl delete secret kuard-example-tls secret \"kuard-example-tls\" deleted Once the HTTP website is up, you can redirect the traffic from HTTP to HTTPS using the annotation ingress.citrix.com/insecure-termination: redirect in the ingress object. Troubleshooting Since the certificate generation involves multiple components, this section summarizes the troubleshooting techniques that you can use in case of failures. Verify the current status of certificate generation Certificate CRD object defines the life cycle management of generation and renewal of the certificates. You can view the status of the certificate using kubectl describe command as shown below. % kubectl get certificate NAME READY SECRET AGE kuard-example-tls False kuard-example-tls 9s % kubectl describe certificate kuard-example-tls Status: Conditions: Last Transition Time: 2019-03-05T09:50:29Z Message: Certificate does not exist Reason: NotFound Status: False Type: Ready Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal OrderCreated 22s cert-manager Created Order resource kuard-example-tls-1754626579 Also you can view the major certificate events using the kubectl events commands: kubectl get events LAST SEEN TYPE REASON KIND MESSAGE 36s Normal Started Challenge Challenge scheduled for processing 36s Normal Created Order Created Challenge resource kuard-example-tls-1754626579-0 for domain acme.cloudpst.net 38s Normal OrderCreated Certificate Created Order resource kuard-example-tls-1754626579 38s Normal CreateCertificate Ingress Successfully created Certificate kuard-example-tls Analyze the logs from cert-manager In case of failure, first step is to analyze the logs from the cert-manager component. Identify the cert-manager pod using the following command: kubectl get po -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-76d48d47bf-5w4vx 1/1 Running 0 23h cert-manager-webhook-67cfb86d56-6qtxr 1/1 Running 0 23h cert-manager-webhook-ca-sync-x4q6f 0/1 Completed 4 23h Here cert-manager-76d48d47bf-5w4vx is the main cert-manager pod, and other two pods are cert-manager webhook pods. Get the logs of the cert-manager using the following command: kubectl logs -f cert-manager-76d48d47bf-5w4vx -n cert-manager If there is any failure to get the certificate, the ERROR logs give details about the failure. Check the Kubernetes secret Use kubectl describe command to verify if both certificates and key are populated in Kubernetes secret. % kubectl describe secret kuard-example-tls Name: kuard-example-tls Namespace: default Labels: certmanager.k8s.io/certificate-name=kuard-example-tls Annotations: certmanager.k8s.io/alt-names: acme.cloudpst.net certmanager.k8s.io/common-name: acme.cloudpst.net certmanager.k8s.io/issuer-kind: ClusterIssuer certmanager.k8s.io/issuer-name: letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3553 bytes tls.key: 1679 bytes ca.crt: 0 bytes If both tls.crt and tls.key are populated in the kubernetes secret, certificate generation is complete. If only tls.key is present, certificate generation is incomplete. Analyze the cert-manager logs for more details about the issue. Analyze the logs from Citrix Ingress Controller If kubernetes secret is generated and complete, but this secret is not uploaded to Citrix ADC CPX or VPX, you can analyze the logs from citrix ingress controller using kubectl logs command. % kubectl logs -f cpx-ingress-685c8bc976-zgz8q","title":"Deploy HTTPs web applications on K8s with CIC and Let\u2019s Encrypt using cert-manager"},{"location":"certificate-management/acme/#deploying-https-web-application-on-kubernetes-with-citrix-ingress-controller-and-lets-encrypt-using-cert-manager","text":"Let's Encrypt and the ACME (Automatic Certificate Management Environment) protocol enables you to set up an HTTPS server and automatically obtain a browser-trusted certificate. To get a certificate for your website\u2019s domain from Let\u2019s Encrypt, you have to demonstrate control over the domain. Currently there are two different challenge types, http-01 and dns-01. A challenge is one of a list of specified tasks that only someone who controls the domain should be able to accomplish, such as: HTTP-01 challenge: Posting a specified file in a specified location on a web site (the HTTP-01 challenge). Let's Encrypt CA verifies the file by making an HTTP request on the HTTP URI to satisfy the challenge. DNS-01 challenge: Posting a specified DNS TXT record in the domain name system. Let's Encrypt requests your domain's DNS servers for the value of the TXT record to satisfy the challenge. On successful validation of the challenge, a certificate is granted for the domain. This topic provides information on how to securely deploy an HTTPS web application on a Kubernetes cluster, using: Citrix Ingress Controller (CIC) JetStack's cert-manager to provision TLS certificates from the Let's Encrypt project .","title":"Deploying HTTPS web application on Kubernetes with Citrix Ingress Controller and Let`s Encrypt using cert-manager"},{"location":"certificate-management/acme/#prerequisites","text":"Ensure that you have: Enabled RBAC on your Kubernetes cluster. Deployed Citrix ADC MPX, VPX, or CPX deployed in Tier 1 or Tier 2 deployment model. In Tier 1 deployment model, Citrix ADC MPX or VPX is used as an Application Delivery Controller (ADC) and Citrix Ingress Controller (CIC) running in kubernetes cluster configures the virtual services for the services running on kubernetes cluster. Citrix ADC runs the virtual service on the publicly routable IP address and offloads SSL for client traffic with the help of Let's Encrypt generated certificate. Similarly in Tier 2 deployment model, a TCP service is configured on the Citrix ADC (VPX/MPX) running outside the Kubernetes cluster to forward the traffic to Citrix ADC CPX instances running in kubernetes cluster. Citrix ADC CPX ends the SSL session and load-balances the traffic to actual service pods. Deployed Citrix ingress controller. Click here for various deployment scenarios. Opened Port 80 for the Virtual IP address on the firewall for the Let's Encrypt CA to validate the domain for HTTP01 challenge. A DNS domain that you control, where you host your web application for ACME DNS01 challenge. Administrator permissions for all the deployment steps. If you encounter failures due to permissions, make sure you have administrator permission.","title":"Prerequisites"},{"location":"certificate-management/acme/#deploy-cert-manager-using-the-manifest-file","text":"To keep things simple, let's skip cert-manager's Helm installation, and instead use the supplied YAML manifests. Download the latest source of cert-manager from github.com/jetstack/cert-manager repository using the following command: wget https://github.com/jetstack/cert-manager/archive/v0.6.2.tar.gz tar -zxvf v0.6.2.tar.gz Then deploy the cert-manager using the following command: kubectl apply -f deploy/manifests/cert-manager.yaml Alternatively, you can also install the cert-manager with Helm, for more information see cert-manager documentation Verify in the cert-manager is up and running using the following command: % kubectl -n cert-manager get all NAME READY STATUS RESTARTS AGE pod/cert-manager-77fd74fb64-d68v7 1/1 Running 0 4m41s pod/cert-manager-webhook-67bf86d45-k77jj 1/1 Running 0 4m41s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cert-manager-webhook ClusterIP 10.108.161.154 none 443/TCP 13d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cert-manager 1/1 1 1 13d deployment.apps/cert-manager-webhook 1/1 1 1 13d NAME DESIRED CURRENT READY AGE replicaset.apps/cert-manager-77fd74fb64 1 1 1 13d replicaset.apps/cert-manager-webhook-67bf86d45 1 1 1 13d NAME COMPLETIONS DURATION AGE job.batch/cert-manager-webhook-ca-sync 1/1 22s 13d job.batch/cert-manager-webhook-ca-sync-1549756800 1/1 21s 10d job.batch/cert-manager-webhook-ca-sync-1550361600 1/1 19s 3d8h NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE cronjob.batch/cert-manager-webhook-ca-sync @weekly False 0 3d8h 13d","title":"Deploy cert-manager using the manifest file"},{"location":"certificate-management/acme/#deploy-a-sample-web-application","text":"Perform the following to deploy a sample web application: Note Kuard , a kubernetes demo application is used for reference in this topic. Create a deployment YAML file ( kuard-deployment.yaml ) for Kuard with the following configuration: apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kuard spec: replicas: 1 template: metadata: labels: app: kuard spec: containers: - image: gcr.io/kuar-demo/kuard-amd64:1 imagePullPolicy: Always name: kuard ports: - containerPort: 8080 Deploy Kuard deployment file ( kuard-deployment.yaml ) to your cluster, using the following commands: % kubectl create -f kuard-deployment.yaml deployment.extensions/kuard created % kubectl get pod -l app=kuard NAME READY STATUS RESTARTS AGE kuard-6fc4d89bfb-djljt 1/1 Running 0 24s Create a service for the deployment. Create a file called service.yaml with the following configuration: apiVersion: v1 kind: Service metadata: name: kuard spec: ports: - port: 80 targetPort: 8080 protocol: TCP selector: app: kuard Deploy and verify the service using the following commands: % kubectl create -f service.yaml service/kuard created % kubectl get svc kuard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kuard ClusterIP 10.103.49.171 none 80/TCP 13s Expose this service to outside world by creating and Ingress that is deployed on Citrix ADC CPX or VPX as Content switching virtual server. Note Ensure that you change kubernetes.io/ingress.class to your ingress class on which CIC is started. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kuard annotations: kubernetes.io/ingress.class: citrix spec: rules: - host: kuard.example.com http: paths: - backend: serviceName: kuard servicePort: 80 Important Change the value of spec.rules.host to the domain that you control. Ensure that a DNS entry exists to route the traffic to Citrix ADC CPX or VPX. Deploy the Ingress using the following command: % kubectl apply -f ingress.yml ingress.extensions/kuard created root@ubuntu-vivek-225:~/cert-manager# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE kuard kuard.example.com 80 7s Verify if the ingress is configured on Citrix ADC CPX or VPX using the following command: $ kubectl exec -it cpx-ingress-5b85d7c69d-ngd72 /bin/bash root@cpx-ingress-5b85d7c69d-ngd72:/# cli_script.sh 'sh cs vs' exec: sh cs vs 1) k8s-10.244.1.50:80:http (10.244.1.50:80) - HTTP Type: CONTENT State: UP Last state change was at Thu Feb 21 09:02:14 2019 Time since last state change: 0 days, 00:00:41.140 Client Idle Timeout: 180 sec Down state flush: ENABLED Disable Primary Vserver On Down : DISABLED Comment: uid=75VBGFO7NZXV7SCI4LSDJML2Q5X6FSNK6NXQPWGMDOYGBW2IMOGQ==== Appflow logging: ENABLED Port Rewrite : DISABLED State Update: DISABLED Default: Content Precedence: RULE Vserver IP and Port insertion: OFF L2Conn: OFF Case Sensitivity: ON Authentication: OFF 401 Based Authentication: OFF Push: DISABLED Push VServer: Push Label Rule: none Listen Policy: NONE IcmpResponse: PASSIVE RHIstate: PASSIVE Traffic Domain: 0 Done root@cpx-ingress-5b85d7c69d-ngd72:/# exit exit Verify if the page is correctly being served when requested using the curl command. % curl -sS -D - kuard.example.com -o /dev/null HTTP/1.1 200 OK Content-Length: 1458 Content-Type: text/html Date: Thu, 21 Feb 2019 09:09:05 GMT","title":"Deploy a sample web application"},{"location":"certificate-management/acme/#configure-issuing-acme-certificate-using-http-challenge","text":"This section describes a way to issue ACME certificate using HTTP validation. If you want to use DNS validation, skip this section and proceed to the next section . HTTP validation using cert-manager is simple way of getting a certificate from Let's Encrypt for your domain, wherein you prove ownership of a domain by ensuring that a particular file is present at the domain. It is assumed that you control the domain if you are able to publish the given file under a given path.","title":"Configure issuing ACME certificate using HTTP challenge"},{"location":"certificate-management/acme/#deploy-the-lets-encrypt-cluster-issuer-with-http01-challenge-provider","text":"The cert-manager supports two different CRDs for configuration, an Issuer , which is scoped to a single namespace, and a ClusterIssuer , which is cluster-wide. For CIC to use ingress from any namespace, use ClusterIssuer . Alternatively you can create an Issuer for each namespace on which you are creating an Ingress resource. Create a file called issuer-letsencrypt-staging.yaml with the following configuration: apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # Enable the HTTP-01 challenge provider http01: {} Note http01 challenge provider is enabled in the ClusterIssuer CRD. Replace user@example.com with your email address. This is the email address that Let's Encrypt uses to communicate with you about certificates you request. For more information, see Issuer reference docs . The staging Let's Encrypt server issues fake certificate, but it is not bound by the API rate limits of the production server . This approach lets you set up and test your environment without worrying about rate limits. You can repeat the same step for Let's Encrypt Production server. After you edit and save the file, deploy the file using the following command: % kubectl apply -f issuer-letsencrypt-staging.yaml clusterissuer letsencrypt-staging created Verify in the issuer is created and registered to the ACME server. % kubectl get issuer NAME AGE letsencrypt-staging 8d Verify if the ClusterIssuer is properly registered using the command kubectl describe issuer letsencrypt-staging : Status: Acme: Uri: https://acme-staging-v02.api.letsencrypt.org/acme/acct/8200869 Conditions: Last Transition Time: 2019-02-11T12:06:31Z Message: The ACME account was registered with the ACME server Reason: ACMEAccountRegistered Status: True Type: Ready","title":"Deploy the Let's Encrypt cluster issuer with http01 challenge provider"},{"location":"certificate-management/acme/#issue-certificate-for-ingress-object","text":"Once the issuer is successfully registered, now lets proceed to get certificate for the ingress domain 'kuard.example.com' You can request certificate for a given ingress resource using the following methods: Adding Ingress-shim annotations to the ingress object. Creating a certificate CRD object. First method is quick and simple, but if you need more customization and granularity in terms of certificate renewal, you can choose the second method. Depending on your selection, skip the other method.","title":"Issue certificate for ingress object"},{"location":"certificate-management/acme/#adding-ingress-shim-annotations-to-ingress-object","text":"In this approach, we'll add these two annotations to ingress object for which you request certificate to be issued by the ACME server. kubernetes.io/tls-acme: true certmanager.k8s.io/cluster-issuer: letsencrypt-staging Note You can find all supported annotations from cert-manager for ingress-shim, click here . Also, modify the ingress.yaml to use TLS by specifying a secret. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kuard annotations: kubernetes.io/ingress.class: citrix kubernetes.io/tls-acme: true certmanager.k8s.io/cluster-issuer: letsencrypt-staging spec: tls: - hosts: - kuard.example.com secretName: kuard-example-tls rules: - host: kuard.example.com http: paths: - backend: serviceName: kuard servicePort: 80 The kubernetes.io/tls-acme: \"true\" annotation tells cert-manager to use the letsencrypt-staging cluster-wide issuer that was created earlier to request a certificate from Let's Encrypt's staging servers. Cert-manager creates a certificate object that is used to manage the lifecycle of the certificate for kuard.example.com , and the value for the domain name and challenge method for the certificate object is derived from the ingress object. Cert-manager manages the contents of the secret as long as the Ingress is present in your cluster. Deploy the ingress.yaml using the following command: % kubectl apply -f ingress.yml ingress.extensions/kuard configured % kubectl get ingress kuard NAME HOSTS ADDRESS PORTS AGE kuard kuard.example.com 80, 443 4h39m","title":"Adding Ingress-shim annotations to Ingress object"},{"location":"certificate-management/acme/#create-a-certificate-crd-resource","text":"Alternatively, you can deploy a certificate CRD object independent of ingress object. Documentation of \"certificate\" CRD can be found here. Create a file with certificate.yaml with the following configuration: apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: kuard-example-tls namespace: default spec: secretName: kuard-exmaple-tls issuerRef: name: letsencrypt-staging commonName: kuard.example.com #Renew before 15 days of expiry renewBefore: 360h dnsNames: - kuard.example.com acme: config: - http01: ingressClass: citrix domains: - kuard.example.com ingressClass refers to the ingress class CIC or CPX is running and spec.secretName is the name of the secret where the certificate is stored on successful issuing the certificate. Deploy the certificate.yaml on the Kubernetes cluster: kubectl create -f certificate.yaml certificate.certmanager.k8s.io/kuard-example-tls created","title":"Create a Certificate CRD resource"},{"location":"certificate-management/acme/#issuing-an-acme-certificate-using-dns-challenge","text":"This section describes a way to use DNS validation to get ACME certificate from Let'sEncrypt CA. With a DNS-01 challenge, you prove the ownership of a domain by proving you control its DNS records. This is done by creating a TXT record with specific content that proves you have control of the domain's DNS records. For detailed explanation of DNS challenge and best security practices in deploying DNS challenge, see A Technical Deep Dive: Securing the Automation of ACME DNS Challenge Validation .","title":"Issuing an ACME certificate using DNS challenge"},{"location":"certificate-management/acme/#deploy-the-lets-encrypt-cluster-issuer-with-dns01-challenge-provider","text":"Create an Issuer or ClusterIssuer with dns01 challenge provider. You can provide multiple providers under dns01, and specify which provider to be used at the time of certificate creation. You need to have access to the DNS provider for cert-manager to create a TXT record, the credentials are stored in Kubernetes secret specified in spec.dns01.secretAccessKeySecretRef . For detailed instructions on how to obtain the credentials, see the DNS provider documentation. apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # Enable the DNS-01 challenge provider dns01: providers: - name: dns route53: region: us-east-1 hostedZoneID: YOURZONEID accessKeyID: YOURACCESSKEYID secretAccessKeySecretRef: name: acme-route53 key: secret-access-key Note Replace user@example.com with your email address. For each domain mentioned in a dns01 stanza, cert-manager will use the provider's credentials from the referenced Issuer to create a TXT record called _acme-challenge . This record is then verified by the ACME server in order to issue the certificate. For more information about the DNS provider configuration, and the list of supported providers, see dns01 reference doc . After you edit and save the file, deploy the file using the following command: % kubectl apply -f issuer-letsencrypt-staging.yaml clusterissuer letsencrypt-staging created Verify if the issuer is created and registered to the ACME server using the following command: % kubectl get issuer NAME AGE letsencrypt-staging 8d Verify if the ClusterIssuer is properly registered using the command kubectl describe issuer letsencrypt-staging : Status: Acme: Uri: https://acme-staging-v02.api.letsencrypt.org/acme/acct/8200869 Conditions: Last Transition Time: 2019-02-11T12:06:31Z Message: The ACME account was registered with the ACME server Reason: ACMEAccountRegistered Status: True Type: Ready","title":"Deploy the Let's Encrypt cluster issuer with dns01 challenge provider"},{"location":"certificate-management/acme/#issue-certificate-for-ingress-object_1","text":"Once the issuer is successfully registered, lets proceed to get certificate for the ingress domain kuard.example.com . Similar to http01 challenge, there are two ways you can request the certificate for a given ingress resource: Adding Ingress-shim annotations to the ingress object. Creating a certificate CRD object. For detailed instructions, see Create a Certificate CRD resource","title":"Issue certificate for ingress object"},{"location":"certificate-management/acme/#adding-ingress-shim-annotations-to-the-ingress-object","text":"Add the following annotations to the ingress object along with spec.tls section: kubernetes.io/tls-acme: true certmanager.k8s.io/cluster-issuer: letsencrypt-staging certmanager.k8s.io/acme-challenge-type: dns01 certmanager.k8s.io/acme-dns01-provider: dns apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kuard annotations: kubernetes.io/ingress.class: citrix kubernetes.io/tls-acme: true certmanager.k8s.io/cluster-issuer: letsencrypt-staging certmanager.k8s.io/acme-challenge-type: dns01 certmanager.k8s.io/acme-dns01-provider: dns spec: tls: - hosts: - kuard.example.com secretName: kuard-example-tls rules: - host: kuard.example.com http: paths: - backend: serviceName: kuard servicePort: 80 The cert-manager creates a Certificate CRD resource with dns01 challenge and it uses the credentials given in the ClusterIssuer to create a TXT record in the DNS server for the domain you own. Then, Let's Encypt CA validates the content of the TXT record to complete the challenge.","title":"Adding Ingress-shim annotations to the ingress object"},{"location":"certificate-management/acme/#verify-if-the-certificate-is-issued","text":"For HTTP challenge, cert-manager will create a temporary ingress resource to route the Let's Encrypt CA generated traffic to cert-manager pods. On successful validations of the domain, this temporary ingress is deleted. You can watch the progress of the certificate as it's issued, use the following command: % kubectl describe certificates kuard-example-tls | tail -n 6 Type Reason Age From Message ---- ------ ---- ---- ------- Normal Generated 25m cert-manager Generated new private key Normal OrderCreated 14m (x2 over 25m) cert-manager Created Order resource kuard-example-tls-1006173429 Normal OrderComplete 13m (x2 over 25m) cert-manager Order kuard-example-tls-1006173429 completed successfully Normal CertIssued 13m (x2 over 25m) cert-manager Certificate issued successfully Letsencrypt CA successfully validated the domain and issued a new certificate for the domain. A kubernetes.io/tls secret is created with the secretName specified in the tls: field of the Ingress. Also, cert-manager automatically initiates a renewal, 30 days before the expiry. Verify in the secret is created using the following command: % kubectl get secret kuard-example-tls NAME TYPE DATA AGE kuard-example-tls kubernetes.io/tls 3 30m The secret is picked up by Citrix ingress controller and binds the certificate to the Content switching virtual server on the Citrix ADC CPX. Log on to Citrix ADC CPX and verify if the certificate is bound to the SSL virtual server. kubectl exec -it cpx-ingress-668bf6695f-4fwh8 bash root@cpx-ingress-668bf6695f-4fwh8:/# cli_script.sh 'sh ssl vserver' exec: sh ssl vserver 1) Vserver Name: k8s-10.244.3.148:443:ssl DH: DISABLED DH Private-Key Exponent Size Limit: DISABLED Ephemeral RSA: ENABLED Refresh Count: 0 Session Reuse: ENABLED Timeout: 120 seconds Cipher Redirect: DISABLED SSLv2 Redirect: DISABLED ClearText Port: 0 Client Auth: DISABLED SSL Redirect: DISABLED Non FIPS Ciphers: DISABLED SNI: ENABLED OCSP Stapling: DISABLED HSTS: DISABLED HSTS IncludeSubDomains: NO HSTS Max-Age: 0 SSLv2: DISABLED SSLv3: ENABLED TLSv1.0: ENABLED TLSv1.1: ENABLED TLSv1.2: ENABLED TLSv1.3: DISABLED Push Encryption Trigger: Always Send Close-Notify: YES Strict Sig-Digest Check: DISABLED Zero RTT Early Data: DISABLED DHE Key Exchange With PSK: NO Tickets Per Authentication Context: 1 Done root@cpx-ingress-668bf6695f-4fwh8:/# cli_script.sh 'sh ssl vserver k8s-10.244.3.148:443:ssl' exec: sh ssl vserver k8s-10.244.3.148:443:ssl Advanced SSL configuration for VServer k8s-10.244.3.148:443:ssl: DH: DISABLED DH Private-Key Exponent Size Limit: DISABLED Ephemeral RSA: ENABLED Refresh Count: 0 Session Reuse: ENABLED Timeout: 120 seconds Cipher Redirect: DISABLED SSLv2 Redirect: DISABLED ClearText Port: 0 Client Auth: DISABLED SSL Redirect: DISABLED Non FIPS Ciphers: DISABLED SNI: ENABLED OCSP Stapling: DISABLED HSTS: DISABLED HSTS IncludeSubDomains: NO HSTS Max-Age: 0 SSLv2: DISABLED SSLv3: ENABLED TLSv1.0: ENABLED TLSv1.1: ENABLED TLSv1.2: ENABLED TLSv1.3: DISABLED Push Encryption Trigger: Always Send Close-Notify: YES Strict Sig-Digest Check: DISABLED Zero RTT Early Data: DISABLED DHE Key Exchange With PSK: NO Tickets Per Authentication Context: 1 , P_256, P_384, P_224, P_5216) CertKey Name: k8s-VN4RXHGMCZSTMQZOQ3XGBVMM2OO Server Certificate for SNI 7) Cipher Name: DEFAULT Description: Default cipher list with encryption strength = 128bit Done rcli_script.sh 'sh certkey k8s-VN4RXHGMCZSTMQZOQ3XGBVMM2OO' exec: sh certkey k8s-VN4RXHGMCZSTMQZOQ3XGBVMM2OO Name: k8s-VN4RXHGMCZSTMQZOQ3XGBVMM2OO Status: Valid, Days to expiration:89 Version: 3 Serial Number: FA0DFEDAB578C0228273927DA7C5E17CF098 Signature Algorithm: sha256WithRSAEncryption Issuer: CN=Fake LE Intermediate X1 Validity Not Before: Feb 25 08:00:38 2019 GMT Not After : May 26 08:00:38 2019 GMT Certificate Type: Client Certificate Server Certificate Subject: CN=kuard.example.com Public Key Algorithm: rsaEncryption Public Key size: 2048 Ocsp Response Status: NONE 2) VServer name: k8s-10.244.3.148:443:ssl Server Certificate for SNI Done The HTTPS webserver is now UP with fake LE signed certificate. Next step is to move to production with actual Let's Encrypt certificate.","title":"Verify if the certificate is issued"},{"location":"certificate-management/acme/#move-to-production","text":"After successfully testing with Let's Encrypt-staging, you can get the actual Let's Encrypt certificates. You need to change Let's Encrypt endpoint from https:acme-staging-v02.api.letsencrypt.org/directory to https:acme-v02.api.letsencrypt.org/directory Then, change the name of the ClusterIssuer from letsencrypt-staging to letsencrypt-production apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prod namespace: cert-manager spec: acme: email: user@example.com http01: {} privateKeySecretRef: name: letsencrypt-prod server: https://acme-v02.api.letsencrypt.org/directory Note Replace user@example.com with your email address. Deploy the file using the following command: % kubectl apply -f letsencrypt-prod.yaml clusterissuer letsencrypt-prod created Now repeat the procedure of modifying the annotation in ingress or creating a new CRD certificate which will trigger the generation of new certificate. Note Ensure that you delete the old secret so that cert-manager starts a fresh challenge with the production CA. % kubectl delete secret kuard-example-tls secret \"kuard-example-tls\" deleted Once the HTTP website is up, you can redirect the traffic from HTTP to HTTPS using the annotation ingress.citrix.com/insecure-termination: redirect in the ingress object.","title":"Move to production"},{"location":"certificate-management/acme/#troubleshooting","text":"Since the certificate generation involves multiple components, this section summarizes the troubleshooting techniques that you can use in case of failures.","title":"Troubleshooting"},{"location":"certificate-management/acme/#verify-the-current-status-of-certificate-generation","text":"Certificate CRD object defines the life cycle management of generation and renewal of the certificates. You can view the status of the certificate using kubectl describe command as shown below. % kubectl get certificate NAME READY SECRET AGE kuard-example-tls False kuard-example-tls 9s % kubectl describe certificate kuard-example-tls Status: Conditions: Last Transition Time: 2019-03-05T09:50:29Z Message: Certificate does not exist Reason: NotFound Status: False Type: Ready Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal OrderCreated 22s cert-manager Created Order resource kuard-example-tls-1754626579 Also you can view the major certificate events using the kubectl events commands: kubectl get events LAST SEEN TYPE REASON KIND MESSAGE 36s Normal Started Challenge Challenge scheduled for processing 36s Normal Created Order Created Challenge resource kuard-example-tls-1754626579-0 for domain acme.cloudpst.net 38s Normal OrderCreated Certificate Created Order resource kuard-example-tls-1754626579 38s Normal CreateCertificate Ingress Successfully created Certificate kuard-example-tls","title":"Verify the current status of certificate generation"},{"location":"certificate-management/acme/#analyze-the-logs-from-cert-manager","text":"In case of failure, first step is to analyze the logs from the cert-manager component. Identify the cert-manager pod using the following command: kubectl get po -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-76d48d47bf-5w4vx 1/1 Running 0 23h cert-manager-webhook-67cfb86d56-6qtxr 1/1 Running 0 23h cert-manager-webhook-ca-sync-x4q6f 0/1 Completed 4 23h Here cert-manager-76d48d47bf-5w4vx is the main cert-manager pod, and other two pods are cert-manager webhook pods. Get the logs of the cert-manager using the following command: kubectl logs -f cert-manager-76d48d47bf-5w4vx -n cert-manager If there is any failure to get the certificate, the ERROR logs give details about the failure.","title":"Analyze the logs from cert-manager"},{"location":"certificate-management/acme/#check-the-kubernetes-secret","text":"Use kubectl describe command to verify if both certificates and key are populated in Kubernetes secret. % kubectl describe secret kuard-example-tls Name: kuard-example-tls Namespace: default Labels: certmanager.k8s.io/certificate-name=kuard-example-tls Annotations: certmanager.k8s.io/alt-names: acme.cloudpst.net certmanager.k8s.io/common-name: acme.cloudpst.net certmanager.k8s.io/issuer-kind: ClusterIssuer certmanager.k8s.io/issuer-name: letsencrypt-staging Type: kubernetes.io/tls Data ==== tls.crt: 3553 bytes tls.key: 1679 bytes ca.crt: 0 bytes If both tls.crt and tls.key are populated in the kubernetes secret, certificate generation is complete. If only tls.key is present, certificate generation is incomplete. Analyze the cert-manager logs for more details about the issue.","title":"Check the Kubernetes secret"},{"location":"certificate-management/acme/#analyze-the-logs-from-citrix-ingress-controller","text":"If kubernetes secret is generated and complete, but this secret is not uploaded to Citrix ADC CPX or VPX, you can analyze the logs from citrix ingress controller using kubectl logs command. % kubectl logs -f cpx-ingress-685c8bc976-zgz8q","title":"Analyze the logs from Citrix Ingress Controller"},{"location":"certificate-management/certificate/","text":"Automated certificate management with cert-manager Citrix Ingress Controller (CIC) supports automatic provisioning and renewal of TLS certificates using cert-manager . The cert-manager is a native Kubernetes certificate management controller. It issues certificates from different sources, such as Let\u2019s Encrypt and HashiCorp Vault . As shown in the following diagram, cert-manager interacts with the external Certificate Authorities (CA) to sign the certificates and converts it to Kubernetes secrets. These secrets are used by CIC to configure SSL virtual server on the Citrix ADC. For detailed configurations, refer: Deploying HTTPS web applications on Kubernetes with Citrix Ingress Controller and Let\u2019s Encrypt using cert-manager Deploying HTTPS web application on Kubernetes with Citrix Ingress Controller and HashiCorp Vault using cert-manager","title":"Introduction"},{"location":"certificate-management/certificate/#automated-certificate-management-with-cert-manager","text":"Citrix Ingress Controller (CIC) supports automatic provisioning and renewal of TLS certificates using cert-manager . The cert-manager is a native Kubernetes certificate management controller. It issues certificates from different sources, such as Let\u2019s Encrypt and HashiCorp Vault . As shown in the following diagram, cert-manager interacts with the external Certificate Authorities (CA) to sign the certificates and converts it to Kubernetes secrets. These secrets are used by CIC to configure SSL virtual server on the Citrix ADC. For detailed configurations, refer: Deploying HTTPS web applications on Kubernetes with Citrix Ingress Controller and Let\u2019s Encrypt using cert-manager Deploying HTTPS web application on Kubernetes with Citrix Ingress Controller and HashiCorp Vault using cert-manager","title":"Automated certificate management with cert-manager"},{"location":"certificate-management/tls-certificate-handling-multiple-ingress/","text":"TLS certificate handling for multiple ingress Overview The TLS handshake is the process that your web browser performs in the background process to create an HTTPs connection for you. In general, this process gets completed in a few seconds. Whenever a request comes to the server for enabling an HTTPs connection, the client and the server perform the TLS handshake. In a situation when there is a higher load to the server, the response from the server might become slow because the server has to complete the TLS handshake process. To avoid this additional processing time from the server, the Citrix ADC instance can do the TLS handshake process to ensure the server response time is faster. A Citrix ADC appliance configured for TLS acceleration transparently accelerates TLS transactions by offloading TLS processing from the server. To configure TLS offloading, you need to configure a virtual server to intercept and process TLS transactions, and send the decrypted traffic to the server (unless you configure end-to-end encryption, in which case the traffic is re-encrypted). Upon receiving the response from the server, the appliance completes the secure transaction with the client. From the client\u2019s perspective, the transaction seems to be directly with the server. Configuring TLS offloading requires a TLS certificate and a key pair, which you must obtain if you do not already have a TLS certificate. Other TLS-related tasks that you might need to perform includes managing certificates, managing certificate revocation lists, configuring client authentication, and managing TLS actions and policies. A non-FIPS Citrix ADC appliance stores the server\u2019s private key on the hard disk. On a FIPS appliance, the key is stored in a cryptographic module known as a hardware security module (HSM). All Citrix ADC instances that do not support a FIPS card (including virtual appliances) support the Thales nShield\u00ae Connect and SafeNet external HSMs. (MPX 9700/10500/12500/15500 appliances do not support an external HSM.) Note FIPS-related options for some of the TLS configuration procedures described in this topic are specific to a FIPS-enabled Citrix ADC appliance. TLS Offload In the Citrix ADC instance, the TLS offloading is configured in the Content Switching Virtual Server (CS Vserver). By default, each Citrix ADC instance can have one certificate and the application receives the traffic based on the policy bound to the certificate. However, you have the Server Name Indication (SNI) option to have multiple certificates bound to multiple applications. If you want to have one certificate, then you can disable the SNI option and the certificate is bound to the CS Vserver. You can define all these requirements (certificate and domain name) in the Citrix Ingress Controller. Define certificates to Citrix Ingress Controller Using the yaml, you can define a certificate under args and under tls section. If you have defined a certificate under args , then the certificate is bound to non-SNI certificate. If you have defined a certificate under tls , then the certificate is bound to SNI certificate. The following is an example yaml that defines a certificate: --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: cic-k8s-role rules: - apiGroups: [ ] resources: [ services , endpoints , ingresses , pods , secrets , nodes ] verbs: [ * ] - apiGroups: [ extensions ] resources: [ ingresses , ingresses/status ] verbs: [ * ] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: cic-k8s-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cic-k8s-role subjects: - kind: ServiceAccount name: cic-k8s-role namespace: default apiVersion: rbac.authorization.k8s.io/v1 --- apiVersion: v1 kind: ServiceAccount metadata: name: cic-k8s-role namespace: default --- apiVersion: v1 kind: Pod metadata: name: cic-k8s-ingress-controller labels: app: cic-k8s-ingress-controller spec: serviceAccountName: cic-k8s-role containers: - name: cic-k8s-ingress-controller image: quay.io/citrix/citrix-k8s-ingress-controller:latest env: # Set Citrix ADC NSIP/SNIP, SNIP in case of HA (mgmt has to be enabled) - name: NS_IP value: x.x.x.x # Set username for Nitro - name: NS_USER valueFrom: secretKeyRef: name: nslogin key: username # Set user password for Nitro - name: NS_PASSWORD valueFrom: secretKeyRef: name: nslogin key: password # Set log level - name: EULA value: yes args: - --ingress-classes citrix - --default-ssl-certificate # secret name Colddrink-secret - --feature-node-watch false imagePullPolicy: Always Note You must provide a default secret name in the yaml. Setting up HTTP(S) Load Balancing with Ingress TLS : You can secure an Ingress by specifying a secret containing TLS pem. CIC configures the certificate resource in Citrix ADC and use it to encrypt the communication. The example in this section shows you how to secure an Ingress using TLS/SSL certificates. Import Existing Certificate : To import an existing certificate or key pair into a Kubernetes cluster, the following sample command can be used: $ kubectl create secret tls colddrink-secret --namespace=team-colddrink --cert=path/to/tls.cert --key=path/to/tls.key secret colddrink-secret created The secret with a PEM formatted certificate under tls.crt key and the PEM formatted private key under tls.key key is created. You can use the same command in the yaml: apiVersion: v1 kind: Secret metadata: name: colddrink-secret data: tls.crt: base64 encoded cert tls.key: base64 encoded key The secret that is created can be used with CIC to secure the communication from client to Citrix ADC using TLS. You can provide the secret to the yaml in two ways: Using CIC yaml file - This is the non-SNI enabled HTTP traffic that is the default certificate for all HTTPs incoming traffic. This certificate does not have the full domain name. Using ingress yaml file - This is the SNI-enabled HTTPs traffic that requires a domain name. Use cases If you do not want to use any certificate, then there is no need to specify any secret-related information in the yaml. In case of default certificate: For example, let us consider a secret colddrink-secret specified under args in the CIC yaml, then the secret is bound as non-SNI certificate. kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: cic-k8s-role rules: - apiGroups: [ ] resources: [ services , endpoints , ingresses , pods , secrets , nodes ] verbs: [ * ] - apiGroups: [ extensions ] resources: [ ingresses , ingresses/status ] verbs: [ * ] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: cic-k8s-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cic-k8s-role subjects: - kind: ServiceAccount name: cic-k8s-role namespace: default apiVersion: rbac.authorization.k8s.io/v1 --- apiVersion: v1 kind: ServiceAccount metadata: name: cic-k8s-role namespace: default --- apiVersion: v1 kind: Pod metadata: name: cic-k8s-ingress-controller labels: app: cic-k8s-ingress-controller spec: serviceAccountName: cic-k8s-role containers: - name: cic-k8s-ingress-controller image: quay.io/citrix/citrix-k8s-ingress-controller:latest env: # Set Citrix ADC NSIP/SNIP, SNIP in case of HA (mgmt has to be enabled) - name: NS_IP value: x.x.x.x # Set username for Nitro - name: NS_USER valueFrom: secretKeyRef: name: nslogin key: username # Set user password for Nitro - name: NS_PASSWORD valueFrom: secretKeyRef: name: nslogin key: password # Set log level - name: EULA value: yes args: - --ingress-classes citrix - --default-ssl-certificate # secret name Colddrink-secret - --feature-node-watch false imagePullPolicy: Always You need to also add an empty secret in ingress file to enable default certificate for that service. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: colddrinks-ingress annotations: kubernetes.io/ingress.class: \u201ccolddrink\u2019 spec: tls: - secretName: rules: - host: items.colddrink.beverages http: paths: - path: / backend: serviceName: frontend-colddrinks servicePort: 443 tls section is required to ensure that the frontend-colddrinks service has the HTTPs traffic and need tls secret for encryption. Hence, secret provided as default under CIC is used for this service. Assumptions : There can be at most 1 default secret under args in CIC yaml that is considered as default certificate for all HTTPs traffic. This default certificate is used globally for all services managed by this CIC yaml file. Need to add empty secret name under TLS section to enable TLS feature for that service. No TLS section is required with ingress if TLS feature not required. All HTTPs request uses default certificate without matching the CN name of certificate used with secret. Secret in CIC ingress Yaml (SNI enabled) : The secret in ingress can be added in two ways: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: colddrinks-ingress annotations: kubernetes.io/ingress.class: \u201ccolddrink\u2019 spec: tls: - secretName: colddrink.secret hosts: - items.colddrink.beverages rules: - host: items.colddrink.beverages http: paths: - path: / backend: serviceName: frontend-colddrinks servicePort: 443 or apiVersion: extensions/v1beta1 kind: Ingress metadata: name: colddrinks-ingress annotations: kubernetes.io/ingress.class: \u201ccolddrink\u2019 spec: tls: - hosts: - items.colddrink.beverages secretName: colddrink.secret rules: - host: items.colddrink.beverages http: paths: - path: / backend: serviceName: frontend-colddrinks servicePort: 443 Assumptions : Secret given under TLS binds to Citrix ADC and performs strict name matching for the host name given under CN name of the certificate. Certificate used for secret given under TLS section must have CN name otherwise it does not bind to Citrix ADC. Default certificate if provided is binded along with the secret given under TLS section. Default certificate is used for the following HTTPs requests: curl -1 -v -k https://1.1.1.1/ curl -1 -v -k -H 'HOST:*.colddrink.beverages' https://1.1.1.1/ while, secret given under TLS section is used for request with full domain name. curl -1 -v -k https://items.colddrink.beverages/ If any request received that does not match with certificates CN name fails. For example, curl -1 -v -k https://items.hotdrink.beverages/ Multiple ingress Secret in CIC ingress yaml (SNI enabled) : If multiple ingress files is used for different services, then all secrets will be used together to bind with context switch virtual server of Citrix ADC instance. Example 1: Hotdrink_Ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: hotdrinks-ingress annotations: kubernetes.io/ingress.class: \u201chotdrink\u2019 spec: tls: - secretName: hotdrink.secret hosts: - items.hotdrink.beverages rules: - host: items.hotdrink.beverages http: paths: - path: / backend: serviceName: frontend-hotdrinks servicePort: 443 Colddrink_Ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: colddrinks-ingress annotations: kubernetes.io/ingress.class: \u201ccolddrink\u2019 spec: tls: - hosts: - items.colddrink.beverages secretName: colddrink.secret rules: - host: items.colddrink.beverages http: paths: - path: / backend: serviceName: frontend-colddrinks servicePort: 443 Example 2: The same secret used in 2 ingress file is handled internally and would not affect the behavior of other ingress, in case of addition or removal of either of ingress yaml. Hotdrink.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: hotdrinks-ingress annotations: kubernetes.io/ingress.class: \u201chotdrink\u2019 spec: tls: - secretName: hotdrink.secret hosts: - items.hotdrink.beverages rules: - host: items.hotdrink.beverages http: paths: - path: / backend: serviceName: frontend-hotdrinks servicePort: 443 Colddrink.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: colddrinks-ingress annotations: kubernetes.io/ingress.class: \u201ccolddrink\u2019 spec: tls: - secretName: hotdrink.secret hosts: - items.hotdrink.beverages - secretName: colddrink.secret hosts: - items.colddrink.beverages rules: - host: items.colddrink.beverages http: paths: - path: / backend: serviceName: frontend-colddrinks servicePort: 443","title":"TLS certificate handling for multiple Ingress"},{"location":"certificate-management/tls-certificate-handling-multiple-ingress/#tls-certificate-handling-for-multiple-ingress","text":"","title":"TLS certificate handling for multiple ingress"},{"location":"certificate-management/tls-certificate-handling-multiple-ingress/#overview","text":"The TLS handshake is the process that your web browser performs in the background process to create an HTTPs connection for you. In general, this process gets completed in a few seconds. Whenever a request comes to the server for enabling an HTTPs connection, the client and the server perform the TLS handshake. In a situation when there is a higher load to the server, the response from the server might become slow because the server has to complete the TLS handshake process. To avoid this additional processing time from the server, the Citrix ADC instance can do the TLS handshake process to ensure the server response time is faster. A Citrix ADC appliance configured for TLS acceleration transparently accelerates TLS transactions by offloading TLS processing from the server. To configure TLS offloading, you need to configure a virtual server to intercept and process TLS transactions, and send the decrypted traffic to the server (unless you configure end-to-end encryption, in which case the traffic is re-encrypted). Upon receiving the response from the server, the appliance completes the secure transaction with the client. From the client\u2019s perspective, the transaction seems to be directly with the server. Configuring TLS offloading requires a TLS certificate and a key pair, which you must obtain if you do not already have a TLS certificate. Other TLS-related tasks that you might need to perform includes managing certificates, managing certificate revocation lists, configuring client authentication, and managing TLS actions and policies. A non-FIPS Citrix ADC appliance stores the server\u2019s private key on the hard disk. On a FIPS appliance, the key is stored in a cryptographic module known as a hardware security module (HSM). All Citrix ADC instances that do not support a FIPS card (including virtual appliances) support the Thales nShield\u00ae Connect and SafeNet external HSMs. (MPX 9700/10500/12500/15500 appliances do not support an external HSM.) Note FIPS-related options for some of the TLS configuration procedures described in this topic are specific to a FIPS-enabled Citrix ADC appliance.","title":"Overview"},{"location":"certificate-management/tls-certificate-handling-multiple-ingress/#tls-offload","text":"In the Citrix ADC instance, the TLS offloading is configured in the Content Switching Virtual Server (CS Vserver). By default, each Citrix ADC instance can have one certificate and the application receives the traffic based on the policy bound to the certificate. However, you have the Server Name Indication (SNI) option to have multiple certificates bound to multiple applications. If you want to have one certificate, then you can disable the SNI option and the certificate is bound to the CS Vserver. You can define all these requirements (certificate and domain name) in the Citrix Ingress Controller.","title":"TLS Offload"},{"location":"certificate-management/tls-certificate-handling-multiple-ingress/#define-certificates-to-citrix-ingress-controller","text":"Using the yaml, you can define a certificate under args and under tls section. If you have defined a certificate under args , then the certificate is bound to non-SNI certificate. If you have defined a certificate under tls , then the certificate is bound to SNI certificate. The following is an example yaml that defines a certificate: --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: cic-k8s-role rules: - apiGroups: [ ] resources: [ services , endpoints , ingresses , pods , secrets , nodes ] verbs: [ * ] - apiGroups: [ extensions ] resources: [ ingresses , ingresses/status ] verbs: [ * ] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: cic-k8s-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cic-k8s-role subjects: - kind: ServiceAccount name: cic-k8s-role namespace: default apiVersion: rbac.authorization.k8s.io/v1 --- apiVersion: v1 kind: ServiceAccount metadata: name: cic-k8s-role namespace: default --- apiVersion: v1 kind: Pod metadata: name: cic-k8s-ingress-controller labels: app: cic-k8s-ingress-controller spec: serviceAccountName: cic-k8s-role containers: - name: cic-k8s-ingress-controller image: quay.io/citrix/citrix-k8s-ingress-controller:latest env: # Set Citrix ADC NSIP/SNIP, SNIP in case of HA (mgmt has to be enabled) - name: NS_IP value: x.x.x.x # Set username for Nitro - name: NS_USER valueFrom: secretKeyRef: name: nslogin key: username # Set user password for Nitro - name: NS_PASSWORD valueFrom: secretKeyRef: name: nslogin key: password # Set log level - name: EULA value: yes args: - --ingress-classes citrix - --default-ssl-certificate # secret name Colddrink-secret - --feature-node-watch false imagePullPolicy: Always Note You must provide a default secret name in the yaml.","title":"Define certificates to Citrix Ingress Controller"},{"location":"certificate-management/tls-certificate-handling-multiple-ingress/#setting-up-https-load-balancing-with-ingress","text":"TLS : You can secure an Ingress by specifying a secret containing TLS pem. CIC configures the certificate resource in Citrix ADC and use it to encrypt the communication. The example in this section shows you how to secure an Ingress using TLS/SSL certificates. Import Existing Certificate : To import an existing certificate or key pair into a Kubernetes cluster, the following sample command can be used: $ kubectl create secret tls colddrink-secret --namespace=team-colddrink --cert=path/to/tls.cert --key=path/to/tls.key secret colddrink-secret created The secret with a PEM formatted certificate under tls.crt key and the PEM formatted private key under tls.key key is created. You can use the same command in the yaml: apiVersion: v1 kind: Secret metadata: name: colddrink-secret data: tls.crt: base64 encoded cert tls.key: base64 encoded key The secret that is created can be used with CIC to secure the communication from client to Citrix ADC using TLS. You can provide the secret to the yaml in two ways: Using CIC yaml file - This is the non-SNI enabled HTTP traffic that is the default certificate for all HTTPs incoming traffic. This certificate does not have the full domain name. Using ingress yaml file - This is the SNI-enabled HTTPs traffic that requires a domain name.","title":"Setting up HTTP(S) Load Balancing with Ingress"},{"location":"certificate-management/tls-certificate-handling-multiple-ingress/#use-cases","text":"If you do not want to use any certificate, then there is no need to specify any secret-related information in the yaml. In case of default certificate: For example, let us consider a secret colddrink-secret specified under args in the CIC yaml, then the secret is bound as non-SNI certificate. kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: cic-k8s-role rules: - apiGroups: [ ] resources: [ services , endpoints , ingresses , pods , secrets , nodes ] verbs: [ * ] - apiGroups: [ extensions ] resources: [ ingresses , ingresses/status ] verbs: [ * ] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: cic-k8s-role roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cic-k8s-role subjects: - kind: ServiceAccount name: cic-k8s-role namespace: default apiVersion: rbac.authorization.k8s.io/v1 --- apiVersion: v1 kind: ServiceAccount metadata: name: cic-k8s-role namespace: default --- apiVersion: v1 kind: Pod metadata: name: cic-k8s-ingress-controller labels: app: cic-k8s-ingress-controller spec: serviceAccountName: cic-k8s-role containers: - name: cic-k8s-ingress-controller image: quay.io/citrix/citrix-k8s-ingress-controller:latest env: # Set Citrix ADC NSIP/SNIP, SNIP in case of HA (mgmt has to be enabled) - name: NS_IP value: x.x.x.x # Set username for Nitro - name: NS_USER valueFrom: secretKeyRef: name: nslogin key: username # Set user password for Nitro - name: NS_PASSWORD valueFrom: secretKeyRef: name: nslogin key: password # Set log level - name: EULA value: yes args: - --ingress-classes citrix - --default-ssl-certificate # secret name Colddrink-secret - --feature-node-watch false imagePullPolicy: Always You need to also add an empty secret in ingress file to enable default certificate for that service. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: colddrinks-ingress annotations: kubernetes.io/ingress.class: \u201ccolddrink\u2019 spec: tls: - secretName: rules: - host: items.colddrink.beverages http: paths: - path: / backend: serviceName: frontend-colddrinks servicePort: 443 tls section is required to ensure that the frontend-colddrinks service has the HTTPs traffic and need tls secret for encryption. Hence, secret provided as default under CIC is used for this service. Assumptions : There can be at most 1 default secret under args in CIC yaml that is considered as default certificate for all HTTPs traffic. This default certificate is used globally for all services managed by this CIC yaml file. Need to add empty secret name under TLS section to enable TLS feature for that service. No TLS section is required with ingress if TLS feature not required. All HTTPs request uses default certificate without matching the CN name of certificate used with secret. Secret in CIC ingress Yaml (SNI enabled) : The secret in ingress can be added in two ways: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: colddrinks-ingress annotations: kubernetes.io/ingress.class: \u201ccolddrink\u2019 spec: tls: - secretName: colddrink.secret hosts: - items.colddrink.beverages rules: - host: items.colddrink.beverages http: paths: - path: / backend: serviceName: frontend-colddrinks servicePort: 443 or apiVersion: extensions/v1beta1 kind: Ingress metadata: name: colddrinks-ingress annotations: kubernetes.io/ingress.class: \u201ccolddrink\u2019 spec: tls: - hosts: - items.colddrink.beverages secretName: colddrink.secret rules: - host: items.colddrink.beverages http: paths: - path: / backend: serviceName: frontend-colddrinks servicePort: 443 Assumptions : Secret given under TLS binds to Citrix ADC and performs strict name matching for the host name given under CN name of the certificate. Certificate used for secret given under TLS section must have CN name otherwise it does not bind to Citrix ADC. Default certificate if provided is binded along with the secret given under TLS section. Default certificate is used for the following HTTPs requests: curl -1 -v -k https://1.1.1.1/ curl -1 -v -k -H 'HOST:*.colddrink.beverages' https://1.1.1.1/ while, secret given under TLS section is used for request with full domain name. curl -1 -v -k https://items.colddrink.beverages/ If any request received that does not match with certificates CN name fails. For example, curl -1 -v -k https://items.hotdrink.beverages/ Multiple ingress Secret in CIC ingress yaml (SNI enabled) : If multiple ingress files is used for different services, then all secrets will be used together to bind with context switch virtual server of Citrix ADC instance. Example 1: Hotdrink_Ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: hotdrinks-ingress annotations: kubernetes.io/ingress.class: \u201chotdrink\u2019 spec: tls: - secretName: hotdrink.secret hosts: - items.hotdrink.beverages rules: - host: items.hotdrink.beverages http: paths: - path: / backend: serviceName: frontend-hotdrinks servicePort: 443 Colddrink_Ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: colddrinks-ingress annotations: kubernetes.io/ingress.class: \u201ccolddrink\u2019 spec: tls: - hosts: - items.colddrink.beverages secretName: colddrink.secret rules: - host: items.colddrink.beverages http: paths: - path: / backend: serviceName: frontend-colddrinks servicePort: 443 Example 2: The same secret used in 2 ingress file is handled internally and would not affect the behavior of other ingress, in case of addition or removal of either of ingress yaml. Hotdrink.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: hotdrinks-ingress annotations: kubernetes.io/ingress.class: \u201chotdrink\u2019 spec: tls: - secretName: hotdrink.secret hosts: - items.hotdrink.beverages rules: - host: items.hotdrink.beverages http: paths: - path: / backend: serviceName: frontend-hotdrinks servicePort: 443 Colddrink.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: colddrinks-ingress annotations: kubernetes.io/ingress.class: \u201ccolddrink\u2019 spec: tls: - secretName: hotdrink.secret hosts: - items.hotdrink.beverages - secretName: colddrink.secret hosts: - items.colddrink.beverages rules: - host: items.colddrink.beverages http: paths: - path: / backend: serviceName: frontend-colddrinks servicePort: 443","title":"Use cases"},{"location":"certificate-management/tls-certificate-handling/","text":"TLS Certificates in Citrix Ingress Controller Citrix Ingress Controller (CIC) can configure tls secrets provided to it from Kubernetes as SSL certkeys on the Citrix ADC. To enable this feature, you need to add the following section in the ingress definition: spec: tls: - secretName: CIC handles the secrets in the following three ways: CIC Default Certificate Preconfigured Certificates TLS Section in the Ingress YAML CIC Default Certificate The CIC default certificate is used to provide a secret on Kubernetes that needs to be used as a non-SNI certificate. You must provide the secret name to be used and namespace from which it should be taken as arguments in the .yaml file of the CIC: --default-ssl-certificate NAMESPACE / SECRET_NAME The following is a sample cic.yaml file with hotdrink.secret tls secret picked from the ssl namespace and provided as the CIC default certificate. apiVersion: v1 kind: Pod metadata: name: cic labels: app: cic spec: serviceAccountName: cpx containers: - name: cic image: xxxx imagePullPolicy: Always args: - --default-ssl-certificate ssl/hotdrink.secret env: # Set NetScaler Management IP - name: NS_IP value: xx.xx.xx.xx # Set port for Nitro - name: NS_PORT value: xx # Set Protocol for Nitro - name: NS_PROTOCOL value: HTTP # Set username for Nitro - name: NS_USER value: nsroot # Set user password for Nitro - name: NS_PASSWORD value: nsroot Preconfigured certificates CIC allows you to use the certkeys that are already configured on the Citrix ADC. You must provide the details about the certificate using an annotation in the ingress definition. The general format for the annotation is: 'ingress.citrix.com/preconfigured-certkey : '{ certs : [ { name : name , [ type : default|sni|ca ]} ] }' You can provide details about multiple certificates as a list within the annotation. Also, you can define the way the certificate should be treated. In the following sample annotation, certkey1 is used as a non-SNI certificate and certkey2 is used as an SNI certificate: 'ingress.citrix.com/preconfigured-certkey : '{ certs : [ { name : certkey1 , type : default }, { name : certkey2 , type : sni } ] } If the type parameter is not provided with the name of a certificate, then it is considered as the default (non-SNI) type. Important Ensure that you use this feature in cases where you want to reuse the certificates that are present on the Citrix ADC and bind them to the applications that is managed by CIC. CIC does not manage the life cycle of the certificates. That is, it does not create or delete the certificates, but only bind them to the necessary applications. TLS Section in the Ingress YAML Kubernetes allows you to provide the tls secrets in the spec: section of an ingress definition. This section describes how CIC uses these secrets. With the host section If the secret name is provided with the host section, CIC binds the secret as an SNI certificate. spec: tls: - secretName: fruitjuice.secret rules: - host: items.fruit.juice Without the host section If the secret name is provided without the host section, CIC binds the secret as a non-SNI certificate. spec: tls: - secretName: colddrink.secret Points to note In cases wherein if multiple secrets are provided to the CIC the following precedence is followed: ssl-default-certificate preconfigured-default-certkey non-host tls secret . If there is a conflict in precedence among the same grade certificates (for example, two ingress files configure a non-host tls secret each, as default/non-SNI type), then the CIC binds the CIC default certificate as the non-SNI certificate and uses all other certificates with SNI.","title":"TLS certificates in Citrix Ingress Controller"},{"location":"certificate-management/tls-certificate-handling/#tls-certificates-in-citrix-ingress-controller","text":"Citrix Ingress Controller (CIC) can configure tls secrets provided to it from Kubernetes as SSL certkeys on the Citrix ADC. To enable this feature, you need to add the following section in the ingress definition: spec: tls: - secretName: CIC handles the secrets in the following three ways: CIC Default Certificate Preconfigured Certificates TLS Section in the Ingress YAML","title":"TLS Certificates in Citrix Ingress Controller"},{"location":"certificate-management/tls-certificate-handling/#cic-default-certificate","text":"The CIC default certificate is used to provide a secret on Kubernetes that needs to be used as a non-SNI certificate. You must provide the secret name to be used and namespace from which it should be taken as arguments in the .yaml file of the CIC: --default-ssl-certificate NAMESPACE / SECRET_NAME The following is a sample cic.yaml file with hotdrink.secret tls secret picked from the ssl namespace and provided as the CIC default certificate. apiVersion: v1 kind: Pod metadata: name: cic labels: app: cic spec: serviceAccountName: cpx containers: - name: cic image: xxxx imagePullPolicy: Always args: - --default-ssl-certificate ssl/hotdrink.secret env: # Set NetScaler Management IP - name: NS_IP value: xx.xx.xx.xx # Set port for Nitro - name: NS_PORT value: xx # Set Protocol for Nitro - name: NS_PROTOCOL value: HTTP # Set username for Nitro - name: NS_USER value: nsroot # Set user password for Nitro - name: NS_PASSWORD value: nsroot","title":"CIC Default Certificate"},{"location":"certificate-management/tls-certificate-handling/#preconfigured-certificates","text":"CIC allows you to use the certkeys that are already configured on the Citrix ADC. You must provide the details about the certificate using an annotation in the ingress definition. The general format for the annotation is: 'ingress.citrix.com/preconfigured-certkey : '{ certs : [ { name : name , [ type : default|sni|ca ]} ] }' You can provide details about multiple certificates as a list within the annotation. Also, you can define the way the certificate should be treated. In the following sample annotation, certkey1 is used as a non-SNI certificate and certkey2 is used as an SNI certificate: 'ingress.citrix.com/preconfigured-certkey : '{ certs : [ { name : certkey1 , type : default }, { name : certkey2 , type : sni } ] } If the type parameter is not provided with the name of a certificate, then it is considered as the default (non-SNI) type. Important Ensure that you use this feature in cases where you want to reuse the certificates that are present on the Citrix ADC and bind them to the applications that is managed by CIC. CIC does not manage the life cycle of the certificates. That is, it does not create or delete the certificates, but only bind them to the necessary applications.","title":"Preconfigured certificates"},{"location":"certificate-management/tls-certificate-handling/#tls-section-in-the-ingress-yaml","text":"Kubernetes allows you to provide the tls secrets in the spec: section of an ingress definition. This section describes how CIC uses these secrets.","title":"TLS Section in the Ingress YAML"},{"location":"certificate-management/tls-certificate-handling/#with-the-host-section","text":"If the secret name is provided with the host section, CIC binds the secret as an SNI certificate. spec: tls: - secretName: fruitjuice.secret rules: - host: items.fruit.juice","title":"With the host section"},{"location":"certificate-management/tls-certificate-handling/#without-the-host-section","text":"If the secret name is provided without the host section, CIC binds the secret as a non-SNI certificate. spec: tls: - secretName: colddrink.secret","title":"Without the host section"},{"location":"certificate-management/tls-certificate-handling/#points-to-note","text":"In cases wherein if multiple secrets are provided to the CIC the following precedence is followed: ssl-default-certificate preconfigured-default-certkey non-host tls secret . If there is a conflict in precedence among the same grade certificates (for example, two ingress files configure a non-host tls secret each, as default/non-SNI type), then the CIC binds the CIC default certificate as the non-SNI certificate and uses all other certificates with SNI.","title":"Points to note"},{"location":"certificate-management/vault/","text":"Deploying HTTPS web application on Kubernetes with Citrix Ingress Controller and Hashicorp Vault using cert-manager This topic provides a sample workflow that uses HashiCorp Vault as self-signed CA to automate TLS certificate provisioning, revocation, and renewal for ingress resources deployed with Citrix ingress controller using cert-manager. Specifically, the workflow uses the Vault PKI Secrets Engine to create a CA. This tutorial assumes that you have a Vault server installed and reachable from the Kubernetes cluster. The PKI secrets engine of Vault is suitable for internal applications and for external facing applications that require public trust, you can refer automating TLS certificates using letsencrypt CA The workflow uses a Vault secret engine and authentication methods, refer the following Vault documentation for full list of features: Vault Secrets Engines Vault Authentication Methods This topic provides you information on how to deploy an HTTPS web application on a Kubernetes cluster, using: Citrix ingress controller (CIC) JetStack's cert-manager to provision TLS certificates from Hashicorp Vault Hashicorp Vault Prerequisites Ensure that you have: The Vault server is installed, unsealed and is reachable from Kubernetes cluster. Enabled RBAC on your Kubernetes cluster. Deployed Citrix ADC MPX, VPX or CPX in Tier 1 or Tier 2 deployment model. In the Tier 1 deployment model, Citrix ADC MPX or VPX is used as an Application Delivery Controller (ADC) and Citrix Ingress Controller (CIC) running in Kubernetes cluster configures the virtual services for the services running on Kubernetes cluster. Citrix ADC runs the virtual service on the publicly routable IP address and offloads SSL for client traffic with the help of Let's Encrypt generated certificate. Similarly in the Tier 2 deployment model, a TCP service is configured on the Citrix ADC (VPX/MPX) running outside the Kubernetes cluster to forward the traffic to Citrix ADC CPX instances running in the Kubernetes cluster. Citrix ADC CPX ends the SSL session and load-balances the traffic to actual service pods. Deployed Citrix ingress controller. Click here for various deployment scenarios. Administrator permissions for all the deployment steps. If you encounter failures due to permissions, make sure you have administrator permission. Deploy cert-manager using the manifest file Perform the following steps to deploy cert-manager using the supplied YAML manifest file. To keep things simple, skip cert-manager's Helm installation, and instead use the supplied YAML manifests. Download the latest source of cert-manager from github.com/jetstack/cert-manager repository using the following command. wget https://github.com/jetstack/cert-manager/archive/v0.6.2.tar.gz tar -zxvf v0.6.2.tar.gz Deploy the cert-manager using the following command. kubectl apply -f deploy/manifests/cert-manager.yaml You can also install the cert-manager with Helm, for more information see cert-manager documentation Verify that the cert-manager is up and running using the following command. % kubectl -n cert-manager get all NAME READY STATUS RESTARTS AGE pod/cert-manager-77fd74fb64-d68v7 1/1 Running 0 4m41s pod/cert-manager-webhook-67bf86d45-k77jj 1/1 Running 0 4m41s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cert-manager-webhook ClusterIP 10.108.161.154 none 443/TCP 13d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cert-manager 1/1 1 1 13d deployment.apps/cert-manager-webhook 1/1 1 1 13d NAME DESIRED CURRENT READY AGE replicaset.apps/cert-manager-77fd74fb64 1 1 1 13d replicaset.apps/cert-manager-webhook-67bf86d45 1 1 1 13d NAME COMPLETIONS DURATION AGE job.batch/cert-manager-webhook-ca-sync 1/1 22s 13d job.batch/cert-manager-webhook-ca-sync-1549756800 1/1 21s 10d job.batch/cert-manager-webhook-ca-sync-1550361600 1/1 19s 3d8h NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE cronjob.batch/cert-manager-webhook-ca-sync @weekly False 0 3d8h 13d Deploy a sample web application Perform the following steps to deploy a sample web application. Note Kuard , a kubernetes demo application is used for reference in this topic. Create a deployment YAML file ( kuard-deployment.yaml ) for Kuard with the following configuration. apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kuard spec: replicas: 1 template: metadata: labels: app: kuard spec: containers: - image: gcr.io/kuar-demo/kuard-amd64:1 imagePullPolicy: Always name: kuard ports: - containerPort: 8080 Deploy Kuard deployment file ( kuard-deployment.yaml ) to your cluster, using the following commands. % kubectl create -f kuard-deployment.yaml deployment.extensions/kuard created % kubectl get pod -l app=kuard NAME READY STATUS RESTARTS AGE kuard-6fc4d89bfb-djljt 1/1 Running 0 24s Create a service for the deployment. Create a file called service.yaml with the following configuration. apiVersion: v1 kind: Service metadata: name: kuard spec: ports: - port: 80 targetPort: 8080 protocol: TCP selector: app: kuard Deploy and verify the service using the following command. % kubectl create -f service.yaml service/kuard created % kubectl get svc kuard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kuard ClusterIP 10.103.49.171 none 80/TCP 13s Expose this service to outside world by creating an Ingress that is deployed on Citrix ADC CPX or VPX as Content switching virtual server. Note Ensure that you change kubernetes.io/ingress.class to your ingress class on which CIC is started. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kuard annotations: kubernetes.io/ingress.class: \"citrix\" spec: rules: - host: kuard.example.com http: paths: - backend: serviceName: kuard servicePort: 80 Important Change the value of spec.rules.host to the domain that you control. Ensure that a DNS entry exists to route the traffic to Citrix ADC CPX or VPX. Deploy the Ingress using the following command. % kubectl apply -f ingress.yml ingress.extensions/kuard created root@ubuntu-vivek-225:~/cert-manager# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE kuard kuard.example.com 80 7s Verify if the ingress is configured on Citrix ADC CPX or VPX using the following command. kubectl exec -it cpx-ingress-5b85d7c69d-ngd72 /bin/bash root@cpx-ingress-5b85d7c69d-ngd72:/# cli_script.sh 'sh cs vs' exec: sh cs vs 1) k8s-10.244.1.50:80:http (10.244.1.50:80) - HTTP Type: CONTENT State: UP Last state change was at Thu Feb 21 09:02:14 2019 Time since last state change: 0 days, 00:00:41.140 Client Idle Timeout: 180 sec Down state flush: ENABLED Disable Primary Vserver On Down : DISABLED Comment: uid=75VBGFO7NZXV7SCI4LSDJML2Q5X6FSNK6NXQPWGMDOYGBW2IMOGQ==== Appflow logging: ENABLED Port Rewrite : DISABLED State Update: DISABLED Default: Content Precedence: RULE Vserver IP and Port insertion: OFF L2Conn: OFF Case Sensitivity: ON Authentication: OFF 401 Based Authentication: OFF Push: DISABLED Push VServer: Push Label Rule: none Listen Policy: NONE IcmpResponse: PASSIVE RHIstate: PASSIVE Traffic Domain: 0 Done root@cpx-ingress-5b85d7c69d-ngd72:/# exit exit Verify if the page is correctly being served when requested using the curl command. % curl -sS -D - kuard.example.com -o /dev/null HTTP/1.1 200 OK Content-Length: 1458 Content-Type: text/html Date: Thu, 21 Feb 2019 09:09:05 GMT Configure Hashicorp Vault as Certificate Authority Set up an intermediate CA certificate signing request using Hashicorp Vault. This Vault endpoint is used by cert-manager to sign the certificate for the ingress resources. Prerequisites Ensure that you have installed the jq utility. Create a root CA For the sample workflow you can generate your own Root Certificate Authority within the Vault. In a production environment, you should use an external Root CA to sign the intermediate CA that Vault uses to generate certificates. If you have a root CA generated elsewhere, skip this step. Note PKI_ROOT is a path where you mount the root CA, typically it is 'pki'. ${DOMAIN} in this procedure is example.com % vault secrets enable -path= ${PKI_ROOT} pki # Set the max TTL for the root CA to 10 years % vault secrets tune -max-lease-ttl=87600h ${PKI_ROOT} % vault write -format=json ${PKI_ROOT} /root/generate/internal \\ common_name= ${DOMAIN} CA root ttl=87600h | tee \\ (jq -r .data.certificate ca.pem) \\ (jq -r .data.issuing_ca issuing_ca.pem) \\ (jq -r .data.private_key ca-key.pem) #Configure the CA and CRL URLs: % vault write ${PKI_ROOT} /config/urls \\ issuing_certificates= ${VAULT_ADDR}/v1/${PKI_ROOT}/ca \\ crl_distribution_points= ${VAULT_ADDR}/v1/${PKI_ROOT}/crl Generate an intermediate CA After creating the root CA, perform the following steps to create an intermediate CSR using the root CA. Enable pki from a different path PKI_INT from root CA, typically pki\\_int . Use the following command: % vault secrets enable -path=${PKI_INT} pki Set the max TTL to 3 year % vault secrets tune -max-lease-ttl=26280h ${PKI_INT} Generate CSR for ${DOMAIN} that needs to be signed by the root CA, the key is stored internally to vault. Use the following command: % vault write -format=json ${PKI_INT} /intermediate/generate/internal \\ common_name= ${DOMAIN} CA intermediate ttl=43800h | tee \\ (jq -r .data.csr pki_int.csr) \\ (jq -r .data.private_key pki_int.pem) Generate and sign the ${DOMAIN} certificate as an intermediate CA using root CA, store it as intermediate.cert.pem . Use the following command: % vault write -format=json ${PKI_ROOT} /root/sign-intermediate csr=@pki_int.csr \\ format=pem_bundle \\ | jq -r '.data.certificate' intermediate.cert.pem If you are using an external root CA, skip the above step and sign the CSR manually using root CA. Once the CSR is signed and the root CA returns a certificate, it needs to added back into the Vault using the following command: % vault write ${PKI_INT} /intermediate/set-signed certificate=@intermediate.cert.pem Set the CA and CRL location using the following command. vault write ${PKI_INT} /config/urls issuing_certificates= ${VAULT_ADDR}/v1/${PKI_INT}/ca crl_distribution_points= ${VAULT_ADDR}/v1/${PKI_INT}/crl An intermediate CA is setup and can be used to sign certificates for ingress resources. Configure a role A role is a logical name which maps to policies, administrator can control the certificate generation through the roles. Create a role for the intermediate CA that provides a set of policies for issuing or signing the certificates using this CA. There are many configurations that can be configured when creating roles, for more information see, Vault role documentation . For the workflow, create a role \" kube-ingress \" that allows you to sign certificates of ${DOMAIN} and its subdomains with a TTL of 90 days. # with a Max TTL of 90 days vault write ${PKI_INT}/roles/kube-ingress \\ allowed_domains=${DOMAIN} \\ allow_subdomains=true \\ max_ttl= 2160h Create Approle based authentication After configuring an intermediate CA to sign the certificates, you need to provide an authentication mechanism for cert-manager to use the vault for signing the certificates. Cert-manager supports Approle authentication method which provides a way for the applications to access the Vault defined roles. An \" AppRole \" represents a set of Vault policies and login constraints that must be met to receive a token with those policies. For detailed understanding of this authentication method, see Approle documentation . Create an Approle Create an approle named \" Kube-role \". The secret id for cert-manager should not be expired to use this Approle for authentication, hence do not set a TTL, or set it to 0. % vault auth enable approle of token ttl 5 minutes % vault write auth/approle/role/kube-role \\ token_ttl=5m \\ token_max_ttl=10m Associate a policy with the Approle Perform the following steps to associate a policy with an Approle. Create a file pki_int.hcl with the following configuration to allow the signing endpoints of the intermediate CA. path ${PKI_INT}/sign/* { capabilities = [ create , update ] } Add the file to a new policy called kube_allow_sign using the following command. vault policy write kube-allow-sign pki_int.hcl Update this policy to the approle using the following command. vault write auth/approle/role/kube-role policies=kube-allow-sign The kube-role approle allows you to sign the CSR with intermediate CA. Generate the Role id and Secret id Role id and Secret id is used by cert-manager to authenticate with Vault. Generate role id and Secret id and encode the secret_id with Base64. Perform the following: % vault read auth/approle/role/kube-role/role-id role_id db02de05-fa39-4855-059b-67221c5c2f63 % vault write -f auth/approle/role/kube-role/secret-id secret_id 6a174c20-f6de-a53c-74d2-6018fcceff64 secret_id_accessor c454f7e5-996e-7230-6074-6ef26b7bcf86 # encode secret_id with base64 % echo 6a174c20-f6de-a53c-74d2-6018fcceff64 | Base64 NmExNzRjMjAtZjZkZS1hNTNjLTc0ZDItNjAxOGZjY2VmZjY0Cg== Configure issuing certificates in Kubernetes After you have configured the Vault as intermediate CA, and the Approle authentication method for cert-manager to access the Vault. You need to configure the certificate for the ingress. Create a secret with Approle secret-id Perform the following to create a secret with Approle secret id. Create a secret file called secretid.yaml with following configuration. apiVersion: v1 kind: Secret type: Opaque metadata: name: cert-manager-vault-approle namespace: cert-manager data: secretId: NmExNzRjMjAtZjZkZS1hNTNjLTc0ZDItNjAxOGZjY2VmZjY0Cg== Note data.secretId is the base64 encoded Secret Id generated in Generate the Role id and Secret id . If you're using an Issuer resource in the next step, Secret must be in same namespace as the Issuer . For ClusterIssuer , secret must be in cert-manager namespace. Deploy the secret file ( secretid.yaml ) using the following command. % kubectl create -f secretid.yaml Deploy the Vault cluster issuer Certmanager supports two different CRDs for configuration, an Issuer , which is scoped to a single namespace, and a ClusterIssuer , which is cluster-wide. For the workflow, you need to use ClusterIssuer . Perform the following steps to deploy the Vault cluster issuer. Create a file called issuer-vault.yaml with the following configuration. apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: vault-issuer spec: vault: path: ${PKI_INT}/sign/kube-ingress server: https://vault_ip caBundle: base64 encoded caBundle PEM file auth: appRole: path: approle roleId: db02de05-fa39-4855-059b-67221c5c2f63 secretRef: name: cert-manager-vault-approle key: secretId Replace PKI_INT with appropriate path of the intermediate CA. SecretRef is the kubernetes secret name created in the previous step. Replace roleId with the role_id retrieved from Vault. An optional base64 encoded caBundle in PEM format can be provided to validate the TLS connection to the Vault Server. When caBundle is set it replaces the CA bundle inside the container running the cert-manager. This parameter has no effect if the connection used is in plain HTTP. Deploy the file ( issuer-vault.yaml ) using the following command. % kubectl create -f issuer-vault.yaml Using the following command verify if the Vault cluster issuer is successfully authenticated with the Vault. % kubectl describe clusterIssuer vault-issuer | tail -n 7 Conditions: Last Transition Time: 2019-02-26T06:18:40Z Message: Vault verified Reason: VaultVerified Status: True Type: Ready Events: none Create a \"certificate\" CRD object for the certificate Once the issuer is successfully registered , you need to get the certificate for the ingress domain kuard.example.com . You need to create a \"certificate\" resource with the commonName and dnsNames. For more information, see cert-manager documentation . You can specify multiple dnsNames which will be used for SAN field in the certificate. To create a \"certificate\" CRD object for the certificate, perform the following: Create a file called certificate.yaml with the following configuration. apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: kuard-example-tls namespace: default spec: secretName: kuard-example-tls issuerRef: kind: ClusterIssuer name: vault-issuer commonName: kuard.example.com duration: 720h #Renew before 7 days of expiry renewBefore: 168h commonName: kuard.example.com dnsNames: - www.kuard.example.com The certificate will have CN= kuard.example.com and SAN= Kuard.example.com,www.kuard.example.com . spec.secretName is the name of the secret where the certificate is stored after the certificate is issued successfully. Deploy the file ( certificate.yaml ) on the Kubernetes cluster using the following command. kubectl create -f certificate.yaml certificate.certmanager.k8s.io/kuard-example-tls created Verify if the certificate is issued You can watch the progress of the certificate as it is issued using the following command: % kubectl describe certificates kuard-example-tls | grep -A5 Events Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CertIssued 48s cert-manager Certificate issued successfully Important You may encounter some errors due to Vault policies. If you encounter any such errors, return to vault and fix it. After successful signing, a kubernetes.io/tls secret is created with the secretName specified in the Certificate resource. % kubectl get secret kuard-example-tls NAME TYPE DATA AGE kuard-exmaple-tls kubernetes.io/tls 3 4m20s Modify the ingress to use the generated secret Perform the following steps to modify the ingress to use the generated secret. Edit the original ingress and add a spec.tls section specifying the secret kuard-example-tls as follows. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kuard annotations: kubernetes.io/ingress.class: citrix spec: tls: - hosts: - kuard.example.com secretName: kuard-example-tls rules: - host: kuard.example.com http: paths: - backend: serviceName: kuard servicePort: 80 Deploy the ingress using the following command. % kubectl apply -f ingress.yml ingress.extensions/kuard created % kubectl get ingress kuard NAME HOSTS ADDRESS PORTS AGE kuard kuard.example.com 80, 443 12s Log on to CPX and verify if the Certificate is bound to the SSL virtual server. kubectl exec -it cpx-ingress-668bf6695f-4fwh8 bash cli_script.sh 'shsslvs' exec: shsslvs 1) Vserver Name: k8s-10.244.3.148:443:ssl DH: DISABLED DH Private-Key Exponent Size Limit: DISABLED Ephemeral RSA: ENABLED Refresh Count: 0 Session Reuse: ENABLED Timeout: 120 seconds Cipher Redirect: DISABLED SSLv2 Redirect: DISABLED ClearText Port: 0 Client Auth: DISABLED SSL Redirect: DISABLED Non FIPS Ciphers: DISABLED SNI: ENABLED OCSP Stapling: DISABLED HSTS: DISABLED HSTS IncludeSubDomains: NO HSTS Max-Age: 0 SSLv2: DISABLED SSLv3: ENABLED TLSv1.0: ENABLED TLSv1.1: ENABLED TLSv1.2: ENABLED TLSv1.3: DISABLED Push Encryption Trigger: Always Send Close-Notify: YES Strict Sig-Digest Check: DISABLED Zero RTT Early Data: DISABLED DHE Key Exchange With PSK: NO Tickets Per Authentication Context: 1 Done root@cpx-ingress-668bf6695f-4fwh8:/# cli_script.sh 'shsslvs k8s-10.244.3.148:443:ssl' exec: shsslvs k8s-10.244.3.148:443:ssl Advanced SSL configuration for VServer k8s-10.244.3.148:443:ssl: DH: DISABLED DH Private-Key Exponent Size Limit: DISABLED Ephemeral RSA: ENABLED Refresh Count: 0 Session Reuse: ENABLED Timeout: 120 seconds Cipher Redirect: DISABLED SSLv2 Redirect: DISABLED ClearText Port: 0 Client Auth: DISABLED SSL Redirect: DISABLED Non FIPS Ciphers: DISABLED SNI: ENABLED OCSP Stapling: DISABLED HSTS: DISABLED HSTS IncludeSubDomains: NO HSTS Max-Age: 0 SSLv2: DISABLED SSLv3: ENABLED TLSv1.0: ENABLED TLSv1.1: ENABLED TLSv1.2: ENABLED TLSv1.3: DISABLED Push Encryption Trigger: Always Send Close-Notify: YES Strict Sig-Digest Check: DISABLED Zero RTT Early Data: DISABLED DHE Key Exchange With PSK: NO Tickets Per Authentication Context: 1 , P_256, P_384, P_224, P_5216) CertKey Name: k8s-LMO3O3U6KC6WXKCBJAQY6K6X6JO Server Certificate for SNI 7) Cipher Name: DEFAULT Description: Default cipher list with encryption strength = 128bit Done root@cpx-ingress-668bf6695f-4fwh8:/# cli_script.sh 'sh certkey k8s-LMO3O3U6KC6WXKCBJAQY6K6X6JO' exec: sh certkey k8s-LMO3O3U6KC6WXKCBJAQY6K6X6JO Name: k8s-LMO3O3U6KC6WXKCBJAQY6K6X6JO Status: Valid, Days to expiration:0 Version: 3 Serial Number: 524C1D9306F784A2F5277C05C2A120D5258D9A2F Signature Algorithm: sha256WithRSAEncryption Issuer: CN=example.com CA intermediate Validity Not Before: Feb 26 06:48:39 2019 GMT Not After : Feb 27 06:49:09 2019 GMT Certificate Type: Client Certificate Server Certificate Subject: CN=kuard.example.com Public Key Algorithm: rsaEncryption Public Key size: 2048 Ocsp Response Status: NONE 2) URI:http://127.0.0.1:8200/v1/pki_int/crl 3) VServer name: k8s-10.244.3.148:443:ssl Server Certificate for SNI Done The HTTPS webserver is UP with the vault signed certificate. Cert-manager automatically renews the certificate as specified in the 'RenewBefore\" parameter in the Certificate, before expiry of the certificate. Note The vault signing of the certificate fails if the expiry of a certificate is beyond the expiry of the root CA or intermediate CA. You should ensure that the CA certificates are renewed in advance before the expiry.","title":"Deploy HTTPs web application on K8s with CIC and HashiCorp vault using cert-manager"},{"location":"certificate-management/vault/#deploying-https-web-application-on-kubernetes-with-citrix-ingress-controller-and-hashicorp-vault-using-cert-manager","text":"This topic provides a sample workflow that uses HashiCorp Vault as self-signed CA to automate TLS certificate provisioning, revocation, and renewal for ingress resources deployed with Citrix ingress controller using cert-manager. Specifically, the workflow uses the Vault PKI Secrets Engine to create a CA. This tutorial assumes that you have a Vault server installed and reachable from the Kubernetes cluster. The PKI secrets engine of Vault is suitable for internal applications and for external facing applications that require public trust, you can refer automating TLS certificates using letsencrypt CA The workflow uses a Vault secret engine and authentication methods, refer the following Vault documentation for full list of features: Vault Secrets Engines Vault Authentication Methods This topic provides you information on how to deploy an HTTPS web application on a Kubernetes cluster, using: Citrix ingress controller (CIC) JetStack's cert-manager to provision TLS certificates from Hashicorp Vault Hashicorp Vault","title":"Deploying HTTPS web application on Kubernetes with Citrix Ingress Controller and Hashicorp Vault using cert-manager"},{"location":"certificate-management/vault/#prerequisites","text":"Ensure that you have: The Vault server is installed, unsealed and is reachable from Kubernetes cluster. Enabled RBAC on your Kubernetes cluster. Deployed Citrix ADC MPX, VPX or CPX in Tier 1 or Tier 2 deployment model. In the Tier 1 deployment model, Citrix ADC MPX or VPX is used as an Application Delivery Controller (ADC) and Citrix Ingress Controller (CIC) running in Kubernetes cluster configures the virtual services for the services running on Kubernetes cluster. Citrix ADC runs the virtual service on the publicly routable IP address and offloads SSL for client traffic with the help of Let's Encrypt generated certificate. Similarly in the Tier 2 deployment model, a TCP service is configured on the Citrix ADC (VPX/MPX) running outside the Kubernetes cluster to forward the traffic to Citrix ADC CPX instances running in the Kubernetes cluster. Citrix ADC CPX ends the SSL session and load-balances the traffic to actual service pods. Deployed Citrix ingress controller. Click here for various deployment scenarios. Administrator permissions for all the deployment steps. If you encounter failures due to permissions, make sure you have administrator permission.","title":"Prerequisites"},{"location":"certificate-management/vault/#deploy-cert-manager-using-the-manifest-file","text":"Perform the following steps to deploy cert-manager using the supplied YAML manifest file. To keep things simple, skip cert-manager's Helm installation, and instead use the supplied YAML manifests. Download the latest source of cert-manager from github.com/jetstack/cert-manager repository using the following command. wget https://github.com/jetstack/cert-manager/archive/v0.6.2.tar.gz tar -zxvf v0.6.2.tar.gz Deploy the cert-manager using the following command. kubectl apply -f deploy/manifests/cert-manager.yaml You can also install the cert-manager with Helm, for more information see cert-manager documentation Verify that the cert-manager is up and running using the following command. % kubectl -n cert-manager get all NAME READY STATUS RESTARTS AGE pod/cert-manager-77fd74fb64-d68v7 1/1 Running 0 4m41s pod/cert-manager-webhook-67bf86d45-k77jj 1/1 Running 0 4m41s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cert-manager-webhook ClusterIP 10.108.161.154 none 443/TCP 13d NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cert-manager 1/1 1 1 13d deployment.apps/cert-manager-webhook 1/1 1 1 13d NAME DESIRED CURRENT READY AGE replicaset.apps/cert-manager-77fd74fb64 1 1 1 13d replicaset.apps/cert-manager-webhook-67bf86d45 1 1 1 13d NAME COMPLETIONS DURATION AGE job.batch/cert-manager-webhook-ca-sync 1/1 22s 13d job.batch/cert-manager-webhook-ca-sync-1549756800 1/1 21s 10d job.batch/cert-manager-webhook-ca-sync-1550361600 1/1 19s 3d8h NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE cronjob.batch/cert-manager-webhook-ca-sync @weekly False 0 3d8h 13d","title":"Deploy cert-manager using the manifest file"},{"location":"certificate-management/vault/#deploy-a-sample-web-application","text":"Perform the following steps to deploy a sample web application. Note Kuard , a kubernetes demo application is used for reference in this topic. Create a deployment YAML file ( kuard-deployment.yaml ) for Kuard with the following configuration. apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kuard spec: replicas: 1 template: metadata: labels: app: kuard spec: containers: - image: gcr.io/kuar-demo/kuard-amd64:1 imagePullPolicy: Always name: kuard ports: - containerPort: 8080 Deploy Kuard deployment file ( kuard-deployment.yaml ) to your cluster, using the following commands. % kubectl create -f kuard-deployment.yaml deployment.extensions/kuard created % kubectl get pod -l app=kuard NAME READY STATUS RESTARTS AGE kuard-6fc4d89bfb-djljt 1/1 Running 0 24s Create a service for the deployment. Create a file called service.yaml with the following configuration. apiVersion: v1 kind: Service metadata: name: kuard spec: ports: - port: 80 targetPort: 8080 protocol: TCP selector: app: kuard Deploy and verify the service using the following command. % kubectl create -f service.yaml service/kuard created % kubectl get svc kuard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kuard ClusterIP 10.103.49.171 none 80/TCP 13s Expose this service to outside world by creating an Ingress that is deployed on Citrix ADC CPX or VPX as Content switching virtual server. Note Ensure that you change kubernetes.io/ingress.class to your ingress class on which CIC is started. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kuard annotations: kubernetes.io/ingress.class: \"citrix\" spec: rules: - host: kuard.example.com http: paths: - backend: serviceName: kuard servicePort: 80 Important Change the value of spec.rules.host to the domain that you control. Ensure that a DNS entry exists to route the traffic to Citrix ADC CPX or VPX. Deploy the Ingress using the following command. % kubectl apply -f ingress.yml ingress.extensions/kuard created root@ubuntu-vivek-225:~/cert-manager# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE kuard kuard.example.com 80 7s Verify if the ingress is configured on Citrix ADC CPX or VPX using the following command. kubectl exec -it cpx-ingress-5b85d7c69d-ngd72 /bin/bash root@cpx-ingress-5b85d7c69d-ngd72:/# cli_script.sh 'sh cs vs' exec: sh cs vs 1) k8s-10.244.1.50:80:http (10.244.1.50:80) - HTTP Type: CONTENT State: UP Last state change was at Thu Feb 21 09:02:14 2019 Time since last state change: 0 days, 00:00:41.140 Client Idle Timeout: 180 sec Down state flush: ENABLED Disable Primary Vserver On Down : DISABLED Comment: uid=75VBGFO7NZXV7SCI4LSDJML2Q5X6FSNK6NXQPWGMDOYGBW2IMOGQ==== Appflow logging: ENABLED Port Rewrite : DISABLED State Update: DISABLED Default: Content Precedence: RULE Vserver IP and Port insertion: OFF L2Conn: OFF Case Sensitivity: ON Authentication: OFF 401 Based Authentication: OFF Push: DISABLED Push VServer: Push Label Rule: none Listen Policy: NONE IcmpResponse: PASSIVE RHIstate: PASSIVE Traffic Domain: 0 Done root@cpx-ingress-5b85d7c69d-ngd72:/# exit exit Verify if the page is correctly being served when requested using the curl command. % curl -sS -D - kuard.example.com -o /dev/null HTTP/1.1 200 OK Content-Length: 1458 Content-Type: text/html Date: Thu, 21 Feb 2019 09:09:05 GMT","title":"Deploy a sample web application"},{"location":"certificate-management/vault/#configure-hashicorp-vault-as-certificate-authority","text":"Set up an intermediate CA certificate signing request using Hashicorp Vault. This Vault endpoint is used by cert-manager to sign the certificate for the ingress resources.","title":"Configure Hashicorp Vault as Certificate Authority"},{"location":"certificate-management/vault/#prerequisites_1","text":"Ensure that you have installed the jq utility.","title":"Prerequisites"},{"location":"certificate-management/vault/#create-a-root-ca","text":"For the sample workflow you can generate your own Root Certificate Authority within the Vault. In a production environment, you should use an external Root CA to sign the intermediate CA that Vault uses to generate certificates. If you have a root CA generated elsewhere, skip this step. Note PKI_ROOT is a path where you mount the root CA, typically it is 'pki'. ${DOMAIN} in this procedure is example.com % vault secrets enable -path= ${PKI_ROOT} pki # Set the max TTL for the root CA to 10 years % vault secrets tune -max-lease-ttl=87600h ${PKI_ROOT} % vault write -format=json ${PKI_ROOT} /root/generate/internal \\ common_name= ${DOMAIN} CA root ttl=87600h | tee \\ (jq -r .data.certificate ca.pem) \\ (jq -r .data.issuing_ca issuing_ca.pem) \\ (jq -r .data.private_key ca-key.pem) #Configure the CA and CRL URLs: % vault write ${PKI_ROOT} /config/urls \\ issuing_certificates= ${VAULT_ADDR}/v1/${PKI_ROOT}/ca \\ crl_distribution_points= ${VAULT_ADDR}/v1/${PKI_ROOT}/crl","title":"Create a root CA"},{"location":"certificate-management/vault/#generate-an-intermediate-ca","text":"After creating the root CA, perform the following steps to create an intermediate CSR using the root CA. Enable pki from a different path PKI_INT from root CA, typically pki\\_int . Use the following command: % vault secrets enable -path=${PKI_INT} pki Set the max TTL to 3 year % vault secrets tune -max-lease-ttl=26280h ${PKI_INT} Generate CSR for ${DOMAIN} that needs to be signed by the root CA, the key is stored internally to vault. Use the following command: % vault write -format=json ${PKI_INT} /intermediate/generate/internal \\ common_name= ${DOMAIN} CA intermediate ttl=43800h | tee \\ (jq -r .data.csr pki_int.csr) \\ (jq -r .data.private_key pki_int.pem) Generate and sign the ${DOMAIN} certificate as an intermediate CA using root CA, store it as intermediate.cert.pem . Use the following command: % vault write -format=json ${PKI_ROOT} /root/sign-intermediate csr=@pki_int.csr \\ format=pem_bundle \\ | jq -r '.data.certificate' intermediate.cert.pem If you are using an external root CA, skip the above step and sign the CSR manually using root CA. Once the CSR is signed and the root CA returns a certificate, it needs to added back into the Vault using the following command: % vault write ${PKI_INT} /intermediate/set-signed certificate=@intermediate.cert.pem Set the CA and CRL location using the following command. vault write ${PKI_INT} /config/urls issuing_certificates= ${VAULT_ADDR}/v1/${PKI_INT}/ca crl_distribution_points= ${VAULT_ADDR}/v1/${PKI_INT}/crl An intermediate CA is setup and can be used to sign certificates for ingress resources.","title":"Generate an intermediate CA"},{"location":"certificate-management/vault/#configure-a-role","text":"A role is a logical name which maps to policies, administrator can control the certificate generation through the roles. Create a role for the intermediate CA that provides a set of policies for issuing or signing the certificates using this CA. There are many configurations that can be configured when creating roles, for more information see, Vault role documentation . For the workflow, create a role \" kube-ingress \" that allows you to sign certificates of ${DOMAIN} and its subdomains with a TTL of 90 days. # with a Max TTL of 90 days vault write ${PKI_INT}/roles/kube-ingress \\ allowed_domains=${DOMAIN} \\ allow_subdomains=true \\ max_ttl= 2160h","title":"Configure a role"},{"location":"certificate-management/vault/#create-approle-based-authentication","text":"After configuring an intermediate CA to sign the certificates, you need to provide an authentication mechanism for cert-manager to use the vault for signing the certificates. Cert-manager supports Approle authentication method which provides a way for the applications to access the Vault defined roles. An \" AppRole \" represents a set of Vault policies and login constraints that must be met to receive a token with those policies. For detailed understanding of this authentication method, see Approle documentation .","title":"Create Approle based authentication"},{"location":"certificate-management/vault/#create-an-approle","text":"Create an approle named \" Kube-role \". The secret id for cert-manager should not be expired to use this Approle for authentication, hence do not set a TTL, or set it to 0. % vault auth enable approle of token ttl 5 minutes % vault write auth/approle/role/kube-role \\ token_ttl=5m \\ token_max_ttl=10m","title":"Create an Approle"},{"location":"certificate-management/vault/#associate-a-policy-with-the-approle","text":"Perform the following steps to associate a policy with an Approle. Create a file pki_int.hcl with the following configuration to allow the signing endpoints of the intermediate CA. path ${PKI_INT}/sign/* { capabilities = [ create , update ] } Add the file to a new policy called kube_allow_sign using the following command. vault policy write kube-allow-sign pki_int.hcl Update this policy to the approle using the following command. vault write auth/approle/role/kube-role policies=kube-allow-sign The kube-role approle allows you to sign the CSR with intermediate CA.","title":"Associate a policy with the Approle"},{"location":"certificate-management/vault/#generate-the-role-id-and-secret-id","text":"Role id and Secret id is used by cert-manager to authenticate with Vault. Generate role id and Secret id and encode the secret_id with Base64. Perform the following: % vault read auth/approle/role/kube-role/role-id role_id db02de05-fa39-4855-059b-67221c5c2f63 % vault write -f auth/approle/role/kube-role/secret-id secret_id 6a174c20-f6de-a53c-74d2-6018fcceff64 secret_id_accessor c454f7e5-996e-7230-6074-6ef26b7bcf86 # encode secret_id with base64 % echo 6a174c20-f6de-a53c-74d2-6018fcceff64 | Base64 NmExNzRjMjAtZjZkZS1hNTNjLTc0ZDItNjAxOGZjY2VmZjY0Cg==","title":"Generate the Role id and Secret id"},{"location":"certificate-management/vault/#configure-issuing-certificates-in-kubernetes","text":"After you have configured the Vault as intermediate CA, and the Approle authentication method for cert-manager to access the Vault. You need to configure the certificate for the ingress.","title":"Configure issuing certificates in Kubernetes"},{"location":"certificate-management/vault/#create-a-secret-with-approle-secret-id","text":"Perform the following to create a secret with Approle secret id. Create a secret file called secretid.yaml with following configuration. apiVersion: v1 kind: Secret type: Opaque metadata: name: cert-manager-vault-approle namespace: cert-manager data: secretId: NmExNzRjMjAtZjZkZS1hNTNjLTc0ZDItNjAxOGZjY2VmZjY0Cg== Note data.secretId is the base64 encoded Secret Id generated in Generate the Role id and Secret id . If you're using an Issuer resource in the next step, Secret must be in same namespace as the Issuer . For ClusterIssuer , secret must be in cert-manager namespace. Deploy the secret file ( secretid.yaml ) using the following command. % kubectl create -f secretid.yaml","title":"Create a secret with Approle secret-id"},{"location":"certificate-management/vault/#deploy-the-vault-cluster-issuer","text":"Certmanager supports two different CRDs for configuration, an Issuer , which is scoped to a single namespace, and a ClusterIssuer , which is cluster-wide. For the workflow, you need to use ClusterIssuer . Perform the following steps to deploy the Vault cluster issuer. Create a file called issuer-vault.yaml with the following configuration. apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: vault-issuer spec: vault: path: ${PKI_INT}/sign/kube-ingress server: https://vault_ip caBundle: base64 encoded caBundle PEM file auth: appRole: path: approle roleId: db02de05-fa39-4855-059b-67221c5c2f63 secretRef: name: cert-manager-vault-approle key: secretId Replace PKI_INT with appropriate path of the intermediate CA. SecretRef is the kubernetes secret name created in the previous step. Replace roleId with the role_id retrieved from Vault. An optional base64 encoded caBundle in PEM format can be provided to validate the TLS connection to the Vault Server. When caBundle is set it replaces the CA bundle inside the container running the cert-manager. This parameter has no effect if the connection used is in plain HTTP. Deploy the file ( issuer-vault.yaml ) using the following command. % kubectl create -f issuer-vault.yaml Using the following command verify if the Vault cluster issuer is successfully authenticated with the Vault. % kubectl describe clusterIssuer vault-issuer | tail -n 7 Conditions: Last Transition Time: 2019-02-26T06:18:40Z Message: Vault verified Reason: VaultVerified Status: True Type: Ready Events: none","title":"Deploy the Vault cluster issuer"},{"location":"certificate-management/vault/#create-a-certificate-crd-object-for-the-certificate","text":"Once the issuer is successfully registered , you need to get the certificate for the ingress domain kuard.example.com . You need to create a \"certificate\" resource with the commonName and dnsNames. For more information, see cert-manager documentation . You can specify multiple dnsNames which will be used for SAN field in the certificate. To create a \"certificate\" CRD object for the certificate, perform the following: Create a file called certificate.yaml with the following configuration. apiVersion: certmanager.k8s.io/v1alpha1 kind: Certificate metadata: name: kuard-example-tls namespace: default spec: secretName: kuard-example-tls issuerRef: kind: ClusterIssuer name: vault-issuer commonName: kuard.example.com duration: 720h #Renew before 7 days of expiry renewBefore: 168h commonName: kuard.example.com dnsNames: - www.kuard.example.com The certificate will have CN= kuard.example.com and SAN= Kuard.example.com,www.kuard.example.com . spec.secretName is the name of the secret where the certificate is stored after the certificate is issued successfully. Deploy the file ( certificate.yaml ) on the Kubernetes cluster using the following command. kubectl create -f certificate.yaml certificate.certmanager.k8s.io/kuard-example-tls created","title":"Create a \"certificate\" CRD object for the certificate"},{"location":"certificate-management/vault/#verify-if-the-certificate-is-issued","text":"You can watch the progress of the certificate as it is issued using the following command: % kubectl describe certificates kuard-example-tls | grep -A5 Events Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CertIssued 48s cert-manager Certificate issued successfully Important You may encounter some errors due to Vault policies. If you encounter any such errors, return to vault and fix it. After successful signing, a kubernetes.io/tls secret is created with the secretName specified in the Certificate resource. % kubectl get secret kuard-example-tls NAME TYPE DATA AGE kuard-exmaple-tls kubernetes.io/tls 3 4m20s","title":"Verify if the certificate is issued"},{"location":"certificate-management/vault/#modify-the-ingress-to-use-the-generated-secret","text":"Perform the following steps to modify the ingress to use the generated secret. Edit the original ingress and add a spec.tls section specifying the secret kuard-example-tls as follows. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kuard annotations: kubernetes.io/ingress.class: citrix spec: tls: - hosts: - kuard.example.com secretName: kuard-example-tls rules: - host: kuard.example.com http: paths: - backend: serviceName: kuard servicePort: 80 Deploy the ingress using the following command. % kubectl apply -f ingress.yml ingress.extensions/kuard created % kubectl get ingress kuard NAME HOSTS ADDRESS PORTS AGE kuard kuard.example.com 80, 443 12s Log on to CPX and verify if the Certificate is bound to the SSL virtual server. kubectl exec -it cpx-ingress-668bf6695f-4fwh8 bash cli_script.sh 'shsslvs' exec: shsslvs 1) Vserver Name: k8s-10.244.3.148:443:ssl DH: DISABLED DH Private-Key Exponent Size Limit: DISABLED Ephemeral RSA: ENABLED Refresh Count: 0 Session Reuse: ENABLED Timeout: 120 seconds Cipher Redirect: DISABLED SSLv2 Redirect: DISABLED ClearText Port: 0 Client Auth: DISABLED SSL Redirect: DISABLED Non FIPS Ciphers: DISABLED SNI: ENABLED OCSP Stapling: DISABLED HSTS: DISABLED HSTS IncludeSubDomains: NO HSTS Max-Age: 0 SSLv2: DISABLED SSLv3: ENABLED TLSv1.0: ENABLED TLSv1.1: ENABLED TLSv1.2: ENABLED TLSv1.3: DISABLED Push Encryption Trigger: Always Send Close-Notify: YES Strict Sig-Digest Check: DISABLED Zero RTT Early Data: DISABLED DHE Key Exchange With PSK: NO Tickets Per Authentication Context: 1 Done root@cpx-ingress-668bf6695f-4fwh8:/# cli_script.sh 'shsslvs k8s-10.244.3.148:443:ssl' exec: shsslvs k8s-10.244.3.148:443:ssl Advanced SSL configuration for VServer k8s-10.244.3.148:443:ssl: DH: DISABLED DH Private-Key Exponent Size Limit: DISABLED Ephemeral RSA: ENABLED Refresh Count: 0 Session Reuse: ENABLED Timeout: 120 seconds Cipher Redirect: DISABLED SSLv2 Redirect: DISABLED ClearText Port: 0 Client Auth: DISABLED SSL Redirect: DISABLED Non FIPS Ciphers: DISABLED SNI: ENABLED OCSP Stapling: DISABLED HSTS: DISABLED HSTS IncludeSubDomains: NO HSTS Max-Age: 0 SSLv2: DISABLED SSLv3: ENABLED TLSv1.0: ENABLED TLSv1.1: ENABLED TLSv1.2: ENABLED TLSv1.3: DISABLED Push Encryption Trigger: Always Send Close-Notify: YES Strict Sig-Digest Check: DISABLED Zero RTT Early Data: DISABLED DHE Key Exchange With PSK: NO Tickets Per Authentication Context: 1 , P_256, P_384, P_224, P_5216) CertKey Name: k8s-LMO3O3U6KC6WXKCBJAQY6K6X6JO Server Certificate for SNI 7) Cipher Name: DEFAULT Description: Default cipher list with encryption strength = 128bit Done root@cpx-ingress-668bf6695f-4fwh8:/# cli_script.sh 'sh certkey k8s-LMO3O3U6KC6WXKCBJAQY6K6X6JO' exec: sh certkey k8s-LMO3O3U6KC6WXKCBJAQY6K6X6JO Name: k8s-LMO3O3U6KC6WXKCBJAQY6K6X6JO Status: Valid, Days to expiration:0 Version: 3 Serial Number: 524C1D9306F784A2F5277C05C2A120D5258D9A2F Signature Algorithm: sha256WithRSAEncryption Issuer: CN=example.com CA intermediate Validity Not Before: Feb 26 06:48:39 2019 GMT Not After : Feb 27 06:49:09 2019 GMT Certificate Type: Client Certificate Server Certificate Subject: CN=kuard.example.com Public Key Algorithm: rsaEncryption Public Key size: 2048 Ocsp Response Status: NONE 2) URI:http://127.0.0.1:8200/v1/pki_int/crl 3) VServer name: k8s-10.244.3.148:443:ssl Server Certificate for SNI Done The HTTPS webserver is UP with the vault signed certificate. Cert-manager automatically renews the certificate as specified in the 'RenewBefore\" parameter in the Certificate, before expiry of the certificate. Note The vault signing of the certificate fails if the expiry of a certificate is beyond the expiry of the root CA or intermediate CA. You should ensure that the CA certificates are renewed in advance before the expiry.","title":"Modify the ingress to use the generated secret"},{"location":"configure/annotations/","text":"Annotations The following are the annotations supported by Citrix: Annotations Possible value Description Default ingress.citrix.com/frontend-ip IP address Use this annotation to customize the virtual IP address (VIP). This IP address is configured in Citrix ADC as VIP. The annotation is mandatory if you are using Citrix ADC VPX or MPX. Note: Do not use the annotation if you want to use the Citrix ADC IP address as VIP. Citrix ADC IP address is used as VIP. ingress.citrix.com/secure-port Port number Use this annotation to configure the port for HTTPS traffic. This port is configured in Citrix ADC as a port value for corresponding Content Switching (CS) virtual server. 443 ingress.citrix.com/insecure-port Port number Use this annotation to configure the port for HTTP traffic. This port is configured in Citrix ADC as a port value for corresponding CS virtual server 80 ingress.citrix.com/insecure-termination one of Use allow to allow HTTP traffic, Use redirect to redirect the HTTP request to HTTPS, or Use disallow if you want to drop the HTTP traffic. disallow For example: ingress.citrix.com/insecure-termination: \"redirect\" ingress.citrix.com/secure-backend In JSON form, list of services for secure-backend Use True , if you want to establish secure HTTPS between Citrix ADC and the application, Use False , if you want to establish insecure HTTP connection Citrix ADC to the application False For example: ingress.citrix.com/secure-backend: {\u2018app1\u2019:\"True\", \u2018app2\u2019:\"False\", \u2018app3\u2019:\"True\"} kubernetes.io/ingress.class ingress class name It is a way to associate a particular ingress resource with an ingress controller. Configures all ingresses For example: kubernetes.io/ingress.class:\"Citrix\" ingress.citrix.com/insecure-service-type Any one of tcp or udp The annotation allows L4 load balancing with tcp/udp/any as protocol. Use tcp , if you want TCP as the protocol. Use udp , if you want udp as the protocol http ingress.citrix.com/service_weights In JSON form, weights distribution (in %) among the backend services. Sum of weight should be 100% It allows CIC to play a role in canary deployment. The values must be in JSON format. For each backend app in the ingress, there should be corresponding traffic %. All weights should be in % and sum should be 100 No weight distribution For example: ingress.citrix.com/service_weights: {\u2018canary-app1\u2019:5, \u2018baseline-app1\u2019:5 \u2018production-app1\u2019:90} Here there are 3 apps and % traffic distribution is 5%, 5%, and 90% ingress.citrix.com/lbvserver In JSON form, settings for lb virtual server or service group This provides smart annotation capability. Using this, an advanced user (who has knowledge of NetScaler LB virtual server and Service group options) can directly apply them. Values must be in JSON format. For each backend app in the ingress, provide key value pair. Key name should match with the corresponding CLI name Default options provided by Citrix ADC For example: ingress.citrix.com/lbvserver: '{\"app-1\":{\"lbmethod\":\"ROUNDROBIN\"}}' ingress.citrix.com/servicegroup: '{\"app-1\":{\"maxReq\":\"100\"}}' Here for app-1, you want to configure ROUND-ROBIN load balance method at LB level and maxReq at service group","title":"Annotations"},{"location":"configure/annotations/#annotations","text":"The following are the annotations supported by Citrix: Annotations Possible value Description Default ingress.citrix.com/frontend-ip IP address Use this annotation to customize the virtual IP address (VIP). This IP address is configured in Citrix ADC as VIP. The annotation is mandatory if you are using Citrix ADC VPX or MPX. Note: Do not use the annotation if you want to use the Citrix ADC IP address as VIP. Citrix ADC IP address is used as VIP. ingress.citrix.com/secure-port Port number Use this annotation to configure the port for HTTPS traffic. This port is configured in Citrix ADC as a port value for corresponding Content Switching (CS) virtual server. 443 ingress.citrix.com/insecure-port Port number Use this annotation to configure the port for HTTP traffic. This port is configured in Citrix ADC as a port value for corresponding CS virtual server 80 ingress.citrix.com/insecure-termination one of Use allow to allow HTTP traffic, Use redirect to redirect the HTTP request to HTTPS, or Use disallow if you want to drop the HTTP traffic. disallow For example: ingress.citrix.com/insecure-termination: \"redirect\" ingress.citrix.com/secure-backend In JSON form, list of services for secure-backend Use True , if you want to establish secure HTTPS between Citrix ADC and the application, Use False , if you want to establish insecure HTTP connection Citrix ADC to the application False For example: ingress.citrix.com/secure-backend: {\u2018app1\u2019:\"True\", \u2018app2\u2019:\"False\", \u2018app3\u2019:\"True\"} kubernetes.io/ingress.class ingress class name It is a way to associate a particular ingress resource with an ingress controller. Configures all ingresses For example: kubernetes.io/ingress.class:\"Citrix\" ingress.citrix.com/insecure-service-type Any one of tcp or udp The annotation allows L4 load balancing with tcp/udp/any as protocol. Use tcp , if you want TCP as the protocol. Use udp , if you want udp as the protocol http ingress.citrix.com/service_weights In JSON form, weights distribution (in %) among the backend services. Sum of weight should be 100% It allows CIC to play a role in canary deployment. The values must be in JSON format. For each backend app in the ingress, there should be corresponding traffic %. All weights should be in % and sum should be 100 No weight distribution For example: ingress.citrix.com/service_weights: {\u2018canary-app1\u2019:5, \u2018baseline-app1\u2019:5 \u2018production-app1\u2019:90} Here there are 3 apps and % traffic distribution is 5%, 5%, and 90% ingress.citrix.com/lbvserver In JSON form, settings for lb virtual server or service group This provides smart annotation capability. Using this, an advanced user (who has knowledge of NetScaler LB virtual server and Service group options) can directly apply them. Values must be in JSON format. For each backend app in the ingress, provide key value pair. Key name should match with the corresponding CLI name Default options provided by Citrix ADC For example: ingress.citrix.com/lbvserver: '{\"app-1\":{\"lbmethod\":\"ROUNDROBIN\"}}' ingress.citrix.com/servicegroup: '{\"app-1\":{\"maxReq\":\"100\"}}' Here for app-1, you want to configure ROUND-ROBIN load balance method at LB level and maxReq at service group","title":"Annotations"},{"location":"configure/ingress-classes/","text":"Ingress class support What is Ingress Class? In Kubernetes cluster, there might be multiple ingress controllers, and you need to have a way to associate a particular ingress resource with an ingress controller. kubernetes.io/ingress.class annotation in the ingress resource enables multiple ingress controllers to run effectively in the cluster. Otherwise every ingress resource is processed by all the ingress controllers in the cluster. Citrix Ingress Controller and Ingress classes Citrix Ingress Controller provides a support to accept multiple ingress resources, which have kuberneters.io/ingress.class annotation. Each ingress resource can be associated with only one ingress.class . However Ingress Controller might need to handle various ingress resources from different classes. You can associate Ingress Controller with multiple ingress classes using --ingress-classes argument under spec section of the YAML file. If ingress-classes are not specified for Ingress Controller, then it accepts all ingress resources irrespective of the presence of kubernetes.io/ingress.class annotation in the ingress object. If ingress-classes are specified, then Ingress Controller accepts only those ingress resources for which match the kubernetes.io/ingress.class annotation. Ingress resource without ingress.class annotation is not handled by Ingress Controller in the given case. Note Ingress class names are case-insensitive. Sample yaml configurations with ingress-classes Following is the snippet from a sample yaml file to associate ingress-classes with the Ingress Controller. This works in both cases where Ingress Controller runs as a standalone pod or runs as sidecar with Citrix ADC CPX. In the given yaml snippet, following ingress classes are associated with the Ingress Controller. my-custom-class Citrix spec: serviceAccountName: cic-k8s-role containers: - name: cic-k8s-ingress-controller image: quayio/citrix/citrix-k8s-ingress-controller:latest # specify the ingress classes names to be supportedbyIngress Controller in args section. # First line should be --ingress-classes, andeverysubsequent line should be # the name of allowed ingress class. In the givenexampletwo classes named # citrix and my-custom-class are accepted. Thiswill be case-insensitive. args: - --ingress-classes Citrix my-custom-class Following is the snippet from an Ingress yaml file where Ingress class association is depicted. In the given example, Ingress resource named web-ingress is associated with the ingress class my-custom-class . If Citrix Ingress Controller is configured to accept my-custom-class , it processes this Ingress resource. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: web-ingress annotations: kubernetes.io/ingress.class: my-custom-class","title":"Ingress class"},{"location":"configure/ingress-classes/#ingress-class-support","text":"","title":"Ingress class support"},{"location":"configure/ingress-classes/#what-is-ingress-class","text":"In Kubernetes cluster, there might be multiple ingress controllers, and you need to have a way to associate a particular ingress resource with an ingress controller. kubernetes.io/ingress.class annotation in the ingress resource enables multiple ingress controllers to run effectively in the cluster. Otherwise every ingress resource is processed by all the ingress controllers in the cluster.","title":"What is Ingress Class?"},{"location":"configure/ingress-classes/#citrix-ingress-controller-and-ingress-classes","text":"Citrix Ingress Controller provides a support to accept multiple ingress resources, which have kuberneters.io/ingress.class annotation. Each ingress resource can be associated with only one ingress.class . However Ingress Controller might need to handle various ingress resources from different classes. You can associate Ingress Controller with multiple ingress classes using --ingress-classes argument under spec section of the YAML file. If ingress-classes are not specified for Ingress Controller, then it accepts all ingress resources irrespective of the presence of kubernetes.io/ingress.class annotation in the ingress object. If ingress-classes are specified, then Ingress Controller accepts only those ingress resources for which match the kubernetes.io/ingress.class annotation. Ingress resource without ingress.class annotation is not handled by Ingress Controller in the given case. Note Ingress class names are case-insensitive.","title":"Citrix Ingress Controller and Ingress classes"},{"location":"configure/ingress-classes/#sample-yaml-configurations-with-ingress-classes","text":"Following is the snippet from a sample yaml file to associate ingress-classes with the Ingress Controller. This works in both cases where Ingress Controller runs as a standalone pod or runs as sidecar with Citrix ADC CPX. In the given yaml snippet, following ingress classes are associated with the Ingress Controller. my-custom-class Citrix spec: serviceAccountName: cic-k8s-role containers: - name: cic-k8s-ingress-controller image: quayio/citrix/citrix-k8s-ingress-controller:latest # specify the ingress classes names to be supportedbyIngress Controller in args section. # First line should be --ingress-classes, andeverysubsequent line should be # the name of allowed ingress class. In the givenexampletwo classes named # citrix and my-custom-class are accepted. Thiswill be case-insensitive. args: - --ingress-classes Citrix my-custom-class Following is the snippet from an Ingress yaml file where Ingress class association is depicted. In the given example, Ingress resource named web-ingress is associated with the ingress class my-custom-class . If Citrix Ingress Controller is configured to accept my-custom-class , it processes this Ingress resource. apiVersion: extensions/v1beta1 kind: Ingress metadata: name: web-ingress annotations: kubernetes.io/ingress.class: my-custom-class","title":"Sample yaml configurations with ingress-classes"},{"location":"configure/logging/","text":"Logging The logs generated by Citrix Ingress Controller(CIC) are available as part of kubernetes logs . You can specify CIC to log in the following log levels: CRITICAL ERROR WARNING INFO DEBUG By default, CIC is set to log in INFO log level. If you want to specify CIC to log in a particular log level then you need to specify the log level in the CIC yaml file before deploying the CIC. You can specify the log level in the spec section of the yaml file as follows: apiVersion: v1 kind: Pod metadata: name: citrixingresscontroller labels: app: citrixingresscontroller spec: serviceAccountName: cpx containers: - name: citrixingresscontroller image: quay.io/citrix/citrix-k8s-ingress-controller:latest env: # Set kube api-server URL - name: kubernetes_url value: https://10.x.x.x:6443 # Set NetScaler Management IP - name: NS_IP value: 10.x.x.x # Set log level - name: LOGLEVEL value: DEBUG - name: EULA value: yes args: - --feature-node-watch true imagePullPolicy: Always Modifying the log levels in CIC To modify the log level configured on the CIC instance, you need to delete the instance and update the log level value in the following section and redeploy the CIC instance: # Set log level - name: LOGLEVEL value: XXXX Once you update the log level, save the YAML file and deploy it using the following command: kubectl create -f citrix-k8s-ingress-controller.yaml","title":"Logging"},{"location":"configure/logging/#logging","text":"The logs generated by Citrix Ingress Controller(CIC) are available as part of kubernetes logs . You can specify CIC to log in the following log levels: CRITICAL ERROR WARNING INFO DEBUG By default, CIC is set to log in INFO log level. If you want to specify CIC to log in a particular log level then you need to specify the log level in the CIC yaml file before deploying the CIC. You can specify the log level in the spec section of the yaml file as follows: apiVersion: v1 kind: Pod metadata: name: citrixingresscontroller labels: app: citrixingresscontroller spec: serviceAccountName: cpx containers: - name: citrixingresscontroller image: quay.io/citrix/citrix-k8s-ingress-controller:latest env: # Set kube api-server URL - name: kubernetes_url value: https://10.x.x.x:6443 # Set NetScaler Management IP - name: NS_IP value: 10.x.x.x # Set log level - name: LOGLEVEL value: DEBUG - name: EULA value: yes args: - --feature-node-watch true imagePullPolicy: Always","title":"Logging"},{"location":"configure/logging/#modifying-the-log-levels-in-cic","text":"To modify the log level configured on the CIC instance, you need to delete the instance and update the log level value in the following section and redeploy the CIC instance: # Set log level - name: LOGLEVEL value: XXXX Once you update the log level, save the YAML file and deploy it using the following command: kubectl create -f citrix-k8s-ingress-controller.yaml","title":"Modifying the log levels in CIC"},{"location":"crds/rewrite-responder/","text":"Using Rewrite and Responder policies in Kubernetes In kubernetes environment, to deploy specific layer 7 policies to handle scenarios such as, redirecting HTTP traffic to a specific URL, blocking a set of IP addresses to mitigate DDoS attacks, imposing HTTP to HTTPS and so on, requires you to add appropriate libraries within the microservices and manually configure the policies. Instead, you can use the Rewrite and Responder features provided by the Ingress Citrix ADC device to deploy these policies. Citrix provides Kubernetes CustomResourceDefinitions (CRDs) that you can use along with Citrix Ingress Controller (CIC) and automate the configurations and deployment of these policies on the Citrix ADCs used as Ingress devices. The Rewrite and Responder CRD provided by Citrix is designed to expose a set of tools used in front-line Citrix ADCs. Using these functionalities you can rewrite the header and payload of ingress and egress HTTP traffic as well as respond to HTTP traffic on behalf of a microservice. Once you deploy the Rewrite and Responder CRD in the Kubernetes cluster. You can define extensive rewrite and responder policies using datasets, patsets, and string maps and also enable audit logs for statistics on the ingress device. For more information on rewrite and responder policy feature provided by Citrix ADC, see Rewrite policy and Responder policy . Deploying the Citrix CRD The Citrix Rewrite and Responder CRD deployment YAML file: rewrite-responder-policies-deployment.yaml . Note Ensure that you do not modify the deployment YAML file. Deploy the CRD, using the following command: root@master:~# kubectl create -f rewrite-responder-policies-deployment.yaml customresourcedefinition.apiextensions.k8s.io/rewritepolicies.citrix.com created CRD attributes The CRD provides attributes for various options required to define the rewrite and responder policies. Also, it provides attributes for dataset, patset, string map, and audit logs to use within the rewrite and responder policies. These CRD attributes correspond to Citrix ADC command and attribute respectively. Rewrite policy The following table lists the CRD attributes that you can use to define a rewrite policy. Also, the table lists the corresponding Citrix ADC command and attributes. CRD attribute Citrix ADC command Citrix ADC attribute rewrite-criteria Add rewrite policy rule default-action Add rewrite policy undefAction operation Add rewrite action type target Add rewrite action target modify-expression Add rewrite action stringBuilderExpr multiple-occurence-modify Add rewrite action Search additional-multiple-occurence-modify Add rewrite action RefineSearch Direction Bind lb vserver Type Responder policy The following table lists the CRD attributes that you can use to define a responder policy. Also, the table lists the corresponding Citrix ADC command and attributes. CRD attribute Citrix ADC command Citrix ADC attribute Redirect Add responder action Type (the value of type) url Add responder action Target redirect-status-code Add responder action responseStatusCode redirect-reason Add responder action reasonPhrase Respond-with Add responder action Type (the value of type) http-payload-string Add responder action Target Noop Add responder policy Action (the value of action) Reset Add responder policy Action (the value of action) Drop Add responder policy Action (the value of action) Respond-criteria Add responder policy Rule Default-action Add responder policy undefAction Audit log The following table lists the CRD attributes provide to enable audit log within the rewrite or responder policies. Also, the table lists the corresponding Citrix ADC command and attributes. CRD attribute Citrix ADC command Citrix ADC attribute Logexpression Add audit message action stringBuilderExpr Loglevel Add audit message action Loglevel Dataset The following table lists the CRD attributes for dataset that you can use within the rewrite or responder policies. Also, the table lists the corresponding Citrix ADC command and attributes. CRD attribute Citrix ADC command Citrix ADC attribute Name Add policy dataset Name Type Add policy dataset Type Values Bind policy dataset Value Patset CRD attribute Citrix ADC command Citrix ADC attribute Name Add policy patset Name Values Bind policy patset string String map CRD attribute Citrix ADC command Citrix ADC attribute Name Add policy stringmap Name Key Bind policy stringmap Key Value Bind policy stringmap Value How to write a policy configuration After you have deployed the CRD provided by Citrix in the Kubernetes cluster, you can define the policy configuration in a .yaml file. In the .yaml file, use rewritepolicy in the kind field and based on your requirement add any of the following individual sections in spec for policy configuration. rewrite-policy - To define rewrite policy configuration. responder-policy - To define responder policy configuration. logpackets - To enable audit logs. dataset - To use dataset for extensive policy configuration. patset - To use patset for extensive policy configuration. stringmaps - To use string maps for extensive policy configuration. In these sections, you need to use the CRD attributes provided for respective policy configuration (rewrite or responder) to define the policy. Also, in the spec you need to include a rewrite-policies section to specify the service or services to which the policy needs to be applied. For more information, see Sample policy configurations . After you deploy the .yaml file, the Citrix Ingress Controller (CIC) applies the policy configuration on the Ingress Citrix ADC device. Points to note: If the CRD is associated with a namespace then, by default, the policy is applied to the services associated with the namespace. For example, if you have the same service name associated with multiple namespaces, then the policy is applied to the service that belongs to the namespace associated with the CRD. If you have defined multiple policies in a single .yaml file then the first policy configuration defined in the file takes priority and the subsequent policy configurations is applied as per the sequence. If you have multiple policies defined in different files then the first policy configuration defined in the file that you deployed first takes priority. Consider a scenario wherein you want to define a policy in Citrix ADC to rewrite all the incoming URLs to new-url-for-the-application and send it to the microservices. Create a .yaml file called target-url-rewrite.yaml and use the appropriate CRD attributes to define the rewrite policy as follows: target-url-rewrite.yaml: apiVersion: citrix.com/v1 kind: rewritepolicy metadata: name: targeturlrewrite spec: rewrite-policies: - servicenames: - citrix-svc logpackets: logexpression: http.req.url loglevel: INFORMATIONAL rewrite-policy: operation: replace target: 'http.req.url' modify-expression: ' new-url-for-the-application ' comment: 'Target URL Rewrite - rewrite the url of the HTTP request' direction: REQUEST rewrite-criteria: 'http.req.is_valid' After you have defined the policy configuration, deploy the .yaml file using the following command: root@master:demo#kubectl create -f target-url-rewrite.yaml After you deploy the .yaml file, the Citrix Ingress Controller (CIC) applies the policy configuration on the Ingress Citrix ADC device. On the master node in the Kubernetes cluster, you can verify if the rewrite policy CRD is created on the CIC using the following command: root@master:~# kubectl logs citrixingresscontroller | grep -i 'SUCCESS\\|FAILURE\\|exception' You can view an entry in the logs as shown in the following image: Also, you can verify if the configuration is applied on the Citrix ADC, do the following: Log on to the Citrix ADC command-line. Use the following command to verify if the configuration is applied to the Citrix ADC: $ show run | grep `lb vserver` add lb vserver k8s-citrix.default.80.k8s-citrix-svc.default.http HTTP 0.0.0.0 0 -persistenceType NONE -lbMethod ROUNDROBIN -cltTimeout 180 -coment uid=67d18ffb-261e-11e9-aad9-8e30e16c8143.ver=1692325 bind lb vserver k8s-citrix.default.80.k8s-citrix-svc.default.http k8s-citrix.default.80.k8s-citrix-svc.default.http bind lb vserver k8s-citrix.default.80.k8s-citrix-svc.default.http k8s-citrix.default.80.k8s-citrix-svc.default.http -policyname k8s_rwpolicy_crd_targeturlrewrite_0_default_1719072 -priority 1006 -gotoPriorityExpression END -type REQUEST You can verify that the policy k8s_rwpolicy_crd_targeturlrewrite_0_default_1719072 is bound to the load balancing virtual server. Sample policy configurations Responder policy configuration black-list-urls.yaml: apiVersion: citrix.com/v1 kind: rewritepolicy metadata: name: blacklisturls spec: responder-policies: - servicenames: - citrix-svc responder-policy: respondwith: http-payload-string: ' HTTP/1.1 401 Access denied ' respond-criteria: 'http.req.url.equals_any( blacklistUrls )' comment: 'Blacklist certain Urls' patset: - name: blacklistUrls values: - '/app1' - '/app2' - '/app3' In this example, if Citrix ADC receives any URL that matches the /app1 , /app2 , or /app3 strings defined in the patset , Citrix ADC blocks the URL. Policy with audit log enabled black-list-urls-audit-log.yaml: apiVersion: citrix.com/v1 kind: rewritepolicy metadata: name: blacklisturls spec: responder-policies: - servicenames: - citrix-svc logpackets: logexpression: http.req.url loglevel: INFORMATIONAL responder-policy: respondwith: http-payload-string: ' HTTP/1.1 401 Access denied ' respond-criteria: 'http.req.url.equals_any( blacklistUrls )' comment: 'Blacklist certain Urls' patset: - name: blacklistUrls values: - '/app1' - '/app2' - '/app3' Multiple policy configurations You can add multiple policy configurations in a single .yaml file and apply the policies to the Citrix ADC device. You need add separate sections for each policy configuration. multi-policy-config.yaml: apiVersion: citrix.com/v1 kind: rewritepolicy metadata: name: multipolicy spec: responder-policies: - servicenames: - citrix-svc responder-policy: redirect: url: ' www.citrix.com ' respond-criteria: 'client.ip.src.TYPECAST_text_t.equals_any( redirectIPs )' comment: 'Redirect IPs to citrix.com' - servicenames: - citrix-svc responder-policy: redirect: url: 'HTTP.REQ.HOSTNAME+http.req.url.MAP_STRING_DEFAULT_TO_KEY( modifyurls )' respond-criteria: 'http.req.is_valid' comment: 'modify specific URLs' rewrite-policies: - servicenames: - citrix-svc rewrite-policy: operation: insert_http_header target: 'sessionID' modify-expression: ' 48592th42gl24456284536tgt2 ' comment: 'insert SessionID in header' direction: RESPONSE rewrite-criteria: 'http.res.is_valid' dataset: - name: redirectIPs type: ipv4 values: - 1.1.1.1 - 2.2.2.2 stringmap: - name: modifyurls comment: Urls to be modified string values: - key: ' /app1/ ' value: ' /internal-app1/ ' - key: ' /app2/ ' value: ' /internal-app2/ ' The example contains two responder policies and a rewrite policy, based on these policies the Citrix ADC device performs the following: Any client request to IP addresses defined in the redirectIP dataset, that is, 1.1.1.1 or 2.2.2.2 respectively, the request is redirected to www.citrix.com . Any incoming URL with strings provided in the modifyurls stringmap is modified to the value provided in the stringmap. For example, if the incoming URL has the string /app1/ is modified to /internal-app1/ Adds a session ID as a new header in the response to the client. Related articles Feature Documentation Citrix ADC Rewrite Feature Documentation Citrix ADC Responder Feature Documentation Developer Documentation Citrix ADC Rewrite Policy Citrix ADC Rewrite Action Citrix ADC Responder Policy Citrix ADC Responder Action Citrix ADC Audit Message Action Citrix ADC Policy Dataset","title":"Using Rewrite and Responder policies in Kubernetes"},{"location":"crds/rewrite-responder/#using-rewrite-and-responder-policies-in-kubernetes","text":"In kubernetes environment, to deploy specific layer 7 policies to handle scenarios such as, redirecting HTTP traffic to a specific URL, blocking a set of IP addresses to mitigate DDoS attacks, imposing HTTP to HTTPS and so on, requires you to add appropriate libraries within the microservices and manually configure the policies. Instead, you can use the Rewrite and Responder features provided by the Ingress Citrix ADC device to deploy these policies. Citrix provides Kubernetes CustomResourceDefinitions (CRDs) that you can use along with Citrix Ingress Controller (CIC) and automate the configurations and deployment of these policies on the Citrix ADCs used as Ingress devices. The Rewrite and Responder CRD provided by Citrix is designed to expose a set of tools used in front-line Citrix ADCs. Using these functionalities you can rewrite the header and payload of ingress and egress HTTP traffic as well as respond to HTTP traffic on behalf of a microservice. Once you deploy the Rewrite and Responder CRD in the Kubernetes cluster. You can define extensive rewrite and responder policies using datasets, patsets, and string maps and also enable audit logs for statistics on the ingress device. For more information on rewrite and responder policy feature provided by Citrix ADC, see Rewrite policy and Responder policy .","title":"Using Rewrite and Responder policies in Kubernetes"},{"location":"crds/rewrite-responder/#deploying-the-citrix-crd","text":"The Citrix Rewrite and Responder CRD deployment YAML file: rewrite-responder-policies-deployment.yaml . Note Ensure that you do not modify the deployment YAML file. Deploy the CRD, using the following command: root@master:~# kubectl create -f rewrite-responder-policies-deployment.yaml customresourcedefinition.apiextensions.k8s.io/rewritepolicies.citrix.com created","title":"Deploying the Citrix CRD"},{"location":"crds/rewrite-responder/#crd-attributes","text":"The CRD provides attributes for various options required to define the rewrite and responder policies. Also, it provides attributes for dataset, patset, string map, and audit logs to use within the rewrite and responder policies. These CRD attributes correspond to Citrix ADC command and attribute respectively.","title":"CRD attributes"},{"location":"crds/rewrite-responder/#rewrite-policy","text":"The following table lists the CRD attributes that you can use to define a rewrite policy. Also, the table lists the corresponding Citrix ADC command and attributes. CRD attribute Citrix ADC command Citrix ADC attribute rewrite-criteria Add rewrite policy rule default-action Add rewrite policy undefAction operation Add rewrite action type target Add rewrite action target modify-expression Add rewrite action stringBuilderExpr multiple-occurence-modify Add rewrite action Search additional-multiple-occurence-modify Add rewrite action RefineSearch Direction Bind lb vserver Type","title":"Rewrite policy"},{"location":"crds/rewrite-responder/#responder-policy","text":"The following table lists the CRD attributes that you can use to define a responder policy. Also, the table lists the corresponding Citrix ADC command and attributes. CRD attribute Citrix ADC command Citrix ADC attribute Redirect Add responder action Type (the value of type) url Add responder action Target redirect-status-code Add responder action responseStatusCode redirect-reason Add responder action reasonPhrase Respond-with Add responder action Type (the value of type) http-payload-string Add responder action Target Noop Add responder policy Action (the value of action) Reset Add responder policy Action (the value of action) Drop Add responder policy Action (the value of action) Respond-criteria Add responder policy Rule Default-action Add responder policy undefAction","title":"Responder policy"},{"location":"crds/rewrite-responder/#audit-log","text":"The following table lists the CRD attributes provide to enable audit log within the rewrite or responder policies. Also, the table lists the corresponding Citrix ADC command and attributes. CRD attribute Citrix ADC command Citrix ADC attribute Logexpression Add audit message action stringBuilderExpr Loglevel Add audit message action Loglevel","title":"Audit log"},{"location":"crds/rewrite-responder/#dataset","text":"The following table lists the CRD attributes for dataset that you can use within the rewrite or responder policies. Also, the table lists the corresponding Citrix ADC command and attributes. CRD attribute Citrix ADC command Citrix ADC attribute Name Add policy dataset Name Type Add policy dataset Type Values Bind policy dataset Value","title":"Dataset"},{"location":"crds/rewrite-responder/#patset","text":"CRD attribute Citrix ADC command Citrix ADC attribute Name Add policy patset Name Values Bind policy patset string","title":"Patset"},{"location":"crds/rewrite-responder/#string-map","text":"CRD attribute Citrix ADC command Citrix ADC attribute Name Add policy stringmap Name Key Bind policy stringmap Key Value Bind policy stringmap Value","title":"String map"},{"location":"crds/rewrite-responder/#how-to-write-a-policy-configuration","text":"After you have deployed the CRD provided by Citrix in the Kubernetes cluster, you can define the policy configuration in a .yaml file. In the .yaml file, use rewritepolicy in the kind field and based on your requirement add any of the following individual sections in spec for policy configuration. rewrite-policy - To define rewrite policy configuration. responder-policy - To define responder policy configuration. logpackets - To enable audit logs. dataset - To use dataset for extensive policy configuration. patset - To use patset for extensive policy configuration. stringmaps - To use string maps for extensive policy configuration. In these sections, you need to use the CRD attributes provided for respective policy configuration (rewrite or responder) to define the policy. Also, in the spec you need to include a rewrite-policies section to specify the service or services to which the policy needs to be applied. For more information, see Sample policy configurations . After you deploy the .yaml file, the Citrix Ingress Controller (CIC) applies the policy configuration on the Ingress Citrix ADC device. Points to note: If the CRD is associated with a namespace then, by default, the policy is applied to the services associated with the namespace. For example, if you have the same service name associated with multiple namespaces, then the policy is applied to the service that belongs to the namespace associated with the CRD. If you have defined multiple policies in a single .yaml file then the first policy configuration defined in the file takes priority and the subsequent policy configurations is applied as per the sequence. If you have multiple policies defined in different files then the first policy configuration defined in the file that you deployed first takes priority. Consider a scenario wherein you want to define a policy in Citrix ADC to rewrite all the incoming URLs to new-url-for-the-application and send it to the microservices. Create a .yaml file called target-url-rewrite.yaml and use the appropriate CRD attributes to define the rewrite policy as follows: target-url-rewrite.yaml: apiVersion: citrix.com/v1 kind: rewritepolicy metadata: name: targeturlrewrite spec: rewrite-policies: - servicenames: - citrix-svc logpackets: logexpression: http.req.url loglevel: INFORMATIONAL rewrite-policy: operation: replace target: 'http.req.url' modify-expression: ' new-url-for-the-application ' comment: 'Target URL Rewrite - rewrite the url of the HTTP request' direction: REQUEST rewrite-criteria: 'http.req.is_valid' After you have defined the policy configuration, deploy the .yaml file using the following command: root@master:demo#kubectl create -f target-url-rewrite.yaml After you deploy the .yaml file, the Citrix Ingress Controller (CIC) applies the policy configuration on the Ingress Citrix ADC device. On the master node in the Kubernetes cluster, you can verify if the rewrite policy CRD is created on the CIC using the following command: root@master:~# kubectl logs citrixingresscontroller | grep -i 'SUCCESS\\|FAILURE\\|exception' You can view an entry in the logs as shown in the following image: Also, you can verify if the configuration is applied on the Citrix ADC, do the following: Log on to the Citrix ADC command-line. Use the following command to verify if the configuration is applied to the Citrix ADC: $ show run | grep `lb vserver` add lb vserver k8s-citrix.default.80.k8s-citrix-svc.default.http HTTP 0.0.0.0 0 -persistenceType NONE -lbMethod ROUNDROBIN -cltTimeout 180 -coment uid=67d18ffb-261e-11e9-aad9-8e30e16c8143.ver=1692325 bind lb vserver k8s-citrix.default.80.k8s-citrix-svc.default.http k8s-citrix.default.80.k8s-citrix-svc.default.http bind lb vserver k8s-citrix.default.80.k8s-citrix-svc.default.http k8s-citrix.default.80.k8s-citrix-svc.default.http -policyname k8s_rwpolicy_crd_targeturlrewrite_0_default_1719072 -priority 1006 -gotoPriorityExpression END -type REQUEST You can verify that the policy k8s_rwpolicy_crd_targeturlrewrite_0_default_1719072 is bound to the load balancing virtual server.","title":"How to write a policy configuration"},{"location":"crds/rewrite-responder/#sample-policy-configurations","text":"","title":"Sample policy configurations"},{"location":"crds/rewrite-responder/#responder-policy-configuration","text":"black-list-urls.yaml: apiVersion: citrix.com/v1 kind: rewritepolicy metadata: name: blacklisturls spec: responder-policies: - servicenames: - citrix-svc responder-policy: respondwith: http-payload-string: ' HTTP/1.1 401 Access denied ' respond-criteria: 'http.req.url.equals_any( blacklistUrls )' comment: 'Blacklist certain Urls' patset: - name: blacklistUrls values: - '/app1' - '/app2' - '/app3' In this example, if Citrix ADC receives any URL that matches the /app1 , /app2 , or /app3 strings defined in the patset , Citrix ADC blocks the URL.","title":"Responder policy configuration"},{"location":"crds/rewrite-responder/#policy-with-audit-log-enabled","text":"black-list-urls-audit-log.yaml: apiVersion: citrix.com/v1 kind: rewritepolicy metadata: name: blacklisturls spec: responder-policies: - servicenames: - citrix-svc logpackets: logexpression: http.req.url loglevel: INFORMATIONAL responder-policy: respondwith: http-payload-string: ' HTTP/1.1 401 Access denied ' respond-criteria: 'http.req.url.equals_any( blacklistUrls )' comment: 'Blacklist certain Urls' patset: - name: blacklistUrls values: - '/app1' - '/app2' - '/app3'","title":"Policy with audit log enabled"},{"location":"crds/rewrite-responder/#multiple-policy-configurations","text":"You can add multiple policy configurations in a single .yaml file and apply the policies to the Citrix ADC device. You need add separate sections for each policy configuration. multi-policy-config.yaml: apiVersion: citrix.com/v1 kind: rewritepolicy metadata: name: multipolicy spec: responder-policies: - servicenames: - citrix-svc responder-policy: redirect: url: ' www.citrix.com ' respond-criteria: 'client.ip.src.TYPECAST_text_t.equals_any( redirectIPs )' comment: 'Redirect IPs to citrix.com' - servicenames: - citrix-svc responder-policy: redirect: url: 'HTTP.REQ.HOSTNAME+http.req.url.MAP_STRING_DEFAULT_TO_KEY( modifyurls )' respond-criteria: 'http.req.is_valid' comment: 'modify specific URLs' rewrite-policies: - servicenames: - citrix-svc rewrite-policy: operation: insert_http_header target: 'sessionID' modify-expression: ' 48592th42gl24456284536tgt2 ' comment: 'insert SessionID in header' direction: RESPONSE rewrite-criteria: 'http.res.is_valid' dataset: - name: redirectIPs type: ipv4 values: - 1.1.1.1 - 2.2.2.2 stringmap: - name: modifyurls comment: Urls to be modified string values: - key: ' /app1/ ' value: ' /internal-app1/ ' - key: ' /app2/ ' value: ' /internal-app2/ ' The example contains two responder policies and a rewrite policy, based on these policies the Citrix ADC device performs the following: Any client request to IP addresses defined in the redirectIP dataset, that is, 1.1.1.1 or 2.2.2.2 respectively, the request is redirected to www.citrix.com . Any incoming URL with strings provided in the modifyurls stringmap is modified to the value provided in the stringmap. For example, if the incoming URL has the string /app1/ is modified to /internal-app1/ Adds a session ID as a new header in the response to the client.","title":"Multiple policy configurations"},{"location":"crds/rewrite-responder/#related-articles","text":"Feature Documentation Citrix ADC Rewrite Feature Documentation Citrix ADC Responder Feature Documentation Developer Documentation Citrix ADC Rewrite Policy Citrix ADC Rewrite Action Citrix ADC Responder Policy Citrix ADC Responder Action Citrix ADC Audit Message Action Citrix ADC Policy Dataset","title":"Related articles"},{"location":"deploy/deploy-azure/","text":"Deploying Citrix ADC CPX as an Ingress Device in a Azure Kubernetes Service Cluster This section explains how to deploy Citrix ADC CPX as an ingress device in a Azure Kubernetes Service (AKS) cluster with basic networking mode (kubenet). You can also configure the Kubernetes cluster on Azure VM and then deploy Citrix ADC CPX as the ingress device. The procedure to deploy Citrix ADC CPX for both AKS and Azure VM is the same. However, if you are configuring Kubernetes on Azure VM you need to deploy the CNI plug-in for the Kubernetes cluster. Prerequisites You should complete the following tasks before performing the steps in the procedure. Ensure that you have a Kubernetes cluster up and running. Ensure that the AKS is configured in the basic networking mode (kubenet) and not in the advanced networking mode (Azure CNI). Note For more information on creating a Kubernetes cluster in AKS, see Guide to create an AKS cluster . Deploying Citrix CPX as an Ingress Device in a AKS Cluster Perform the following steps to deploy Citrix CPX as an ingress device in a AKS cluster. Note In this procedure, Apache web server is used as the sample application. Deploy the required application in your Kubernetes cluster and expose it as a service in your cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/apache.yaml Note In this example, apache.yaml is used. You should use the specific YAML file for your application. Deploy Citrix CPX as an ingress device in the cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/standalone_cpx.yaml Create the ingress resource using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/cpx_ingress.yaml Create a service of type LoadBalancer for accessing the Citrix CPX by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/cpx_service.yaml Note This command creates a load balancer with an external IP for receiving traffic. This is supported in kubernetes since v1.10.0. Verify the service and check whether the load balancer has created an external IP. Wait for some time if the external IP is not created. kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE apache ClusterIP 10.0.103.3 none 80/TCP 2m cpx-ingress LoadBalancer 10.0.37.255 pending 80:32258/TCP,443:32084/TCP 2m kubernetes ClusterIP 10.0.0.1 none 443/TCP 22h Once the external IP for the load-balancer is available as follows, you can access your resources using the external IP for the load balancer. kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE apache ClusterIP 10.0.103.3 none 80/TCP 3m cpx-ingress LoadBalancer 10.0.37.255 EXTERNAL-IP CREATED 80:32258/TCP,443:32084/TCP 3m kubernetes ClusterIP 10.0.0.1 none 443/TCP 22h Note The health check for the cloud load-balancer is obtained from the readinessProbe configured in the Citrix CPX deployment yaml file. If the health check fails, you should check the readinessProbe configured for Citrix CPX. For more information, see readinessProbe and external Load balancer . Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com Deployment Models You can use the following deployment solutions for deploying CPX as an ingress device in a AKS cluster. Standalone Citrix CPX deployment High availability Citrix CPX deployment Citrix CPX per node deployment Note For the ease of deployment, the deployment models in this section are explained with an all-in-one manifest file that combines the steps explained in the previous section. You can modify the manifest file to suit your application and configuration. Deploying a Standalone Citrix CPX as the Ingress Device To deploy Citrix CPX as an Ingress device in a standalone deployment model in AKS, you should use the service type as LoadBalancer which would create a load balancer in the Azure cloud. This is supported in Kubernetes since v1.10.0. Perform the following steps to deploy a stand alone Citrix CPX as the ingress device. Deploy a Citrix CPX ingress with inbuilt Citrix Ingress Controller in your Kubernetes cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one.yaml Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Note To delete the deployment, use the following command. kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one.yaml Deploying Citrix CPX for High Availability In the standalone deployment of Citrix CPX as the ingress, if the ingress device fails for some reason there would be a traffic outage for a few seconds. To avoid this traffic disruption, you can deploy two Citrix CPX ingress devices instead of deploying a single Citrix CPX ingress device. In such deployments, even if one Citrix CPX fails the other Citrix CPX is available to handle the traffic till the failed Citrix CPX comes up. Perform the following steps to deploy two Citrix CPX devices for high availability. Deploy Citrix CPX ingress devices for high availability in your Kubernetes cluster by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one-ha.yaml Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Note To delete the deployment, use the following command. kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one-ha.yaml Deploying Citrix CPX per Node In some cases where cluster nodes are added and removed from the cluster, Citrix CPX can also be deployed as daemonsets so that every node will have a Citirx CPX ingress in them. This is a much more reliable solution than deploying two Citrix CPX devices as ingress devices when the traffic is high. Perform the followings steps to deploy Citrix CPX as a ingress device on each node in the cluster. Deploy Citrix CPX ingress device in each node of your Kubernetes cluster by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one-reliable.yaml Access the application by using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com Note To delete the deployment, use the following command: kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one-reliable.yaml","title":"Deploy Citrix ADC CPX as an Ingress in Azure Kubernetes Engine"},{"location":"deploy/deploy-azure/#deploying-citrix-adc-cpx-as-an-ingress-device-in-a-azure-kubernetes-service-cluster","text":"This section explains how to deploy Citrix ADC CPX as an ingress device in a Azure Kubernetes Service (AKS) cluster with basic networking mode (kubenet). You can also configure the Kubernetes cluster on Azure VM and then deploy Citrix ADC CPX as the ingress device. The procedure to deploy Citrix ADC CPX for both AKS and Azure VM is the same. However, if you are configuring Kubernetes on Azure VM you need to deploy the CNI plug-in for the Kubernetes cluster.","title":"Deploying Citrix ADC CPX as an Ingress Device in a Azure Kubernetes Service Cluster"},{"location":"deploy/deploy-azure/#prerequisites","text":"You should complete the following tasks before performing the steps in the procedure. Ensure that you have a Kubernetes cluster up and running. Ensure that the AKS is configured in the basic networking mode (kubenet) and not in the advanced networking mode (Azure CNI). Note For more information on creating a Kubernetes cluster in AKS, see Guide to create an AKS cluster .","title":"Prerequisites"},{"location":"deploy/deploy-azure/#deploying-citrix-cpx-as-an-ingress-device-in-a-aks-cluster","text":"Perform the following steps to deploy Citrix CPX as an ingress device in a AKS cluster. Note In this procedure, Apache web server is used as the sample application. Deploy the required application in your Kubernetes cluster and expose it as a service in your cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/apache.yaml Note In this example, apache.yaml is used. You should use the specific YAML file for your application. Deploy Citrix CPX as an ingress device in the cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/standalone_cpx.yaml Create the ingress resource using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/cpx_ingress.yaml Create a service of type LoadBalancer for accessing the Citrix CPX by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/cpx_service.yaml Note This command creates a load balancer with an external IP for receiving traffic. This is supported in kubernetes since v1.10.0. Verify the service and check whether the load balancer has created an external IP. Wait for some time if the external IP is not created. kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE apache ClusterIP 10.0.103.3 none 80/TCP 2m cpx-ingress LoadBalancer 10.0.37.255 pending 80:32258/TCP,443:32084/TCP 2m kubernetes ClusterIP 10.0.0.1 none 443/TCP 22h Once the external IP for the load-balancer is available as follows, you can access your resources using the external IP for the load balancer. kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE apache ClusterIP 10.0.103.3 none 80/TCP 3m cpx-ingress LoadBalancer 10.0.37.255 EXTERNAL-IP CREATED 80:32258/TCP,443:32084/TCP 3m kubernetes ClusterIP 10.0.0.1 none 443/TCP 22h Note The health check for the cloud load-balancer is obtained from the readinessProbe configured in the Citrix CPX deployment yaml file. If the health check fails, you should check the readinessProbe configured for Citrix CPX. For more information, see readinessProbe and external Load balancer . Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com","title":"Deploying Citrix CPX as an Ingress Device in a AKS Cluster"},{"location":"deploy/deploy-azure/#deployment-models","text":"You can use the following deployment solutions for deploying CPX as an ingress device in a AKS cluster. Standalone Citrix CPX deployment High availability Citrix CPX deployment Citrix CPX per node deployment Note For the ease of deployment, the deployment models in this section are explained with an all-in-one manifest file that combines the steps explained in the previous section. You can modify the manifest file to suit your application and configuration.","title":"Deployment Models"},{"location":"deploy/deploy-azure/#deploying-a-standalone-citrix-cpx-as-the-ingress-device","text":"To deploy Citrix CPX as an Ingress device in a standalone deployment model in AKS, you should use the service type as LoadBalancer which would create a load balancer in the Azure cloud. This is supported in Kubernetes since v1.10.0. Perform the following steps to deploy a stand alone Citrix CPX as the ingress device. Deploy a Citrix CPX ingress with inbuilt Citrix Ingress Controller in your Kubernetes cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one.yaml Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Note To delete the deployment, use the following command. kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one.yaml","title":"Deploying a Standalone Citrix CPX as the Ingress Device"},{"location":"deploy/deploy-azure/#deploying-citrix-cpx-for-high-availability","text":"In the standalone deployment of Citrix CPX as the ingress, if the ingress device fails for some reason there would be a traffic outage for a few seconds. To avoid this traffic disruption, you can deploy two Citrix CPX ingress devices instead of deploying a single Citrix CPX ingress device. In such deployments, even if one Citrix CPX fails the other Citrix CPX is available to handle the traffic till the failed Citrix CPX comes up. Perform the following steps to deploy two Citrix CPX devices for high availability. Deploy Citrix CPX ingress devices for high availability in your Kubernetes cluster by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one-ha.yaml Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Note To delete the deployment, use the following command. kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one-ha.yaml","title":"Deploying Citrix CPX for High Availability"},{"location":"deploy/deploy-azure/#deploying-citrix-cpx-per-node","text":"In some cases where cluster nodes are added and removed from the cluster, Citrix CPX can also be deployed as daemonsets so that every node will have a Citirx CPX ingress in them. This is a much more reliable solution than deploying two Citrix CPX devices as ingress devices when the traffic is high. Perform the followings steps to deploy Citrix CPX as a ingress device on each node in the cluster. Deploy Citrix CPX ingress device in each node of your Kubernetes cluster by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one-reliable.yaml Access the application by using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com Note To delete the deployment, use the following command: kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/azure/manifest/all-in-one-reliable.yaml","title":"Deploying Citrix CPX per Node"},{"location":"deploy/deploy-cic-helm/","text":"Deploying Citrix Ingress Controller using Helm charts You can deploy Citrix Ingress Controller (CIC) in the following modes: As a standalone pod in the Kubernetes cluster. Use this mode if you are controlling Citrix ADCs (Citrix ADC MPX or Citrix ADC VPX) outside the cluster. For example, with dual-tier topologies, or single-tier topology where the single tier is a Citrix ADC MPX or VPX. As a sidecar (in the same pod) with Citrix ADC CPX in the Kubernetes cluster. The sidecar controller is only responsible for the associated Citrix ADC CPX within the same pod. This mode is used in dual-tier or cloud ) topologies. The CIC repo contains the Helm charts of CIC that you can use to configure Citrix ADCs such as, VPX, MPX, or CPX in Kubernetes environment. The repo contains a directory called stable that includes stable version of the charts that are created and tested by Citrix. Deploying CIC as a pod in the Kubernetes cluster Use the citrix-k8s-ingress-controller chart to run Citrix Ingress Controller (CIC) as a pod in your Kubernetes cluster. The chart deploys CIC as a pod in your Kubernetes cluster and configures the Citrix ADC VPX or MPX ingress device. Prerequisites Determine the NS_IP IP address needed by the controller to communicate with the appliance. The IP address might be anyone of the following depending on the type of Citrix ADC deployment: (Standalone appliances) NSIP - The management IP address of a standalone Citrix ADC appliance. For more information, see IP Addressing in Citrix ADC . (Appliances in High Availability mode) SNIP - The subnet IP address. For more information, see IP Addressing in Citrix ADC . (Appliances in Clustered mode) CLIP - The cluster management IP (CLIP) address for a clustered Citrix ADC deployment. For more information, see IP addressing for a cluster . The username and password of the Citrix ADC VPX or MPX appliance used as the Ingress device. The Citrix ADC appliance needs to have system user account (non-default) with certain privileges so that CIC can configure the Citrix ADC VPX or MPX appliance. For instructions to create the system user account on Citrix ADC, see Create System User Account for CIC in Citrix ADC . You can directly pass the username and password or use Kubernetes secrets. If you want to use Kubernetes secrets, create a secrete for the username and password using the following command: kubectl create secret generic nslogin --from-literal=username='cic' --from-literal=password='mypassword' Create System User Account for CIC in Citrix ADC Citrix Ingress Controller (CIC) configures the Citrix ADC using a system user account of the Citrix ADC. The system user account should have certain privileges so that the CIC has permission to configure the following on the Citrix ADC: Add, Delete, or View Content Switching (CS) virtual server Configure CS policies and actions Configure Load Balancing (LB) virtual server Configure Service groups Cofigure SSl certkeys Configure routes Configure user monitors Add system file (for uploading SSL certkeys from Kubernetes) Configure Virtual IP address (VIP) Check the status of the Citrix ADC appliance To create the system user account, do the following: Log on to the Citrix ADC appliance. Perform the following: Use an SSH client, such as PuTTy, to open an SSH connection to the Citrix ADC appliance. Log on to the appliance by using the administrator credentials. Create the system user account using the following command: add system user username password For example: add system user cic mypassword Create a policy to provide required permissions to the system user account. Use the following command: add cmdpolicy cic-policy ALLOW (^\\S+\\s+cs\\s+\\S+)|(^\\S+\\s+lb\\s+\\S+)|(^\\S+\\s+service\\s+\\S+)|(^\\S+\\s+servicegroup\\s+\\S+)|(^stat\\s+system)|(^show\\s+ha)|(^\\S+\\s+ssl\\s+certKey)|(^\\S+\\s+ssl)|(^\\S+\\s+route)|(^\\S+\\s+monitor)|(^show\\s+ns\\s+ip)|(^\\S+\\s+system\\s+file) Note The system user account would have privileges based on the command policy that you define. Bind the policy to the system user account using the following command: bind system user cic cic-policy 0 To deploy the citrix-k8s-ingress-controller chart, perform the following: Use the helm install command to install the citrix-k8s-ingress-controller chart. For example: helm install citrix-k8s-ingress-controller -set nsIP= NSIP ,license.accept=yes,ingressClass= ingressClassName Note By default the chart installs the recommended RBAC roles and role bindings. To configure the CIC when installing the chart, you need to pass CIC specific parameters in helm install . The following table lists the mandatory and optional parameters that you can use: Note You can also use the values.yaml to pass the parameters. Parameters Mandatory or Optional Default value Description nsIP Mandatory N/A The IP address of the Citrix ADC device. For details, see Prerequisites . license.accept Mandatory no Set yes to accept the CIC end user license agreement. image.repository Mandatory quay.io/citrix/citrix-k8s-ingress-controller The repository of the CIC image image.tag Mandatory 1.1.1 The CIC image tag. image.pullPolicy Mandatory Always The CIC image pull policy. nsPort Optional 443 The port used by CIC to communicate with Citrix ADC. You can port 80 for HTTP. nsProtocol Optional HTTPS The protocol used by CIC to communicate with Citrix ADC. You can also use HTTP on port 80. logLevel Optional DEPUG The loglevel to control the logs generated by CIC. The supported loglevels are: CRITICAL, ERROR, WARNING, INFO, and DEBUG. For more information, see Logging . kubernetesURL Optional N/A The kube-apiserver url that CIC uses to register the events. If the value is not specified, CIC uses the internal kube-apiserver IP address . ingressClass Optional N/A If multiple ingress load balancers are used to load balance different ingress resources. You can use this parameter to specify CIC to configure Citrix ADC associated with specific ingress class. name Optional N/A Use the argument to specify the release name using which you want to install the chart. For example: helm install citrix-k8s-ingress-controller --name my-release --set nsIP= NSIP ,license.accept=yes,ingressClass= ingressClassName exporter.require=1.0 Optional N/A Use the argument If you want to run the Exporter for Citrix ADC Stats along with CIC to pull metrics for the Citrix ADC VPX or MPX For example: helm install citrix-k8s-ingress-controller --name my-release --set license.accept=yes,ingressClass= ingressClassName ,exporter.require=1.0 Deploying CIC as a sidecar with Citrix ADC CPX Use the citrix-k8s-cpx-ingress-controller chart to deploy a Citrix ADC CPX with CIC as a sidecar. The chart deploys a Citrix ADC CPX instance that is used for load balancing the North-South traffic to the microservices in your Kubernetes cluster and the sidecar CIC configures the Citrix ADC CPX. Use the helm install command to install the citrix-k8s-cpx-ingress-controller chart. Note By default the chart installs the recommended RBAC roles and role bindings. To configure the CIC when installing the chart, you need to pass CIC specific parameters in helm install . The following table lists the mandatory and optional parameters that you can use: Tip You can also use the values.yaml . Parameters Mandatory or Optional Default value Description license.accept Mandatory no Set yes to accept the CIC end user license agreement. cpximage.repository Mandatory quay.io/citrix/citrix-k8s-cpx-ingress The Citrix ADC CPX image repository. cpximage.tag Mandatory 12.1-51.16 The Citrix ADC CPX image tag. cpximage.pullPolicy Mandatory Always The Citrix ADC CPX image pull policy. cicimage.repository Mandatory quay.io/citrix/citrix-k8s-ingress-controller The CIC image repository. cicimage.tag Mandatory 1.1.1 The CIC image tag. cicimage.pullPolicy Mandatory Always The CIC image pull policy. exporter.require=1.0 Optional N/A Use the argument if you want to run the Exporter for Citrix ADC Stats along with CIC to pull metrics for the Citrix ADC VPX or MPX exporter.image.repository Optional quay.io/citrix/netscaler-metrics-exporter The Exporter for Citrix ADC Stats image repository. exporter.image.tag Optional v1.0.0 The Exporter for Citrix ADC Stats image tag. exporter.image.pullPolicy Optional Always The Exporter for Citrix ADC Stats image pull policy. exporter.ports.containerPort Optional 8888 The Exporter for Citrix ADC Stats container port. ingressClass Optional N/A If multiple ingress load balancers are used to load balance different ingress resources. You can use this parameter to specify CIC to configure Citrix ADC associated with specific ingress class.","title":"Using Helm charts"},{"location":"deploy/deploy-cic-helm/#deploying-citrix-ingress-controller-using-helm-charts","text":"You can deploy Citrix Ingress Controller (CIC) in the following modes: As a standalone pod in the Kubernetes cluster. Use this mode if you are controlling Citrix ADCs (Citrix ADC MPX or Citrix ADC VPX) outside the cluster. For example, with dual-tier topologies, or single-tier topology where the single tier is a Citrix ADC MPX or VPX. As a sidecar (in the same pod) with Citrix ADC CPX in the Kubernetes cluster. The sidecar controller is only responsible for the associated Citrix ADC CPX within the same pod. This mode is used in dual-tier or cloud ) topologies. The CIC repo contains the Helm charts of CIC that you can use to configure Citrix ADCs such as, VPX, MPX, or CPX in Kubernetes environment. The repo contains a directory called stable that includes stable version of the charts that are created and tested by Citrix.","title":"Deploying Citrix Ingress Controller using Helm charts"},{"location":"deploy/deploy-cic-helm/#deploying-cic-as-a-pod-in-the-kubernetes-cluster","text":"Use the citrix-k8s-ingress-controller chart to run Citrix Ingress Controller (CIC) as a pod in your Kubernetes cluster. The chart deploys CIC as a pod in your Kubernetes cluster and configures the Citrix ADC VPX or MPX ingress device.","title":"Deploying CIC as a pod in the Kubernetes cluster"},{"location":"deploy/deploy-cic-helm/#prerequisites","text":"Determine the NS_IP IP address needed by the controller to communicate with the appliance. The IP address might be anyone of the following depending on the type of Citrix ADC deployment: (Standalone appliances) NSIP - The management IP address of a standalone Citrix ADC appliance. For more information, see IP Addressing in Citrix ADC . (Appliances in High Availability mode) SNIP - The subnet IP address. For more information, see IP Addressing in Citrix ADC . (Appliances in Clustered mode) CLIP - The cluster management IP (CLIP) address for a clustered Citrix ADC deployment. For more information, see IP addressing for a cluster . The username and password of the Citrix ADC VPX or MPX appliance used as the Ingress device. The Citrix ADC appliance needs to have system user account (non-default) with certain privileges so that CIC can configure the Citrix ADC VPX or MPX appliance. For instructions to create the system user account on Citrix ADC, see Create System User Account for CIC in Citrix ADC . You can directly pass the username and password or use Kubernetes secrets. If you want to use Kubernetes secrets, create a secrete for the username and password using the following command: kubectl create secret generic nslogin --from-literal=username='cic' --from-literal=password='mypassword'","title":"Prerequisites"},{"location":"deploy/deploy-cic-helm/#create-system-user-account-for-cic-in-citrix-adc","text":"Citrix Ingress Controller (CIC) configures the Citrix ADC using a system user account of the Citrix ADC. The system user account should have certain privileges so that the CIC has permission to configure the following on the Citrix ADC: Add, Delete, or View Content Switching (CS) virtual server Configure CS policies and actions Configure Load Balancing (LB) virtual server Configure Service groups Cofigure SSl certkeys Configure routes Configure user monitors Add system file (for uploading SSL certkeys from Kubernetes) Configure Virtual IP address (VIP) Check the status of the Citrix ADC appliance To create the system user account, do the following: Log on to the Citrix ADC appliance. Perform the following: Use an SSH client, such as PuTTy, to open an SSH connection to the Citrix ADC appliance. Log on to the appliance by using the administrator credentials. Create the system user account using the following command: add system user username password For example: add system user cic mypassword Create a policy to provide required permissions to the system user account. Use the following command: add cmdpolicy cic-policy ALLOW (^\\S+\\s+cs\\s+\\S+)|(^\\S+\\s+lb\\s+\\S+)|(^\\S+\\s+service\\s+\\S+)|(^\\S+\\s+servicegroup\\s+\\S+)|(^stat\\s+system)|(^show\\s+ha)|(^\\S+\\s+ssl\\s+certKey)|(^\\S+\\s+ssl)|(^\\S+\\s+route)|(^\\S+\\s+monitor)|(^show\\s+ns\\s+ip)|(^\\S+\\s+system\\s+file) Note The system user account would have privileges based on the command policy that you define. Bind the policy to the system user account using the following command: bind system user cic cic-policy 0 To deploy the citrix-k8s-ingress-controller chart, perform the following: Use the helm install command to install the citrix-k8s-ingress-controller chart. For example: helm install citrix-k8s-ingress-controller -set nsIP= NSIP ,license.accept=yes,ingressClass= ingressClassName Note By default the chart installs the recommended RBAC roles and role bindings. To configure the CIC when installing the chart, you need to pass CIC specific parameters in helm install . The following table lists the mandatory and optional parameters that you can use: Note You can also use the values.yaml to pass the parameters. Parameters Mandatory or Optional Default value Description nsIP Mandatory N/A The IP address of the Citrix ADC device. For details, see Prerequisites . license.accept Mandatory no Set yes to accept the CIC end user license agreement. image.repository Mandatory quay.io/citrix/citrix-k8s-ingress-controller The repository of the CIC image image.tag Mandatory 1.1.1 The CIC image tag. image.pullPolicy Mandatory Always The CIC image pull policy. nsPort Optional 443 The port used by CIC to communicate with Citrix ADC. You can port 80 for HTTP. nsProtocol Optional HTTPS The protocol used by CIC to communicate with Citrix ADC. You can also use HTTP on port 80. logLevel Optional DEPUG The loglevel to control the logs generated by CIC. The supported loglevels are: CRITICAL, ERROR, WARNING, INFO, and DEBUG. For more information, see Logging . kubernetesURL Optional N/A The kube-apiserver url that CIC uses to register the events. If the value is not specified, CIC uses the internal kube-apiserver IP address . ingressClass Optional N/A If multiple ingress load balancers are used to load balance different ingress resources. You can use this parameter to specify CIC to configure Citrix ADC associated with specific ingress class. name Optional N/A Use the argument to specify the release name using which you want to install the chart. For example: helm install citrix-k8s-ingress-controller --name my-release --set nsIP= NSIP ,license.accept=yes,ingressClass= ingressClassName exporter.require=1.0 Optional N/A Use the argument If you want to run the Exporter for Citrix ADC Stats along with CIC to pull metrics for the Citrix ADC VPX or MPX For example: helm install citrix-k8s-ingress-controller --name my-release --set license.accept=yes,ingressClass= ingressClassName ,exporter.require=1.0","title":"Create System User Account for CIC in Citrix ADC"},{"location":"deploy/deploy-cic-helm/#deploying-cic-as-a-sidecar-with-citrix-adc-cpx","text":"Use the citrix-k8s-cpx-ingress-controller chart to deploy a Citrix ADC CPX with CIC as a sidecar. The chart deploys a Citrix ADC CPX instance that is used for load balancing the North-South traffic to the microservices in your Kubernetes cluster and the sidecar CIC configures the Citrix ADC CPX. Use the helm install command to install the citrix-k8s-cpx-ingress-controller chart. Note By default the chart installs the recommended RBAC roles and role bindings. To configure the CIC when installing the chart, you need to pass CIC specific parameters in helm install . The following table lists the mandatory and optional parameters that you can use: Tip You can also use the values.yaml . Parameters Mandatory or Optional Default value Description license.accept Mandatory no Set yes to accept the CIC end user license agreement. cpximage.repository Mandatory quay.io/citrix/citrix-k8s-cpx-ingress The Citrix ADC CPX image repository. cpximage.tag Mandatory 12.1-51.16 The Citrix ADC CPX image tag. cpximage.pullPolicy Mandatory Always The Citrix ADC CPX image pull policy. cicimage.repository Mandatory quay.io/citrix/citrix-k8s-ingress-controller The CIC image repository. cicimage.tag Mandatory 1.1.1 The CIC image tag. cicimage.pullPolicy Mandatory Always The CIC image pull policy. exporter.require=1.0 Optional N/A Use the argument if you want to run the Exporter for Citrix ADC Stats along with CIC to pull metrics for the Citrix ADC VPX or MPX exporter.image.repository Optional quay.io/citrix/netscaler-metrics-exporter The Exporter for Citrix ADC Stats image repository. exporter.image.tag Optional v1.0.0 The Exporter for Citrix ADC Stats image tag. exporter.image.pullPolicy Optional Always The Exporter for Citrix ADC Stats image pull policy. exporter.ports.containerPort Optional 8888 The Exporter for Citrix ADC Stats container port. ingressClass Optional N/A If multiple ingress load balancers are used to load balance different ingress resources. You can use this parameter to specify CIC to configure Citrix ADC associated with specific ingress class.","title":"Deploying CIC as a sidecar with Citrix ADC CPX"},{"location":"deploy/deploy-cic-yaml/","text":"Deploying Citrix Ingress Controller using YAML You can deploy Citrix Ingress Controller (CIC) in the following modes: As a standalone pod in the Kubernetes cluster. Use this mode if you are controlling Citrix ADCs (Citrix ADC MPX or Citrix ADC VPX) outside the cluster. For example, with dual-tier topologies, or single-tier topology where the single tier is a Citrix ADC MPX or VPX. As a sidecar (in the same pod) with Citrix ADC CPX in the Kubernetes cluster. The sidecar controller is only responsible for the associated Citrix ADC CPX within the same pod. This mode is used in dual-tier or cloud topologies. Deploying CIC as a standalone pod in the Kubernetes cluster for Citrix ADC MPX or VPX appliances Use the citrix-k8s-ingress-controller.yaml file to run Citrix Ingress Controller (CIC) as a standalone pod in your Kubernetes cluster. Note The Citrix ADC MPX or VPX can be deployed in standalone , high-availability , or clustered modes. Prerequisites Determine the NS_IP IP address needed by the controller to communicate with the appliance. The IP address might be anyone of the following depending on the type of Citrix ADC deployment: (Standalone appliances) NSIP - The management IP address of a standalone Citrix ADC appliance. For more information, see IP Addressing in Citrix ADC (Appliances in High Availability mode) SNIP - The subnet IP address. For more information, see IP Addressing in Citrix ADC (Appliances in Clustered mode) CLIP - The cluster management IP (CLIP) address for a clustered Citrix ADC deployment. For more information, see IP addressing for a cluster The username and password of the Citrix ADC VPX or MPX appliance used as the Ingress device. The Citrix ADC appliance must have a system user account (non-default) with certain privileges so that CIC can configure the Citrix ADC VPX or MPX appliance. For instructions to create the system user account on Citrix ADC, see Create System User Account for CIC in Citrix ADC You can directly pass the username and password as environment variables to the controller, or use Kubernetes secrets (recommended). If you want to use Kubernetes secrets, create a secret for the username and password using the following command: kubectl create secret generic nslogin --from-literal=username='cic' --from-literal=password='mypassword' Create System User Account for CIC in Citrix ADC Citrix Ingress Controller (CIC) configures the Citrix ADC appliance (MPX or VPX) using a system user account of the Citrix ADC. The system user account should have certain privileges so that the CIC has permission to configure the following on the Citrix ADC: Add, Delete, or View Content Switching (CS) virtual server Configure CS policies and actions Configure Load Balancing (LB) virtual server Configure Service groups Cofigure SSl certkeys Configure routes Configure user monitors Add system file (for uploading SSL certkeys from Kubernetes) Configure Virtual IP address (VIP) Check the status of the Citrix ADC appliance To create the system user account, perform the following: Log on to the Citrix ADC appliance. Perform the following: Use an SSH client, such as PuTTy, to open an SSH connection to the Citrix ADC appliance. Log on to the appliance by using the administrator credentials. Create the system user account using the following command: add system user username password For example: add system user cic mypassword Create a policy to provide required permissions to the system user account. Use the following command: add cmdpolicy cic-policy ALLOW (^\\S+\\s+cs\\s+\\S+)|(^\\S+\\s+lb\\s+\\S+)|(^\\S+\\s+service\\s+\\S+)|(^\\S+\\s+servicegroup\\s+\\S+)|(^stat\\s+system)|(^show\\s+ha)|(^\\S+\\s+ssl\\s+certKey)|(^\\S+\\s+ssl)|(^\\S+\\s+route)|(^\\S+\\s+monitor)|(^show\\s+ns\\s+ip)|(^\\S+\\s+system\\s+file) Note The system user account would have the privileges based on the command policy that you define. Bind the policy to the system user account using the following command: bind system user cic cic-policy 0 Deploy CIC as a pod Perform the following: Download the citrix-k8s-ingress-controller.yaml using the following command: wget https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/baremetal/citrix-k8s-ingress-controller.yaml Edit the citrix-k8s-ingress-controller.yaml file and enter the values for the following environmental variables: Environment Variable Mandatory or Optional Description NS_IP Mandatory The IP address of the Citrix ADC appliance. For more details, see Prerequisites . NS_USER and NS_PASSWORD Mandatory The username and password of the Citrix ADC VPX or MPX appliance used as the Ingress device. For more details, see Prerequisites . EULA Mandatory The End User License Agreement. Specify the value as Yes . Kubernetes_url Optional The kube-apiserver url that CIC uses to register the events. If the value is not specified, CIC uses the internal kube-apiserver IP address . LOGLEVEL Optional The log levels to control the logs generated by CIC. By default, the value is set to DEBUG. The supported values are: CRITICAL, ERROR, WARNING, INFO, and DEBUG. For more information, see Logging NS_PROTOCOL and NS_PORT Optional Defines the protocol and port that must be used by CIC to communicate with Citrix ADC. By default, CIC uses HTTPS on port 443. You can also use HTTP on port 80. ingress-classes Optional If multiple ingress load balancers are used to load balance different ingress resources. You can use this environment variable to specify CIC to configure Citrix ADC associated with specific ingress class. For information on Ingress classes, see Ingress class support NS_IP Optional CIC uses the IP address provided in this environment variable to configure a virtual IP address to the Citrix ADC that receives Ingress traffic. Note: NS_IP takes precedence over the frontend-ip annotation. Once you update the environment variables, save the YAML file and deploy it using the following command: kubectl create -f citrix-k8s-ingress-controller.yaml Verify if CIC is deployed successfully using the following command: kubectl get pods --all-namespaces Deploying CIC as a sidecar with Citrix ADC CPX Use the citrix-k8s-cpx-ingress.yaml file to deploy a Citrix ADC CPX with CIC as a sidecar. The YAML file deploys a Citrix ADC CPX instance that is used for load balancing the North-South traffic to the microservices in your Kubernetes cluster. Perform the following: Download the citrix-k8s-cpx-ingress.yaml using the following command: wget https://github.com/citrix/citrix-k8s-ingress-controller/blob/master/deployment/baremetal/citrix-k8s-cpx-ingress.yml Edit the citrix-k8s-cpx-ingress.yaml file and enter the values for the following environmental variables: Environment Variable Mandatory or Optional Description NS_IP Mandatory The IP address of the Citrix ADC appliance. For more details, see Prerequisites . NS_USER and NS_PASSWORD Mandatory The username and password of the Citrix ADC VPX or MPX appliance used as the Ingress device. For more details, see Prerequisites . EULA Mandatory The End User License Agreement. Specify the value as Yes . NS_PROTOCOL and NS_PORT Optional Defines the protocol and port that must be used by CIC to communicate with Citrix ADC. By default, CIC uses HTTPS on port 443. You can also use HTTP on port 80. Once you update the environment variables, save the YAML file and deploy it using the following command: kubectl create -f citrix-k8s-cpx-ingress.yaml Verify if CIC is deployed successfully using the following command: kubectl get pods --all-namespaces","title":"Using YAML"},{"location":"deploy/deploy-cic-yaml/#deploying-citrix-ingress-controller-using-yaml","text":"You can deploy Citrix Ingress Controller (CIC) in the following modes: As a standalone pod in the Kubernetes cluster. Use this mode if you are controlling Citrix ADCs (Citrix ADC MPX or Citrix ADC VPX) outside the cluster. For example, with dual-tier topologies, or single-tier topology where the single tier is a Citrix ADC MPX or VPX. As a sidecar (in the same pod) with Citrix ADC CPX in the Kubernetes cluster. The sidecar controller is only responsible for the associated Citrix ADC CPX within the same pod. This mode is used in dual-tier or cloud topologies.","title":"Deploying Citrix Ingress Controller using YAML"},{"location":"deploy/deploy-cic-yaml/#deploying-cic-as-a-standalone-pod-in-the-kubernetes-cluster-for-citrix-adc-mpx-or-vpx-appliances","text":"Use the citrix-k8s-ingress-controller.yaml file to run Citrix Ingress Controller (CIC) as a standalone pod in your Kubernetes cluster. Note The Citrix ADC MPX or VPX can be deployed in standalone , high-availability , or clustered modes.","title":"Deploying CIC as a standalone pod in the Kubernetes cluster for Citrix ADC MPX or VPX appliances"},{"location":"deploy/deploy-cic-yaml/#prerequisites","text":"Determine the NS_IP IP address needed by the controller to communicate with the appliance. The IP address might be anyone of the following depending on the type of Citrix ADC deployment: (Standalone appliances) NSIP - The management IP address of a standalone Citrix ADC appliance. For more information, see IP Addressing in Citrix ADC (Appliances in High Availability mode) SNIP - The subnet IP address. For more information, see IP Addressing in Citrix ADC (Appliances in Clustered mode) CLIP - The cluster management IP (CLIP) address for a clustered Citrix ADC deployment. For more information, see IP addressing for a cluster The username and password of the Citrix ADC VPX or MPX appliance used as the Ingress device. The Citrix ADC appliance must have a system user account (non-default) with certain privileges so that CIC can configure the Citrix ADC VPX or MPX appliance. For instructions to create the system user account on Citrix ADC, see Create System User Account for CIC in Citrix ADC You can directly pass the username and password as environment variables to the controller, or use Kubernetes secrets (recommended). If you want to use Kubernetes secrets, create a secret for the username and password using the following command: kubectl create secret generic nslogin --from-literal=username='cic' --from-literal=password='mypassword'","title":"Prerequisites"},{"location":"deploy/deploy-cic-yaml/#create-system-user-account-for-cic-in-citrix-adc","text":"Citrix Ingress Controller (CIC) configures the Citrix ADC appliance (MPX or VPX) using a system user account of the Citrix ADC. The system user account should have certain privileges so that the CIC has permission to configure the following on the Citrix ADC: Add, Delete, or View Content Switching (CS) virtual server Configure CS policies and actions Configure Load Balancing (LB) virtual server Configure Service groups Cofigure SSl certkeys Configure routes Configure user monitors Add system file (for uploading SSL certkeys from Kubernetes) Configure Virtual IP address (VIP) Check the status of the Citrix ADC appliance","title":"Create System User Account for CIC in Citrix ADC"},{"location":"deploy/deploy-cic-yaml/#to-create-the-system-user-account-perform-the-following","text":"Log on to the Citrix ADC appliance. Perform the following: Use an SSH client, such as PuTTy, to open an SSH connection to the Citrix ADC appliance. Log on to the appliance by using the administrator credentials. Create the system user account using the following command: add system user username password For example: add system user cic mypassword Create a policy to provide required permissions to the system user account. Use the following command: add cmdpolicy cic-policy ALLOW (^\\S+\\s+cs\\s+\\S+)|(^\\S+\\s+lb\\s+\\S+)|(^\\S+\\s+service\\s+\\S+)|(^\\S+\\s+servicegroup\\s+\\S+)|(^stat\\s+system)|(^show\\s+ha)|(^\\S+\\s+ssl\\s+certKey)|(^\\S+\\s+ssl)|(^\\S+\\s+route)|(^\\S+\\s+monitor)|(^show\\s+ns\\s+ip)|(^\\S+\\s+system\\s+file) Note The system user account would have the privileges based on the command policy that you define. Bind the policy to the system user account using the following command: bind system user cic cic-policy 0","title":"To create the system user account, perform the following:"},{"location":"deploy/deploy-cic-yaml/#deploy-cic-as-a-pod","text":"Perform the following: Download the citrix-k8s-ingress-controller.yaml using the following command: wget https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/baremetal/citrix-k8s-ingress-controller.yaml Edit the citrix-k8s-ingress-controller.yaml file and enter the values for the following environmental variables: Environment Variable Mandatory or Optional Description NS_IP Mandatory The IP address of the Citrix ADC appliance. For more details, see Prerequisites . NS_USER and NS_PASSWORD Mandatory The username and password of the Citrix ADC VPX or MPX appliance used as the Ingress device. For more details, see Prerequisites . EULA Mandatory The End User License Agreement. Specify the value as Yes . Kubernetes_url Optional The kube-apiserver url that CIC uses to register the events. If the value is not specified, CIC uses the internal kube-apiserver IP address . LOGLEVEL Optional The log levels to control the logs generated by CIC. By default, the value is set to DEBUG. The supported values are: CRITICAL, ERROR, WARNING, INFO, and DEBUG. For more information, see Logging NS_PROTOCOL and NS_PORT Optional Defines the protocol and port that must be used by CIC to communicate with Citrix ADC. By default, CIC uses HTTPS on port 443. You can also use HTTP on port 80. ingress-classes Optional If multiple ingress load balancers are used to load balance different ingress resources. You can use this environment variable to specify CIC to configure Citrix ADC associated with specific ingress class. For information on Ingress classes, see Ingress class support NS_IP Optional CIC uses the IP address provided in this environment variable to configure a virtual IP address to the Citrix ADC that receives Ingress traffic. Note: NS_IP takes precedence over the frontend-ip annotation. Once you update the environment variables, save the YAML file and deploy it using the following command: kubectl create -f citrix-k8s-ingress-controller.yaml Verify if CIC is deployed successfully using the following command: kubectl get pods --all-namespaces","title":"Deploy CIC as a pod"},{"location":"deploy/deploy-cic-yaml/#deploying-cic-as-a-sidecar-with-citrix-adc-cpx","text":"Use the citrix-k8s-cpx-ingress.yaml file to deploy a Citrix ADC CPX with CIC as a sidecar. The YAML file deploys a Citrix ADC CPX instance that is used for load balancing the North-South traffic to the microservices in your Kubernetes cluster. Perform the following: Download the citrix-k8s-cpx-ingress.yaml using the following command: wget https://github.com/citrix/citrix-k8s-ingress-controller/blob/master/deployment/baremetal/citrix-k8s-cpx-ingress.yml Edit the citrix-k8s-cpx-ingress.yaml file and enter the values for the following environmental variables: Environment Variable Mandatory or Optional Description NS_IP Mandatory The IP address of the Citrix ADC appliance. For more details, see Prerequisites . NS_USER and NS_PASSWORD Mandatory The username and password of the Citrix ADC VPX or MPX appliance used as the Ingress device. For more details, see Prerequisites . EULA Mandatory The End User License Agreement. Specify the value as Yes . NS_PROTOCOL and NS_PORT Optional Defines the protocol and port that must be used by CIC to communicate with Citrix ADC. By default, CIC uses HTTPS on port 443. You can also use HTTP on port 80. Once you update the environment variables, save the YAML file and deploy it using the following command: kubectl create -f citrix-k8s-cpx-ingress.yaml Verify if CIC is deployed successfully using the following command: kubectl get pods --all-namespaces","title":"Deploying CIC as a sidecar with Citrix ADC CPX"},{"location":"deploy/deploy-gcp/","text":"Deploying Citrix ADC CPX as an Ingress device in Google Cloud Platform This section explains how to deploy Citrix ADC CPX as an ingress device in Google Kubernetes Engine (GKE) and Google Compute Engine (GCE) clusters. The procedure to deploy the Citrix ADC CPX is the same for both Google Kubernetes Engine (GKE) and Google Compute Engine (GCE). However, if you configure Kubernetes on Google Compute Engine (GCE), then you need to deploy the CNI plug-in for the Kubernetes cluster. Prerequisites You should complete the following tasks before performing the steps in the procedure. Ensure that you have a Kubernetes Cluster up and running. If you are running your cluster in GKE, ensure that you have configured a cluster-admin role binding. You can use the following command to configure cluster-admin role binding. kubectl create clusterrolebinding citrix-cluster-admin --clusterrole=cluster-admin --user= email-id of your google account You can get your Google account details using the following command. gcloud info | grep Account Deploying Citrix ADC CPX as an ingress device in Google Cloud Platform Deploy the required application in your Kubernetes cluster and expose it as a service in your cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/apache.yaml Note In this example, apache.yaml is used. You should use the specific YAML file for your application. Deploy Citrix ADC CPX as an ingress device in the cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/standalone_cpx.yaml Create the ingress resource using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/cpx_ingress.yaml Create a service of type LoadBalancer for accessing the Citrix ADC CPX by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/cpx_service.yaml Note This command creates a load balancer with an external IP for receiving traffic. Verify the service and check whether the load balancer has created an external IP. Wait for some time if the external IP is not created. kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE apache ClusterIP 10.7.248.216 none 80/TCP 2m cpx-ingress LoadBalancer 10.7.241.6 pending 80:32258/TCP,443:32084/TCP 2m kubernetes ClusterIP 10.7.240.1 none 443/TCP 22h Once the external IP for the load-balancer is available as follows, you can access your resources using the external IP for the load balancer. kubectl get svc Name Type Cluster-IP External IP Port(s) Age apache ClusterIP 10.7.248.216 none 80/TCP 3m cpx-ingress LoadBalancer 10.7.241.6 EXTERNAL-IP CREATED 80:32258/TCP,443:32084/TCP 3m kubernetes ClusterIP 10.7.240.1 none 443/TCP 22h Note The health check for the cloud load-balancer is obtained from the readinessProbe configured in the Citrix ADC CPX deployment YAML file. If the health check fails, you should check the readinessProbe configured for Citrix ADC CPX. For more information, see readinessProbe and external Load balancer . Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Deployment models You can use the following deployment solutions for deploying CPX as an ingress device in Google Cloud. Standalone Citrix ADC CPX deployment High availability Citrix ADC CPX deployment Citrix ADC CPX per node deployment Note For the ease of deployment, the deployment models in this section are explained with an all-in-one manifest file that combines the steps explained in the previous section. You can modify the manifest file to suit your application and configuration. Deploying a standalone Citrix ADC CPX as the ingress device To deploy Citrix ADC CPX as an Ingress in a standalone deployment model in GCP, you should use the Service Type as LoadBalancer. This step creates a load balancer in the Google cloud. Deploy a Citrix ADC CPX ingress with in built Citrix Ingress Controller in your Kubernetes cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one.yaml Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Note To delete the deployment, use the following command: kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one.yaml Deploying Citrix ADC CPX for high availability In the standalone deployment of Citrix ADC CPX as ingress, if the ingress device fails, there would be a traffic outage for a few seconds. To avoid this traffic disruption, you can deploy two Citrix ADC CPX ingress devices instead of deploying a single Citrix ADC CPX ingress device. In such deployments, even if one Citrix ADC CPX fails the other Citrix ADC CPX is available to handle the traffic until the failed Citrix ADC CPX comes up. Deploy Citrix ADC CPX ingress devices for high availability in your Kubernetes cluster by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one-ha.yaml Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Note To delete the deployment, use the following command. kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one-ha.yaml Deploying Citrix ADC CPX per node Sometimes when cluster nodes are added and removed from the cluster, CPX can also be deployed as DaemonSets. This deployment ensures that every node has a CPX ingress device in it. When the traffic is high, such a deployment is a much more reliable solution than deploying two Citrix ADC CPX devices as ingress devices. Deploy Citrix ADC CPX ingress device in each node of your Kubernetes cluster by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one-reliable.yaml Access the application by using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Note To delete the deployment, use the following command. kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one-reliable.yaml","title":"Deploy Citrix ADC CPX as an Ingress in Google Cloud Platform"},{"location":"deploy/deploy-gcp/#deploying-citrix-adc-cpx-as-an-ingress-device-in-google-cloud-platform","text":"This section explains how to deploy Citrix ADC CPX as an ingress device in Google Kubernetes Engine (GKE) and Google Compute Engine (GCE) clusters. The procedure to deploy the Citrix ADC CPX is the same for both Google Kubernetes Engine (GKE) and Google Compute Engine (GCE). However, if you configure Kubernetes on Google Compute Engine (GCE), then you need to deploy the CNI plug-in for the Kubernetes cluster.","title":"Deploying Citrix ADC CPX as an Ingress device in Google Cloud Platform"},{"location":"deploy/deploy-gcp/#prerequisites","text":"You should complete the following tasks before performing the steps in the procedure. Ensure that you have a Kubernetes Cluster up and running. If you are running your cluster in GKE, ensure that you have configured a cluster-admin role binding. You can use the following command to configure cluster-admin role binding. kubectl create clusterrolebinding citrix-cluster-admin --clusterrole=cluster-admin --user= email-id of your google account You can get your Google account details using the following command. gcloud info | grep Account","title":"Prerequisites"},{"location":"deploy/deploy-gcp/#deploying-citrix-adc-cpx-as-an-ingress-device-in-google-cloud-platform_1","text":"Deploy the required application in your Kubernetes cluster and expose it as a service in your cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/apache.yaml Note In this example, apache.yaml is used. You should use the specific YAML file for your application. Deploy Citrix ADC CPX as an ingress device in the cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/standalone_cpx.yaml Create the ingress resource using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/cpx_ingress.yaml Create a service of type LoadBalancer for accessing the Citrix ADC CPX by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/cpx_service.yaml Note This command creates a load balancer with an external IP for receiving traffic. Verify the service and check whether the load balancer has created an external IP. Wait for some time if the external IP is not created. kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE apache ClusterIP 10.7.248.216 none 80/TCP 2m cpx-ingress LoadBalancer 10.7.241.6 pending 80:32258/TCP,443:32084/TCP 2m kubernetes ClusterIP 10.7.240.1 none 443/TCP 22h Once the external IP for the load-balancer is available as follows, you can access your resources using the external IP for the load balancer. kubectl get svc Name Type Cluster-IP External IP Port(s) Age apache ClusterIP 10.7.248.216 none 80/TCP 3m cpx-ingress LoadBalancer 10.7.241.6 EXTERNAL-IP CREATED 80:32258/TCP,443:32084/TCP 3m kubernetes ClusterIP 10.7.240.1 none 443/TCP 22h Note The health check for the cloud load-balancer is obtained from the readinessProbe configured in the Citrix ADC CPX deployment YAML file. If the health check fails, you should check the readinessProbe configured for Citrix ADC CPX. For more information, see readinessProbe and external Load balancer . Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com'","title":"Deploying Citrix ADC CPX as an ingress device in Google Cloud Platform"},{"location":"deploy/deploy-gcp/#deployment-models","text":"You can use the following deployment solutions for deploying CPX as an ingress device in Google Cloud. Standalone Citrix ADC CPX deployment High availability Citrix ADC CPX deployment Citrix ADC CPX per node deployment Note For the ease of deployment, the deployment models in this section are explained with an all-in-one manifest file that combines the steps explained in the previous section. You can modify the manifest file to suit your application and configuration.","title":"Deployment models"},{"location":"deploy/deploy-gcp/#deploying-a-standalone-citrix-adc-cpx-as-the-ingress-device","text":"To deploy Citrix ADC CPX as an Ingress in a standalone deployment model in GCP, you should use the Service Type as LoadBalancer. This step creates a load balancer in the Google cloud. Deploy a Citrix ADC CPX ingress with in built Citrix Ingress Controller in your Kubernetes cluster using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one.yaml Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Note To delete the deployment, use the following command: kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one.yaml","title":"Deploying a standalone Citrix ADC CPX as the ingress device"},{"location":"deploy/deploy-gcp/#deploying-citrix-adc-cpx-for-high-availability","text":"In the standalone deployment of Citrix ADC CPX as ingress, if the ingress device fails, there would be a traffic outage for a few seconds. To avoid this traffic disruption, you can deploy two Citrix ADC CPX ingress devices instead of deploying a single Citrix ADC CPX ingress device. In such deployments, even if one Citrix ADC CPX fails the other Citrix ADC CPX is available to handle the traffic until the failed Citrix ADC CPX comes up. Deploy Citrix ADC CPX ingress devices for high availability in your Kubernetes cluster by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one-ha.yaml Access the application using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Note To delete the deployment, use the following command. kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one-ha.yaml","title":"Deploying Citrix ADC CPX for high availability"},{"location":"deploy/deploy-gcp/#deploying-citrix-adc-cpx-per-node","text":"Sometimes when cluster nodes are added and removed from the cluster, CPX can also be deployed as DaemonSets. This deployment ensures that every node has a CPX ingress device in it. When the traffic is high, such a deployment is a much more reliable solution than deploying two Citrix ADC CPX devices as ingress devices. Deploy Citrix ADC CPX ingress device in each node of your Kubernetes cluster by using the following command. kubectl create -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one-reliable.yaml Access the application by using the following command. curl http:// External-ip-of-loadbalancer / -H 'Host: citrix-ingress.com' Note To delete the deployment, use the following command. kubectl delete -f https://raw.githubusercontent.com/citrix/citrix-k8s-ingress-controller/master/deployment/gcp/manifest/all-in-one-reliable.yaml","title":"Deploying Citrix ADC CPX per node"},{"location":"metrics/promotheus-grafana/","text":"Viewing metrics of Citrix ADCs in Kubernetes You can use the Citrix ADC metrics exporter and Prometheus-Operator to monitor Citrix ADC VPX or CPX ingress devices and Citrix ADC CPX (east-west) devices. Citrix ADC metrics exporter Citrix ADC metrics exporter is a simple server that collects Citrix ADC stats and exports them to Prometheus using HTTP . You can then add Prometheus as a data source to Grafana and graphically view the Citrix ADC stats. For more information see, Citrix ADC metrics exporter . Launching prometheus-operator Prometheus Operator has an expansive method of monitoring services on Kubernetes. To get started, this topic uses kube-prometheus and its manifest files. The manifest files help you to deploy a basic working model. Deploy the Prometheus Operator in your Kubernetes environment using the following commands: git clone https://github.com/coreos/prometheus-operator.git kubectl create -f prometheus-operator/contrib/kube-prometheus/manifests/ Once you deploy Prometheus-Operator , several pods and services are deployed, of which prometheus-k8s-xx pods are for metrics aggregation and timestamping and grafana pods are for visualization. If you view all the container images running in the cluster, you can see the following output: $ kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE alertmanager-main-0 2/2 Running 0 2h alertmanager-main-1 2/2 Running 0 2h alertmanager-main-2 2/2 Running 0 2h grafana-5b68464b84-5fvxq 1/1 Running 0 2h kube-state-metrics-6588b6b755-d6ftg 4/4 Running 0 2h node-exporter-4hbcp 2/2 Running 0 2h node-exporter-kn9dg 2/2 Running 0 2h node-exporter-tpxhp 2/2 Running 0 2h prometheus-k8s-0 3/3 Running 1 2h prometheus-k8s-1 3/3 Running 1 2h prometheus-operator-7d9fd546c4-m8t7v 1/1 Running 0 2h Note It is recommended to expose the Prometheus and Grafana pods through NodePorts. To do so, you need to modify the prometheus-service.yaml and grafana-service.yaml files as follows: prometheus-service.yaml : apiVersion: v1 kind: Service metadata: labels: prometheus: k8s name: prometheus-k8s namespace: monitoring spec: type: NodePort ports: - name: web port: 9090 targetPort: web selector: app: prometheus prometheus: k8s To apply these changes into the kubernetes cluster use the following command: kubectl apply -f prometheus-service.yaml grafana-service.yaml : apiVersion: v1 kind: Service metadata: name: grafana namespace: monitoring spec: type: NodePort ports: - name: http port: 3000 targetPort: http selector: app: grafana To apply these changes into the kubernetes cluster use the following command: kubectl apply -f grafana-service.yaml Configuring Citrix ADC metrics exporter This topic describes how to integrate the Citrix ADC metrics exporter with Citrix ADC VPX or CPX ingress or Citrix ADC CPX (east-west) devices. Citrix ADC VPX Ingress device : To monitor an ingress Citrix ADC VPX device, the Citrix ADC metrics exporter is run as a pod within the kubernetes cluster. The IP address of the Citrix ADC VPX ingress device is provided as an argument to the Citrix ADC metrics exporter. The following is a sample yaml file to deploy such an exporter: apiVersion: v1 kind: Pod metadata: name: exporter-vpx-ingress labels: app: exporter-vpx-ingress spec: containers: - name: exporter image: quay.io/citrix/netscaler-metrics-exporter:v1.0.0 imagePullPolicy: IfNotPresent args: - --target-nsip= IP_and_port_of_VPX - --port=8888 --- kind: Service apiVersion: v1 metadata: name: exporter-vpx-ingress labels: service-type: citrix-adc-monitor spec: selector: name: exporter-vpx-ingress ports: - name: exporter-port port: 8888 targetPort: 8888 The IP and port of the Citrix ADC VPX device needs to be provided in the --target-nsip parameter. For example, --target-nsip=10.0.0.20 . Citrix ADC CPX Ingress device : To monitor a Citrix ADC CPX ingress device, the Citrix ADC metrics exporter is added as a side-car. The following is a sample yaml file of a CPX ingress device with an exporter as a side car: --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cpx-ingress labels: app: cpx-ingress spec: replicas: 1 selector: matchLabels: app: cpx-ingress template: metadata: labels: app: cpx-ingress annotations: NETSCALER_AS_APP: True spec: serviceAccountName: cpx containers: - name: cpx-ingress image: us.gcr.io/citrix-217108/citrix-k8s-cpx-ingress:latest imagePullPolicy: IfNotPresent securityContext: privileged: true env: - name: EULA value: YES - name: NS_PROTOCOL value: HTTP #Define the NITRO port here - name: NS_PORT value: 9080 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: nitro-http containerPort: 9080 - name: nitro-https containerPort: 9443 # Adding exporter as a side-car - name: exporter image: quay.io/citrix/netscaler-metrics-exporter:v1.0.0 imagePullPolicy: IfNotPresent args: - --target-nsip=192.0.0.2 - --port=8888 --- kind: Service apiVersion: v1 metadata: name: exporter-cpx-ingress labels: service-type: citrix-adc-monitor spec: selector: app: cpx-ingress ports: - name: exporter-port port: 8888 targetPort: 8888 Here, the exporter uses the local IP address ( 192.0.0.2 ) to fetch metrics from the Citrix ADC CPX. Citrix ADC CPX (east-west) device : To monitor a Citrix ADC CPX (east-west) device, the Citrix ADC metrics exporter is added as a side-car. The following is a sample yaml file of a Citrix ADC CPX (east-west) device with an exporter as a side car: apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: cpx-ew spec: template: metadata: name: cpx-ew labels: app: cpx-ew annotations: NETSCALER_AS_APP: True spec: serviceAccountName: cpx hostNetwork: true containers: - name: cpx image: in-docker-reg.eng.citrite.net/cpx-dev/cpx:12.1-48.118 securityContext: privileged: true env: - name: EULA value: yes - name: NS_NETMODE value: HOST #- name: kubernetes_url # value: https://10..xx.xx:6443 # Add exporter as a sidecar - name: exporter image: quay.io/citrix/netscaler-metrics-exporter:v1.0.0 args: - --target-nsip=192.168.0.2:80 - --port=8888 imagePullPolicy: IfNotPresent --- kind: Service apiVersion: v1 metadata: name: exporter-cpx-ew labels: service-type: citrix-adc-monitor spec: selector: app: cpx-ew ports: - name: exporter-port port: 8888 targetPort: 8888 Here, the exporter uses the local IP ( 192.168.0.2 ) to fetch metrics from the Citrix ADC CPX (east-west) device. ServiceMonitors to detect Citrix ADC The Citrix ADC metrics exporter helps collect data from the Citrix ADC VPX or CPX ingress and Citrix ADC CPX (east-west) devices. Prometheus Operator need to detect these exporters so that the metrics can be timestamped, stored, and exposed for visualization on Grafana. Prometheus Operator uses the concept of ServiceMonitors to detect pods belonging to a service, using the labels attached to that service. The following example yaml file detects all the exporter services (given in the sample yaml files) which have the label service-type: citrix-adc-monitor associated with them. apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: citrix-adc-servicemonitor labels: servicemonitor: citrix-adc spec: endpoints: - interval: 30s port: exporter-port selector: matchLabels: service-type: citrix-adc-monitor namespaceSelector: matchNames: - monitoring - default Viewing the metrics in Grafana The Citrix ADC instances which were detected for monitoring appears in the Targets page of the prometheus container. It can be accessed using http:// k8s_cluster_ip : prometheus_nodeport /targets and looks like the following image: To view the metrics graphically: Log into grafana using http:// k8s_cluster_ip : grafafa_nodeport with default credentials admin:admin Import the sample grafana dashboard by selecting the + icon on the left panel and clicking import. A dashboard containing graphs similar to the following should appear: The dashboard can be further enhanced using Grafana's documentation or demo videos .","title":"Integrate with Promotheus and Grafana"},{"location":"metrics/promotheus-grafana/#viewing-metrics-of-citrix-adcs-in-kubernetes","text":"You can use the Citrix ADC metrics exporter and Prometheus-Operator to monitor Citrix ADC VPX or CPX ingress devices and Citrix ADC CPX (east-west) devices.","title":"Viewing metrics of Citrix ADCs in Kubernetes"},{"location":"metrics/promotheus-grafana/#citrix-adc-metrics-exporter","text":"Citrix ADC metrics exporter is a simple server that collects Citrix ADC stats and exports them to Prometheus using HTTP . You can then add Prometheus as a data source to Grafana and graphically view the Citrix ADC stats. For more information see, Citrix ADC metrics exporter .","title":"Citrix ADC metrics exporter"},{"location":"metrics/promotheus-grafana/#launching-prometheus-operator","text":"Prometheus Operator has an expansive method of monitoring services on Kubernetes. To get started, this topic uses kube-prometheus and its manifest files. The manifest files help you to deploy a basic working model. Deploy the Prometheus Operator in your Kubernetes environment using the following commands: git clone https://github.com/coreos/prometheus-operator.git kubectl create -f prometheus-operator/contrib/kube-prometheus/manifests/ Once you deploy Prometheus-Operator , several pods and services are deployed, of which prometheus-k8s-xx pods are for metrics aggregation and timestamping and grafana pods are for visualization. If you view all the container images running in the cluster, you can see the following output: $ kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE alertmanager-main-0 2/2 Running 0 2h alertmanager-main-1 2/2 Running 0 2h alertmanager-main-2 2/2 Running 0 2h grafana-5b68464b84-5fvxq 1/1 Running 0 2h kube-state-metrics-6588b6b755-d6ftg 4/4 Running 0 2h node-exporter-4hbcp 2/2 Running 0 2h node-exporter-kn9dg 2/2 Running 0 2h node-exporter-tpxhp 2/2 Running 0 2h prometheus-k8s-0 3/3 Running 1 2h prometheus-k8s-1 3/3 Running 1 2h prometheus-operator-7d9fd546c4-m8t7v 1/1 Running 0 2h Note It is recommended to expose the Prometheus and Grafana pods through NodePorts. To do so, you need to modify the prometheus-service.yaml and grafana-service.yaml files as follows: prometheus-service.yaml : apiVersion: v1 kind: Service metadata: labels: prometheus: k8s name: prometheus-k8s namespace: monitoring spec: type: NodePort ports: - name: web port: 9090 targetPort: web selector: app: prometheus prometheus: k8s To apply these changes into the kubernetes cluster use the following command: kubectl apply -f prometheus-service.yaml grafana-service.yaml : apiVersion: v1 kind: Service metadata: name: grafana namespace: monitoring spec: type: NodePort ports: - name: http port: 3000 targetPort: http selector: app: grafana To apply these changes into the kubernetes cluster use the following command: kubectl apply -f grafana-service.yaml","title":"Launching prometheus-operator"},{"location":"metrics/promotheus-grafana/#configuring-citrix-adc-metrics-exporter","text":"This topic describes how to integrate the Citrix ADC metrics exporter with Citrix ADC VPX or CPX ingress or Citrix ADC CPX (east-west) devices. Citrix ADC VPX Ingress device : To monitor an ingress Citrix ADC VPX device, the Citrix ADC metrics exporter is run as a pod within the kubernetes cluster. The IP address of the Citrix ADC VPX ingress device is provided as an argument to the Citrix ADC metrics exporter. The following is a sample yaml file to deploy such an exporter: apiVersion: v1 kind: Pod metadata: name: exporter-vpx-ingress labels: app: exporter-vpx-ingress spec: containers: - name: exporter image: quay.io/citrix/netscaler-metrics-exporter:v1.0.0 imagePullPolicy: IfNotPresent args: - --target-nsip= IP_and_port_of_VPX - --port=8888 --- kind: Service apiVersion: v1 metadata: name: exporter-vpx-ingress labels: service-type: citrix-adc-monitor spec: selector: name: exporter-vpx-ingress ports: - name: exporter-port port: 8888 targetPort: 8888 The IP and port of the Citrix ADC VPX device needs to be provided in the --target-nsip parameter. For example, --target-nsip=10.0.0.20 . Citrix ADC CPX Ingress device : To monitor a Citrix ADC CPX ingress device, the Citrix ADC metrics exporter is added as a side-car. The following is a sample yaml file of a CPX ingress device with an exporter as a side car: --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: cpx-ingress labels: app: cpx-ingress spec: replicas: 1 selector: matchLabels: app: cpx-ingress template: metadata: labels: app: cpx-ingress annotations: NETSCALER_AS_APP: True spec: serviceAccountName: cpx containers: - name: cpx-ingress image: us.gcr.io/citrix-217108/citrix-k8s-cpx-ingress:latest imagePullPolicy: IfNotPresent securityContext: privileged: true env: - name: EULA value: YES - name: NS_PROTOCOL value: HTTP #Define the NITRO port here - name: NS_PORT value: 9080 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: nitro-http containerPort: 9080 - name: nitro-https containerPort: 9443 # Adding exporter as a side-car - name: exporter image: quay.io/citrix/netscaler-metrics-exporter:v1.0.0 imagePullPolicy: IfNotPresent args: - --target-nsip=192.0.0.2 - --port=8888 --- kind: Service apiVersion: v1 metadata: name: exporter-cpx-ingress labels: service-type: citrix-adc-monitor spec: selector: app: cpx-ingress ports: - name: exporter-port port: 8888 targetPort: 8888 Here, the exporter uses the local IP address ( 192.0.0.2 ) to fetch metrics from the Citrix ADC CPX. Citrix ADC CPX (east-west) device : To monitor a Citrix ADC CPX (east-west) device, the Citrix ADC metrics exporter is added as a side-car. The following is a sample yaml file of a Citrix ADC CPX (east-west) device with an exporter as a side car: apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: cpx-ew spec: template: metadata: name: cpx-ew labels: app: cpx-ew annotations: NETSCALER_AS_APP: True spec: serviceAccountName: cpx hostNetwork: true containers: - name: cpx image: in-docker-reg.eng.citrite.net/cpx-dev/cpx:12.1-48.118 securityContext: privileged: true env: - name: EULA value: yes - name: NS_NETMODE value: HOST #- name: kubernetes_url # value: https://10..xx.xx:6443 # Add exporter as a sidecar - name: exporter image: quay.io/citrix/netscaler-metrics-exporter:v1.0.0 args: - --target-nsip=192.168.0.2:80 - --port=8888 imagePullPolicy: IfNotPresent --- kind: Service apiVersion: v1 metadata: name: exporter-cpx-ew labels: service-type: citrix-adc-monitor spec: selector: app: cpx-ew ports: - name: exporter-port port: 8888 targetPort: 8888 Here, the exporter uses the local IP ( 192.168.0.2 ) to fetch metrics from the Citrix ADC CPX (east-west) device.","title":"Configuring Citrix ADC metrics exporter"},{"location":"metrics/promotheus-grafana/#servicemonitors-to-detect-citrix-adc","text":"The Citrix ADC metrics exporter helps collect data from the Citrix ADC VPX or CPX ingress and Citrix ADC CPX (east-west) devices. Prometheus Operator need to detect these exporters so that the metrics can be timestamped, stored, and exposed for visualization on Grafana. Prometheus Operator uses the concept of ServiceMonitors to detect pods belonging to a service, using the labels attached to that service. The following example yaml file detects all the exporter services (given in the sample yaml files) which have the label service-type: citrix-adc-monitor associated with them. apiVersion: monitoring.coreos.com/v1 kind: ServiceMonitor metadata: name: citrix-adc-servicemonitor labels: servicemonitor: citrix-adc spec: endpoints: - interval: 30s port: exporter-port selector: matchLabels: service-type: citrix-adc-monitor namespaceSelector: matchNames: - monitoring - default","title":"ServiceMonitors to detect Citrix ADC"},{"location":"metrics/promotheus-grafana/#viewing-the-metrics-in-grafana","text":"The Citrix ADC instances which were detected for monitoring appears in the Targets page of the prometheus container. It can be accessed using http:// k8s_cluster_ip : prometheus_nodeport /targets and looks like the following image: To view the metrics graphically: Log into grafana using http:// k8s_cluster_ip : grafafa_nodeport with default credentials admin:admin Import the sample grafana dashboard by selecting the + icon on the left panel and clicking import. A dashboard containing graphs similar to the following should appear: The dashboard can be further enhanced using Grafana's documentation or demo videos .","title":"Viewing the metrics in Grafana"},{"location":"network/staticrouting/","text":"Configure Static Routing One of the ways to achieve network connectivity between pods and Citrix ADC instance is to configure routes on the Citrix ADC instance to the overlay network. You can either do this manually or Citrix Ingress Controller (CIC) provides an option to automatically configure the network. Note Ensure that the Citrix ADC instance (MPX or VPX) has SNIP configured on the host network. The host network is the network on which the Kubernetes nodes communicate with each other. Manually configure route on Citrix ADC instance Perform the following: On the master node in the Kubernetes cluster, get the podCIDR using the following command: # kubectl get nodes -o jsonpath= {range .items[*]}{'podNetwork: '}{.spec.podCIDR}{'\\t'}{'gateway: '}{.status.addresses[0].address}{'\\n'}{end} podNetwork: 10.244.0.0/24 gateway: 10.106.162.108 podNetwork: 10.244.2.0/24 gateway: 10.106.162.109 podNetwork: 10.244.1.0/24 gateway: 10.106.162.106 Log on to the Citrix ADC instance. Add route on the Citrix ADC instance using the podCIDR information. Use the following command: add route pod_network podCIDR_netmask gateway For example, add route 10.244.0.0 255.255.255.0 10.106.162.108 add route 10.244.2.0 255.255.255.0 10.106.162.109 add route 10.244.1.0 255.255.255.0 10.106.162.106 Automatically configure route on Citrix ADC instance In the citrix-k8s-ingress-controller.yaml file, you can use an argument, feature-node-watch to automatically configure route on the associated Citrix ADC instance. Set the feature-node-watch argument to true to enable automatic route configuration. You can specify this argument in the citrix-k8s-ingress-controller.yaml file as follows: spec: serviceAccountName: cic-k8s-role containers: - name: cic-k8s-ingress-controller image: quay.io/citrix/citrix-k8s-ingress-controller:latest # feature-node-watch argument configures route(s) on the Ingress NetScaler # to provide connectivity to the pod network. By default, this feature is disabled. args: - --feature-node-watch true Points to Note By default, the feature-node-watch argument is set to false . Set the argument to true to enable the automatic route configuration. For automatic route configuration, you must provide permissions to listen to the events of nodes resource type. You can provide the required permissions in the citrix-k8s-ingress-controller.yaml file as follows: kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: cic-k8s-role rules: - apiGroups: [ ] resources: [ services , endpoints , ingresses , pods , secrets , nodes ] verbs: [ * ]","title":"Static routing"},{"location":"network/staticrouting/#configure-static-routing","text":"One of the ways to achieve network connectivity between pods and Citrix ADC instance is to configure routes on the Citrix ADC instance to the overlay network. You can either do this manually or Citrix Ingress Controller (CIC) provides an option to automatically configure the network. Note Ensure that the Citrix ADC instance (MPX or VPX) has SNIP configured on the host network. The host network is the network on which the Kubernetes nodes communicate with each other.","title":"Configure Static Routing"},{"location":"network/staticrouting/#manually-configure-route-on-citrix-adc-instance","text":"Perform the following: On the master node in the Kubernetes cluster, get the podCIDR using the following command: # kubectl get nodes -o jsonpath= {range .items[*]}{'podNetwork: '}{.spec.podCIDR}{'\\t'}{'gateway: '}{.status.addresses[0].address}{'\\n'}{end} podNetwork: 10.244.0.0/24 gateway: 10.106.162.108 podNetwork: 10.244.2.0/24 gateway: 10.106.162.109 podNetwork: 10.244.1.0/24 gateway: 10.106.162.106 Log on to the Citrix ADC instance. Add route on the Citrix ADC instance using the podCIDR information. Use the following command: add route pod_network podCIDR_netmask gateway For example, add route 10.244.0.0 255.255.255.0 10.106.162.108 add route 10.244.2.0 255.255.255.0 10.106.162.109 add route 10.244.1.0 255.255.255.0 10.106.162.106","title":"Manually configure route on Citrix ADC instance"},{"location":"network/staticrouting/#automatically-configure-route-on-citrix-adc-instance","text":"In the citrix-k8s-ingress-controller.yaml file, you can use an argument, feature-node-watch to automatically configure route on the associated Citrix ADC instance. Set the feature-node-watch argument to true to enable automatic route configuration. You can specify this argument in the citrix-k8s-ingress-controller.yaml file as follows: spec: serviceAccountName: cic-k8s-role containers: - name: cic-k8s-ingress-controller image: quay.io/citrix/citrix-k8s-ingress-controller:latest # feature-node-watch argument configures route(s) on the Ingress NetScaler # to provide connectivity to the pod network. By default, this feature is disabled. args: - --feature-node-watch true Points to Note By default, the feature-node-watch argument is set to false . Set the argument to true to enable the automatic route configuration. For automatic route configuration, you must provide permissions to listen to the events of nodes resource type. You can provide the required permissions in the citrix-k8s-ingress-controller.yaml file as follows: kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: cic-k8s-role rules: - apiGroups: [ ] resources: [ services , endpoints , ingresses , pods , secrets , nodes ] verbs: [ * ]","title":"Automatically configure route on Citrix ADC instance"}]}